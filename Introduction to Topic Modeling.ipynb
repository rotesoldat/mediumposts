{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "** **\n",
    "Topic Models, in a nutshell, are a type of statistical language models used for uncovering hidden structure in a collection of texts. In a practical and more intuitively, you can think of it as a task of:\n",
    "\n",
    "- **Dimensionality Reduction**, where rather than representing a text T in its feature space as {Word_i: count(Word_i, T) for Word_i in Vocabulary}, you can represent it in a topic space as {Topic_i: Weight(Topic_i, T) for Topic_i in Topics}\n",
    "- **Unsupervised Learning**, where it can be compared to clustering, as in the case of clustering, the number of topics, like the number of clusters, is an output parameter. By doing topic modeling, we build clusters of words rather than clusters of texts. A text is thus a mixture of all the topics, each having a specific weight\n",
    "- **Tagging**, abstract “topics” that occur in a collection of documents that best represents the information in them.\n",
    "\n",
    "There are several existing algorithms you can use to perform the topic modeling. The most common of it are, Latent Semantic Analysis (LSA/LSI), Probabilistic Latent Semantic Analysis (pLSA), and Latent Dirichlet Allocation (LDA)\n",
    "\n",
    "In this tutorial, we’ll take a closer look at LDA, and implement our first topic model using the sklearn implementation in python 2.7\n",
    "\n",
    "### Theoretical Overview\n",
    "LDA is a generative probabilistic model that assumes each topic is a mixture over an underlying set of words, and each document is a mixture of over a set of topic probabilities.\n",
    "\n",
    "![LDA_Model](https://github.com/chdoig/pytexas2015-topic-modeling/blob/master/images/lda-4.png?raw=true)\n",
    "\n",
    "We can describe the generative process of LDA as, given the M number of documents, N number of words, and prior K number of topics, the model trains to output:\n",
    "\n",
    "- `psi`, the distribution of words for each topic K\n",
    "- `phi`, the distribution of topics for each document i\n",
    "\n",
    "#### Parameters of LDA\n",
    "\n",
    "- `Alpha parameter` is Dirichlet prior concentration parameter that represents document-topic density — with a higher alpha, documents are assumed to be made up of more topics and result in more specific topic distribution per document.\n",
    "- `Beta parameter` is the same prior concentration parameter that represents topic-word density — with high beta, topics are assumed to made of up most of the words and result in a more specific word distribution per topic.\n",
    "\n",
    "**To read more: https://towardsdatascience.com/end-to-end-topic-modeling-in-python-latent-dirichlet-allocation-lda-35ce4ed6b3e0**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** **\n",
    "### LDA Implementation\n",
    "\n",
    "1. [Loading data](#load_data)\n",
    "2. [Data cleaning](#clean_data)\n",
    "3. [Exploratory analysis](#eda)\n",
    "4. [Prepare data for LDA analysis](#data_preparation)\n",
    "5. [LDA model training](#train_model)\n",
    "6. [Analyzing LDA model results](#results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** **\n",
    "For this tutorial, we’ll use the dataset of papers published in NeurIPS (NIPS) conference which is one of the most prestigious yearly events in the machine learning community. The CSV data file contains information on the different NeurIPS papers that were published from 1987 until 2016 (29 years!). These papers discuss a wide variety of topics in machine learning, from neural networks to optimization methods, and many more.\n",
    "\n",
    "<img src=\"https://s3.amazonaws.com/assets.datacamp.com/production/project_158/img/nips_logo.png\" alt=\"The logo of NIPS (Neural Information Processing Systems)\">\n",
    "\n",
    "Let’s start by looking at the content of the file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** **\n",
    "#### Step 1: Loading Data <a class=\"anchor\\\" id=\"load_data\"></a>\n",
    "** **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>year</th>\n",
       "      <th>title</th>\n",
       "      <th>event_type</th>\n",
       "      <th>pdf_name</th>\n",
       "      <th>abstract</th>\n",
       "      <th>paper_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1987</td>\n",
       "      <td>Self-Organization of Associative Database and ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1-self-organization-of-associative-database-an...</td>\n",
       "      <td>Abstract Missing</td>\n",
       "      <td>767\\n\\nSELF-ORGANIZATION OF ASSOCIATIVE DATABA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>1987</td>\n",
       "      <td>A Mean Field Theory of Layer IV of Visual Cort...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10-a-mean-field-theory-of-layer-iv-of-visual-c...</td>\n",
       "      <td>Abstract Missing</td>\n",
       "      <td>683\\n\\nA MEAN FIELD THEORY OF LAYER IV OF VISU...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100</td>\n",
       "      <td>1988</td>\n",
       "      <td>Storing Covariance by the Associative Long-Ter...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100-storing-covariance-by-the-associative-long...</td>\n",
       "      <td>Abstract Missing</td>\n",
       "      <td>394\\n\\nSTORING COVARIANCE BY THE ASSOCIATIVE\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000</td>\n",
       "      <td>1994</td>\n",
       "      <td>Bayesian Query Construction for Neural Network...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1000-bayesian-query-construction-for-neural-ne...</td>\n",
       "      <td>Abstract Missing</td>\n",
       "      <td>Bayesian Query Construction for Neural\\nNetwor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1001</td>\n",
       "      <td>1994</td>\n",
       "      <td>Neural Network Ensembles, Cross Validation, an...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1001-neural-network-ensembles-cross-validation...</td>\n",
       "      <td>Abstract Missing</td>\n",
       "      <td>Neural Network Ensembles, Cross\\nValidation, a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id  year                                              title event_type  \\\n",
       "0     1  1987  Self-Organization of Associative Database and ...        NaN   \n",
       "1    10  1987  A Mean Field Theory of Layer IV of Visual Cort...        NaN   \n",
       "2   100  1988  Storing Covariance by the Associative Long-Ter...        NaN   \n",
       "3  1000  1994  Bayesian Query Construction for Neural Network...        NaN   \n",
       "4  1001  1994  Neural Network Ensembles, Cross Validation, an...        NaN   \n",
       "\n",
       "                                            pdf_name          abstract  \\\n",
       "0  1-self-organization-of-associative-database-an...  Abstract Missing   \n",
       "1  10-a-mean-field-theory-of-layer-iv-of-visual-c...  Abstract Missing   \n",
       "2  100-storing-covariance-by-the-associative-long...  Abstract Missing   \n",
       "3  1000-bayesian-query-construction-for-neural-ne...  Abstract Missing   \n",
       "4  1001-neural-network-ensembles-cross-validation...  Abstract Missing   \n",
       "\n",
       "                                          paper_text  \n",
       "0  767\\n\\nSELF-ORGANIZATION OF ASSOCIATIVE DATABA...  \n",
       "1  683\\n\\nA MEAN FIELD THEORY OF LAYER IV OF VISU...  \n",
       "2  394\\n\\nSTORING COVARIANCE BY THE ASSOCIATIVE\\n...  \n",
       "3  Bayesian Query Construction for Neural\\nNetwor...  \n",
       "4  Neural Network Ensembles, Cross\\nValidation, a...  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing modules\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# os.chdir('..')\n",
    "\n",
    "# Read data into papers\n",
    "papers = pd.read_csv('papers.csv')\n",
    "\n",
    "# Print head\n",
    "papers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6560"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(papers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28333.5"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papers['symbols'] = papers['paper_text'].apply(lambda x: len(str(x)))\n",
    "papers['symbols'].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_list = [x.split('\\n') for x in papers.head()['paper_text'].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['767',\n",
       "  '',\n",
       "  'SELF-ORGANIZATION OF ASSOCIATIVE DATABASE',\n",
       "  'AND ITS APPLICATIONS',\n",
       "  'Hisashi Suzuki and Suguru Arimoto',\n",
       "  'Osaka University, Toyonaka, Osaka 560, Japan',\n",
       "  'ABSTRACT',\n",
       "  'An efficient method of self-organizing associative databases is proposed together with',\n",
       "  'applications to robot eyesight systems. The proposed databases can associate any input',\n",
       "  'with some output. In the first half part of discussion, an algorithm of self-organization is',\n",
       "  'proposed. From an aspect of hardware, it produces a new style of neural network. In the',\n",
       "  'latter half part, an applicability to handwritten letter recognition and that to an autonomous',\n",
       "  'mobile robot system are demonstrated.',\n",
       "  '',\n",
       "  'INTRODUCTION',\n",
       "  'Let a mapping f : X -+ Y be given. Here, X is a finite or infinite set, and Y is another',\n",
       "  'finite or infinite set. A learning machine observes any set of pairs (x, y) sampled randomly',\n",
       "  'from X x Y. (X x Y means the Cartesian product of X and Y.) And, it computes some',\n",
       "  'estimate j : X -+ Y of f to make small, the estimation error in some measure.',\n",
       "  'Usually we say that: the faster the decrease of estimation error with increase of the number of samples, the better the learning machine. However, such expression on performance',\n",
       "  'is incomplete. Since, it lacks consideration on the candidates of J of j assumed preliminarily. Then, how should we find out good learning machines? To clarify this conception,',\n",
       "  'let us discuss for a while on some types of learning machines. And, let us advance the',\n",
       "  'understanding of the self-organization of associative database .',\n",
       "  '. Parameter Type',\n",
       "  \"An ordinary type of learning machine assumes an equation relating x's and y's with\",\n",
       "  'parameters being indefinite, namely, a structure of f. It is equivalent to define implicitly a',\n",
       "  'set F of candidates of',\n",
       "  '(F is some subset of mappings from X to Y.) And, it computes',\n",
       "  'values of the parameters based on the observed samples. We call such type a parameter',\n",
       "  'type.',\n",
       "  'For a learning machine defined well, if F 3 f, j approaches f as the number of samples',\n",
       "  'increases. In the alternative case, however, some estimation error remains eternally. Thus,',\n",
       "  'a problem of designing a learning machine returns to find out a proper structure of f in this',\n",
       "  'sense.',\n",
       "  'On the other hand, the assumed structure of f is demanded to be as compact as possible',\n",
       "  'to achieve a fast learning. In other words, the number of parameters should be small. Since,',\n",
       "  'if the parameters are few, some j can be uniquely determined even though the observed',\n",
       "  'samples are few. However, this demand of being proper contradicts to that of being compact.',\n",
       "  'Consequently, in the parameter type, the better the compactness of the assumed structure',\n",
       "  'that is proper, the better the learning machine. This is the most elementary conception',\n",
       "  'when we design learning machines .',\n",
       "  '',\n",
       "  '1.',\n",
       "  '',\n",
       "  '. Universality and Ordinary Neural Networks',\n",
       "  'Now suppose that a sufficient knowledge on f is given though J itself is unknown. In',\n",
       "  'this case, it is comparatively easy to find out proper and compact structures of J. In the',\n",
       "  'alternative case, however, it is sometimes difficult. A possible solution is to give up the',\n",
       "  \"compactness and assume an almighty structure that can cover various 1's. A combination\",\n",
       "  'of some orthogonal bases of the infinite dimension is such a structure. Neural networks 1 ,2',\n",
       "  'are its approximations obtained by truncating finitely the dimension for implementation.',\n",
       "  '',\n",
       "  '? American Institute of Physics 1988',\n",
       "  '',\n",
       "  '\\x0c768',\n",
       "  'A main topic in designing neural networks is to establish such desirable structures of 1.',\n",
       "  'This work includes developing practical procedures that compute values of coefficients from',\n",
       "  'the observed samples. Such discussions are :flourishing since 1980 while many efficient methods have been proposed. Recently, even hardware units computing coefficients in parallel',\n",
       "  'for speed-up are sold, e.g., ANZA, Mark III, Odyssey and E-1.',\n",
       "  'Nevertheless, in neural networks, there always exists a danger of some error remaining',\n",
       "  'eternally in estimating /. Precisely speaking, suppose that a combination of the bases of a',\n",
       "  'finite number can define a structure of 1 essentially. In other words, suppose that F 3 /, or',\n",
       "  '1 is located near F. In such case, the estimation error is none or negligible. However, if 1',\n",
       "  'is distant from F, the estimation error never becomes negligible. Indeed, many researches',\n",
       "  'report that the following situation appears when 1 is too complex. Once the estimation',\n",
       "  'error converges to some value (> 0) as the number of samples increases, it decreases hardly',\n",
       "  'even though the dimension is heighten. This property sometimes is a considerable defect of',\n",
       "  'neural networks .',\n",
       "  '. Recursi ve Type',\n",
       "  'The recursive type is founded on another methodology of learning that should be as',\n",
       "  'follows. At the initial stage of no sample, the set Fa (instead of notation F) of candidates',\n",
       "  'of I equals to the set of all mappings from X to Y. After observing the first sample',\n",
       "  '(Xl, Yl) E X x Y, Fa is reduced to Fi so that I(xt) = Yl for any I E F. After observing',\n",
       "  \"the second sample (X2' Y2) E X x Y, Fl is further reduced to F2 so that i(xt) = Yl and\",\n",
       "  'I(X2) = Y2 for any I E F. Thus, the candidate set F becomes gradually small as observation',\n",
       "  'of samples proceeds. The after observing i-samples, which we write',\n",
       "  'is one of the most',\n",
       "  'likelihood estimation of 1 selected in fi;. Hence, contrarily to the parameter type, the',\n",
       "  'recursive type guarantees surely that j approaches to 1 as the number of samples increases.',\n",
       "  'The recursive type, if observes a sample (x\" yd, rewrites values 1,-l(X),S to I,(x)\\'s for',\n",
       "  \"some x's correlated to the sample. Hence, this type has an architecture composed of a rule\",\n",
       "  'for rewriting and a free memory space. Such architecture forms naturally a kind of database',\n",
       "  'that builds up management systems of data in a self-organizing way. However, this database',\n",
       "  'differs from ordinary ones in the following sense. It does not only record the samples already',\n",
       "  'observed, but computes some estimation of l(x) for any x E X. We call such database an',\n",
       "  'associative database.',\n",
       "  'The first subject in constructing associative databases is how we establish the rule for',\n",
       "  'rewri ting. For this purpose, we adap t a measure called the dissimilari ty. Here, a dissimilari ty',\n",
       "  'means a mapping d : X x X -+ {reals > O} such that for any (x, x) E X x X, d(x, x) > 0',\n",
       "  'whenever l(x) t /(x). However, it is not necessarily defined with a single formula. It is',\n",
       "  'definable with, for example, a collection of rules written in forms of \"if? .. then?? .. \"',\n",
       "  'The dissimilarity d defines a structure of 1 locally in X x Y. Hence, even though',\n",
       "  'the knowledge on f is imperfect, we can re:flect it on d in some heuristic way. Hence,',\n",
       "  'contrarily to neural networks, it is possible to accelerate the speed of learning by establishing',\n",
       "  \"d well. Especially, we can easily find out simple d's for those l's which process analogically\",\n",
       "  \"information like a human. (See the applications in this paper.) And, for such /'s, the\",\n",
       "  'recursive type shows strongly its effectiveness.',\n",
       "  \"We denote a sequence of observed samples by (Xl, Yd, (X2' Y2),???. One of the simplest\",\n",
       "  'constructions of associative databases after observing i-samples (i = 1,2,.,,) is as follows.',\n",
       "  '',\n",
       "  'i',\n",
       "  '',\n",
       "  'i\"',\n",
       "  '',\n",
       "  'I,',\n",
       "  '',\n",
       "  'Algorithm 1. At the initial stage, let So be the empty set. For every i =',\n",
       "  '1,2\" .. , let i,-l(x) for any x E X equal some y* such that (x*,y*) E S,-l and',\n",
       "  '',\n",
       "  'd(x, x*) =',\n",
       "  '',\n",
       "  'min',\n",
       "  '(%,y)ES.-t',\n",
       "  '',\n",
       "  'd(x, x) .',\n",
       "  '',\n",
       "  'Furthermore, add (x\" y,) to S;-l to produce Sa, i.e., S, = S,_l U {(x\"',\n",
       "  '',\n",
       "  '(1)',\n",
       "  '',\n",
       "  'y,n.',\n",
       "  '',\n",
       "  '\\x0c769',\n",
       "  '',\n",
       "  'Another version improved to economize the memory is as follows.',\n",
       "  '',\n",
       "  'Algorithm 2, At the initial stage, let So be composed of an arbitrary element',\n",
       "  'in X x Y. For every i = 1,2\"\", let ii-lex) for any x E X equal some y. such',\n",
       "  'that (x?, y.) E Si-l and',\n",
       "  'd(x, x?) =',\n",
       "  '',\n",
       "  'min',\n",
       "  '',\n",
       "  'd(x, x) .',\n",
       "  '',\n",
       "  '(i,i)ES.-l',\n",
       "  '',\n",
       "  'Furthermore, if ii-l(Xi) # Yi then let Si = Si-l, or add (Xi, Yi) to Si-l to',\n",
       "  \"produce Si, i.e., Si = Si-l U {(Xi, Yi)}'\",\n",
       "  'In either construction, ii approaches to f as i increases. However, the computation time',\n",
       "  'grows proportionally to the size of Si. The second subject in constructing associative',\n",
       "  'databases is what addressing rule we should employ to economize the computation time. In',\n",
       "  'the subsequent chapters, a construction of associative database for this purpose is proposed.',\n",
       "  'It manages data in a form of binary tree.',\n",
       "  '',\n",
       "  'SELF-ORGANIZATION OF ASSOCIATIVE DATABASE',\n",
       "  'Given a sample sequence (Xl, Yl), (X2\\' Y2), .. \" the algorithm for constructing associative',\n",
       "  'database is as follows.',\n",
       "  '',\n",
       "  \"Algorithm 3,'\",\n",
       "  '',\n",
       "  'Step I(Initialization): Let (x[root], y[root]) = (Xl, Yd. Here, x[.] and y[.] are',\n",
       "  'variables assigned for respective nodes to memorize data.. Furthermore, let t = 1.',\n",
       "  'Step 2: Increase t by 1, and put x, in. After reset a pointer n to the root, repeat',\n",
       "  'the following until n arrives at some terminal node, i.e., leaf.',\n",
       "  'Notations nand',\n",
       "  'd(xt, x[n)), let n',\n",
       "  '',\n",
       "  'n mean the descendant nodes of n.',\n",
       "  '=n. Otherwise, let n =n.',\n",
       "  '',\n",
       "  'If d(x\" r[n)) ~',\n",
       "  '',\n",
       "  'Step 3: Display yIn] as the related information. Next, put y, in. If yIn] = y\" back',\n",
       "  'to step 2. Otherwise, first establish new descendant nodes n and n. Secondly,',\n",
       "  'let',\n",
       "  '',\n",
       "  '(x[n], yIn))',\n",
       "  '(x[n], yIn))',\n",
       "  '',\n",
       "  '(x[n], yIn)),',\n",
       "  '(Xt, y,).',\n",
       "  '',\n",
       "  '(2)',\n",
       "  '(3)',\n",
       "  '',\n",
       "  'Finally, back to step 2. Here, the loop of step 2-3 can be stopped at any time',\n",
       "  'and also can be continued.',\n",
       "  'Now, suppose that gate elements, namely, artificial \"synapses\" that play the role of branching by d are prepared. Then, we obtain a new style of neural network with gate elements',\n",
       "  'being randomly connected by this algorithm.',\n",
       "  '',\n",
       "  'LETTER RECOGNITION',\n",
       "  'Recen tly, the vertical slitting method for recognizing typographic English letters3 , the',\n",
       "  'elastic matching method for recognizing hand written discrete English letters4 , the global',\n",
       "  'training and fuzzy logic search method for recognizing Chinese characters written in square',\n",
       "  'styleS, etc. are published. The self-organization of associative database realizes the recognition of handwritten continuous English letters.',\n",
       "  '',\n",
       "  '\\x0c770',\n",
       "  '',\n",
       "  '9 /wn\"',\n",
       "  '',\n",
       "  'NOV',\n",
       "  '',\n",
       "  '~ ~ ~ -xk :La.t',\n",
       "  '',\n",
       "  '~~ ~ ~~~',\n",
       "  '',\n",
       "  \"dw1lo'\",\n",
       "  '',\n",
       "  '~~~~~of~~',\n",
       "  '',\n",
       "  '~~~ 4,-?~~4Fig. 1. Source document.',\n",
       "  \"2~~---------------'\",\n",
       "  '',\n",
       "  \"lOO~---------------'\",\n",
       "  '',\n",
       "  'H',\n",
       "  '',\n",
       "  'o',\n",
       "  '',\n",
       "  'o',\n",
       "  'Fig. 2. Windowing.',\n",
       "  '',\n",
       "  '1000',\n",
       "  '',\n",
       "  '2000',\n",
       "  '',\n",
       "  '3000',\n",
       "  '',\n",
       "  '4000',\n",
       "  '',\n",
       "  'Number of samples',\n",
       "  '',\n",
       "  'o',\n",
       "  '',\n",
       "  '1000',\n",
       "  '',\n",
       "  '2000',\n",
       "  '',\n",
       "  '3000',\n",
       "  '',\n",
       "  '4000',\n",
       "  '',\n",
       "  'NUAlber of sampl es',\n",
       "  '',\n",
       "  'Fig. 3. An experiment result.',\n",
       "  '',\n",
       "  'An image scanner takes a document image (Fig. 1). The letter recognizer uses a parallelogram window that at least can cover the maximal letter (Fig. 2), and processes the',\n",
       "  'sequence of letters while shifting the window. That is, the recognizer scans a word in a',\n",
       "  'slant direction. And, it places the window so that its left vicinity may be on the first black',\n",
       "  'point detected. Then, the window catches a letter and some part of the succeeding letter.',\n",
       "  'If recognition of the head letter is performed, its end position, namely, the boundary line',\n",
       "  'between two letters becomes known. Hence, by starting the scanning from this boundary',\n",
       "  'and repeating the above operations, the recognizer accomplishes recursively the task. Thus',\n",
       "  'the major problem comes to identifying the head letter in the window.',\n",
       "  'Considering it, we define the following.',\n",
       "  \"? Regard window images as x's, and define X accordingly.\",\n",
       "  '? For a (x, x) E X x X, denote by B a black point in the left area from the boundary on',\n",
       "  'window image X. Project each B onto window image x. Then, measure the Euclidean',\n",
       "  'distance 6 between fj and a black point B on x being the closest to B. Let d(x, x) be',\n",
       "  \"the summation of 6's for all black points B's on x divided by the number of B's.\",\n",
       "  '? Regard couples of the \"reading\" and the position of boundary as y\\'s, and define Y',\n",
       "  'accordingly.',\n",
       "  'An operator teaches the recognizer in interaction the relation between window image and',\n",
       "  'reading& boundary with algorithm 3. Precisely, if the recalled reading is incorrect, the',\n",
       "  'operator teaches a correct reading via the console. Moreover, if the boundary position is',\n",
       "  'incorrect, he teaches a correct position via the mouse.',\n",
       "  'Fig. 1 shows partially a document image used in this experiment. Fig. 3 shows the',\n",
       "  'change of the number of nodes and that of the recognition rate defined as the relative',\n",
       "  'frequency of correct answers in the past 1000 trials. Speciiications of the window are height',\n",
       "  '= 20dot, width = 10dot, and slant angular = 68deg. In this example, the levels of tree',\n",
       "  'were distributed in 6-19 at time 4000 and the recognition rate converged to about 74%.',\n",
       "  'Experimentally, the recognition rate converges to about 60-85% in most cases, and to 95% at',\n",
       "  'a rare case. However, it does not attain 100% since, e.g., \"c\" and \"e\" are not distinguishable',\n",
       "  'because of excessive lluctuation in writing. If the consistency of the x, y-relation is not',\n",
       "  'assured like this, the number of nodes increases endlessly (d. Fig. 3). Hence, it is clever to',\n",
       "  'stop the learning when the recognition rate attains some upper limit. To improve further',\n",
       "  'the recognition rate, we must consider the spelling of words. It is one of future subjects.',\n",
       "  '',\n",
       "  '\\x0c771',\n",
       "  '',\n",
       "  'OBSTACLE AVOIDING MOVEMENT',\n",
       "  'Various systems of camera type autonomous mobile robot are reported flourishingly6-1O.',\n",
       "  'The system made up by the authors (Fig. 4) also belongs to this category. Now, in mathematical methodologies, we solve usually the problem of obstacle avoiding movement as',\n",
       "  'a cost minimization problem under some cost criterion established artificially. Contrarily,',\n",
       "  'the self-organization of associative database reproduces faithfully the cost criterion of an',\n",
       "  'operator. Therefore, motion of the robot after learning becomes very natural.',\n",
       "  'Now, the length, width and height of the robot are all about O.7m, and the weight is',\n",
       "  'about 30kg. The visual angle of camera is about 55deg. The robot has the following three',\n",
       "  'factors of motion. It turns less than ?30deg, advances less than 1m, and controls speed less',\n",
       "  'than 3km/h. The experiment was done on the passageway of wid th 2.5m inside a building',\n",
       "  \"which the authors' laboratories exist in (Fig. 5). Because of an experimental intention, we\",\n",
       "  'arrange boxes, smoking stands, gas cylinders, stools, handcarts, etc. on the passage way at',\n",
       "  'random. We let the robot take an image through the camera, recall a similar image, and',\n",
       "  'trace the route preliminarily recorded on it. For this purpose, we define the following.',\n",
       "  '? Let the camera face 28deg downward to take an image, and process it through a low',\n",
       "  'pass filter. Scanning vertically the filtered image from the bottom to the top, search',\n",
       "  'the first point C where the luminance changes excessively. Then, su bstitu te all points',\n",
       "  'from the bottom to C for white, and all points from C to the top for black (Fig. 6).',\n",
       "  \"(If no obstacle exists just in front of the robot, the white area shows the ''free'' area\",\n",
       "  'where the robot can move around.) Regard binary 32 x 32dot images processed thus',\n",
       "  \"as x's, and define X accordingly.\",\n",
       "  '? For every (x, x) E X x X, let d(x, x) be the number of black points on the exclusive-or',\n",
       "  'image between x and X.',\n",
       "  \"? Regard as y's the images obtained by drawing routes on images x's, and define Y\",\n",
       "  'accordingly.',\n",
       "  'The robot superimposes, on the current camera image x, the route recalled for x, and',\n",
       "  'inquires the operator instructions. The operator judges subjectively whether the suggested',\n",
       "  'route is appropriate or not. In the negative answer, he draws a desirable route on x with the',\n",
       "  'mouse to teach a new y to the robot. This opera.tion defines implicitly a sample sequence',\n",
       "  'of (x, y) reflecting the cost criterion of the operator.',\n",
       "  '',\n",
       "  '.::l\" !',\n",
       "  '-',\n",
       "  '',\n",
       "  'IibUBe',\n",
       "  '',\n",
       "  '_. -',\n",
       "  '',\n",
       "  '22',\n",
       "  '',\n",
       "  '11',\n",
       "  '',\n",
       "  'Roan',\n",
       "  '',\n",
       "  '12',\n",
       "  '',\n",
       "  '{-',\n",
       "  '',\n",
       "  '13',\n",
       "  '',\n",
       "  'Stationary uni t',\n",
       "  '',\n",
       "  'Fig. 4. Configuration of',\n",
       "  'autonomous mobile robot system.',\n",
       "  '',\n",
       "  '~',\n",
       "  '',\n",
       "  'I',\n",
       "  '',\n",
       "  ',',\n",
       "  '',\n",
       "  '23',\n",
       "  '',\n",
       "  '24',\n",
       "  '',\n",
       "  'North',\n",
       "  '14',\n",
       "  '',\n",
       "  'rmbi Ie unit (robot)',\n",
       "  '',\n",
       "  '-',\n",
       "  '',\n",
       "  'Roan',\n",
       "  '',\n",
       "  'y',\n",
       "  '',\n",
       "  't',\n",
       "  '',\n",
       "  'Fig. 5. Experimental',\n",
       "  'environment.',\n",
       "  '',\n",
       "  '\\x0c772',\n",
       "  '',\n",
       "  'Wall',\n",
       "  '',\n",
       "  'Camera image',\n",
       "  '',\n",
       "  'Preprocessing',\n",
       "  '',\n",
       "  'A',\n",
       "  '',\n",
       "  '::: !fa',\n",
       "  '',\n",
       "  '?',\n",
       "  '',\n",
       "  'Preprocessing',\n",
       "  '',\n",
       "  '0',\n",
       "  '',\n",
       "  'O',\n",
       "  '',\n",
       "  'Course',\n",
       "  'suggest ion',\n",
       "  '',\n",
       "  '??',\n",
       "  '',\n",
       "  '..',\n",
       "  '',\n",
       "  'Search',\n",
       "  '',\n",
       "  'A',\n",
       "  '',\n",
       "  'Fig. 6. Processing for',\n",
       "  'obstacle avoiding movement.',\n",
       "  '',\n",
       "  'x',\n",
       "  '',\n",
       "  'Fig. 1. Processing for',\n",
       "  'position identification.',\n",
       "  'We define the satisfaction rate by the relative frequency of acceptable suggestions of',\n",
       "  'route in the past 100 trials. In a typical experiment, the change of satisfaction rate showed',\n",
       "  'a similar tendency to Fig. 3, and it attains about 95% around time 800. Here, notice that',\n",
       "  'the rest 5% does not mean directly the percentage of collision. (In practice, we prevent the',\n",
       "  'collision by adopting some supplementary measure.) At time 800, the number of nodes was',\n",
       "  '145, and the levels of tree were distributed in 6-17.',\n",
       "  'The proposed method reflects delicately various characters of operator. For example, a',\n",
       "  'robot trained by an operator 0 moves slowly with enough space against obstacles while one',\n",
       "  \"trained by another operator 0' brushes quickly against obstacles. This fact gives us a hint\",\n",
       "  'on a method of printing \"characters\" into machines.',\n",
       "  'POSITION IDENTIFICATION',\n",
       "  'The robot can identify its position by recalling a similar landscape with the position data',\n",
       "  'to a camera image. For this purpose, in principle, it suffices to regard camera images and',\n",
       "  \"position data as x's and y's, respectively. However, the memory capacity is finite in actual\",\n",
       "  'compu ters. Hence, we cannot but compress the camera images at a slight loss of information.',\n",
       "  'Such compression is admittable as long as the precision of position identification is in an',\n",
       "  'acceptable area. Thus, the major problem comes to find out some suitable compression',\n",
       "  'method.',\n",
       "  'In the experimental environment (Fig. 5), juts are on the passageway at intervals of',\n",
       "  '3.6m, and each section between adjacent juts has at most one door. The robot identifies',\n",
       "  'roughly from a surrounding landscape which section itself places in. And, it uses temporarily',\n",
       "  'a triangular surveying technique if an exact measure is necessary. To realize the former task,',\n",
       "  'we define the following .',\n",
       "  '? Turn the camera to take a panorama image of 360deg. Scanning horizontally the',\n",
       "  'center line, substitute the points where the luminance excessively changes for black',\n",
       "  'and the other points for white (Fig. 1). Regard binary 360dot line images processed',\n",
       "  \"thus as x's, and define X accordingly.\",\n",
       "  '? For every (x, x) E X x X, project each black point A on x onto x. And, measure the',\n",
       "  'Euclidean distance 6 between A and a black point A on x being the closest to A. Let',\n",
       "  'the summation of 6 be S. Similarly, calculate S by exchanging the roles of x and X.',\n",
       "  \"Denoting the numbers of A's and A's respectively by nand n, define\",\n",
       "  '',\n",
       "  '\\x0c773',\n",
       "  '',\n",
       "  'd(x, x) =',\n",
       "  '',\n",
       "  '~(~',\n",
       "  '+ ~).',\n",
       "  '2 n',\n",
       "  'n',\n",
       "  '',\n",
       "  '(4)',\n",
       "  '',\n",
       "  \"? Regard positive integers labeled on sections as y's (cf. Fig. 5), and define Y accordingly.\",\n",
       "  'In the learning mode, the robot checks exactly its position with a counter that is reset periodically by the operator. The robot runs arbitrarily on the passageways within 18m area',\n",
       "  'and learns the relation between landscapes and position data. (Position identification beyond 18m area is achieved by crossing plural databases one another.) This task is automatic',\n",
       "  'excepting the periodic reset of counter, namely, it is a kind of learning without teacher.',\n",
       "  'We define the identification rate by the relative frequency of correct recalls of position',\n",
       "  'data in the past 100 trials. In a typical example, it converged to about 83% around time',\n",
       "  '400. At time 400, the number of levels was 202, and the levels oftree were distributed in 522. Since the identification failures of 17% can be rejected by considering the trajectory, no',\n",
       "  'pro blem arises in practical use. In order to improve the identification rate, the compression',\n",
       "  'ratio of camera images must be loosened. Such possibility depends on improvement of the',\n",
       "  'hardware in the future.',\n",
       "  'Fig. 8 shows an example of actual motion of the robot based on the database for obstacle',\n",
       "  'avoiding movement and that for position identification. This example corresponds to a case',\n",
       "  'of moving from 14 to 23 in Fig. 5. Here, the time interval per frame is about 40sec.',\n",
       "  '',\n",
       "  ',~. .~ (',\n",
       "  ';~\"i..',\n",
       "  '~',\n",
       "  '',\n",
       "  '\"',\n",
       "  '',\n",
       "  '\"',\n",
       "  '',\n",
       "  '.',\n",
       "  '',\n",
       "  '..I',\n",
       "  '',\n",
       "  'I',\n",
       "  '',\n",
       "  '?',\n",
       "  '?',\n",
       "  '',\n",
       "  '\"',\n",
       "  '',\n",
       "  \"I'\",\n",
       "  '.',\n",
       "  \"'.1\",\n",
       "  't',\n",
       "  '',\n",
       "  ';',\n",
       "  '',\n",
       "  'i',\n",
       "  '',\n",
       "  '-:',\n",
       "  \", . . , 'II\",\n",
       "  '',\n",
       "  'Fig. 8. Actual motion of the robot.',\n",
       "  '',\n",
       "  '\\x0c774',\n",
       "  '',\n",
       "  'CONCLUSION',\n",
       "  'A method of self-organizing associative databases was proposed with the application to',\n",
       "  'robot eyesight systems. The machine decomposes a global structure unknown into a set of',\n",
       "  'local structures known and learns universally any input-output response. This framework',\n",
       "  'of problem implies a wide application area other than the examples shown in this paper.',\n",
       "  'A defect of the algorithm 3 of self-organization is that the tree is balanced well only',\n",
       "  'for a subclass of structures of f. A subject imposed us is to widen the class. A probable',\n",
       "  'solution is to abolish the addressing rule depending directly on values of d and, instead, to',\n",
       "  'establish another rule depending on the distribution function of values of d. It is now under',\n",
       "  'investigation.',\n",
       "  '',\n",
       "  'REFERENCES',\n",
       "  '1. Hopfield, J. J. and D. W. Tank, \"Computing with Neural Circuit: A Model/\\'',\n",
       "  '',\n",
       "  'Science 233 (1986), pp. 625-633.',\n",
       "  '2. Rumelhart, D. E. et al., \"Learning Representations by Back-Propagating Errors,\" Nature 323 (1986), pp. 533-536.',\n",
       "  '',\n",
       "  '3. Hull, J. J., \"Hypothesis Generation in a Computational Model for Visual Word',\n",
       "  'Recognition,\" IEEE Expert, Fall (1986), pp. 63-70.',\n",
       "  '4. Kurtzberg, J. M., \"Feature Analysis for Symbol Recognition by Elastic Matching,\" IBM J. Res. Develop. 31-1 (1987), pp. 91-95.',\n",
       "  '',\n",
       "  '5. Wang, Q. R. and C. Y. Suen, \"Large Tree Classifier with Heuristic Search and',\n",
       "  'Global Training,\" IEEE Trans. Pattern. Anal. & Mach. Intell. PAMI 9-1',\n",
       "  '(1987) pp. 91-102.',\n",
       "  '6. Brooks, R. A. et al, \"Self Calibration of Motion and Stereo Vision for Mobile',\n",
       "  'Robots,\" 4th Int. Symp. of Robotics Research (1987), pp. 267-276.',\n",
       "  '7. Goto, Y. and A. Stentz, \"The CMU System for Mobile Robot Navigation,\" 1987',\n",
       "  'IEEE Int. Conf. on Robotics & Automation (1987), pp. 99-105.',\n",
       "  '8. Madarasz, R. et al., \"The Design of an Autonomous Vehicle for the Disabled,\"',\n",
       "  'IEEE Jour. of Robotics & Automation RA 2-3 (1986), pp. 117-125.',\n",
       "  '9. Triendl, E. and D. J. Kriegman, \"Stereo Vision and Navigation within Buildings,\" 1987 IEEE Int. Conf. on Robotics & Automation (1987), pp. 1725-1730.',\n",
       "  '10. Turk, M. A. et al., \"Video Road-Following for the Autonomous Land Vehicle,\"',\n",
       "  '1987 IEEE Int. Conf. on Robotics & Automation (1987), pp. 273-279.',\n",
       "  '',\n",
       "  '\\x0c'],\n",
       " ['683',\n",
       "  '',\n",
       "  'A MEAN FIELD THEORY OF LAYER IV OF VISUAL CORTEX',\n",
       "  'AND ITS APPLICATION TO ARTIFICIAL NEURAL NETWORKS*',\n",
       "  'Christopher L. Scofield',\n",
       "  'Center for Neural Science and Physics Department',\n",
       "  'Brown University',\n",
       "  'Providence, Rhode Island 02912',\n",
       "  'and',\n",
       "  'Nestor, Inc., 1 Richmond Square, Providence, Rhode Island,',\n",
       "  '02906.',\n",
       "  'ABSTRACT',\n",
       "  'A single cell theory for the development of selectivity and',\n",
       "  'ocular dominance in visual cortex has been presented previously',\n",
       "  'by Bienenstock, Cooper and Munrol. This has been extended to a',\n",
       "  'network applicable to layer IV of visual cortex 2 . In this paper',\n",
       "  'we present a mean field approximation that captures in a fairly',\n",
       "  'transparent manner the qualitative, and many of the',\n",
       "  'quantitative, results of the network theory. Finally, we consider',\n",
       "  'the application of this theory to artificial neural networks and',\n",
       "  'show that a significant reduction in architectural complexity is',\n",
       "  'possible.',\n",
       "  'A SINGLE LAYER NETWORK AND THE MEAN FIELD',\n",
       "  'APPROXIMATION',\n",
       "  'We consider a single layer network of ideal neurons which',\n",
       "  'receive signals from outside of the layer and from cells within',\n",
       "  'the layer (Figure 1). The activity of the ith cell in the network is',\n",
       "  'c\\'1 -- m\\'1 d + \"\"\\'',\n",
       "  \"~ T .. c'\",\n",
       "  \"~J J'\",\n",
       "  '',\n",
       "  'J',\n",
       "  '',\n",
       "  '(1)',\n",
       "  '',\n",
       "  'Each cell',\n",
       "  'd is a vector of afferent signals to the network.',\n",
       "  'receives input from n fibers outside of the cortical network',\n",
       "  \"through the matrix of synapses mi' Intra-layer input to each cell\",\n",
       "  'is then transmitted through the matrix of cortico-cortical',\n",
       "  'synapses L.',\n",
       "  '? American Institute of Physics 1988',\n",
       "  '',\n",
       "  '\\x0c684',\n",
       "  '',\n",
       "  'Afferent',\n",
       "  'Signals',\n",
       "  '',\n",
       "  '>',\n",
       "  '',\n",
       "  '... ..',\n",
       "  '',\n",
       "  'm2',\n",
       "  '',\n",
       "  'm1',\n",
       "  '',\n",
       "  'mn',\n",
       "  '',\n",
       "  '~',\n",
       "  '',\n",
       "  'r;.',\n",
       "  '',\n",
       "  '\",...-',\n",
       "  '',\n",
       "  'd',\n",
       "  '',\n",
       "  '.L',\n",
       "  ':',\n",
       "  '',\n",
       "  '1',\n",
       "  '',\n",
       "  ',~',\n",
       "  '',\n",
       "  '2',\n",
       "  '',\n",
       "  '... ..',\n",
       "  '',\n",
       "  ', ...c.. ,',\n",
       "  '',\n",
       "  '~',\n",
       "  '',\n",
       "  '~',\n",
       "  '',\n",
       "  'Figure 1: The general single layer recurrent',\n",
       "  'network.',\n",
       "  'Light circles are the LGN -cortical',\n",
       "  'synapses.',\n",
       "  'Dark circles are the (nonmodifiable) cortico-cortical synapses.',\n",
       "  'We now expand the response of the i th cell into individual',\n",
       "  'terms describing the number of cortical synapses traversed by',\n",
       "  'the signal d before arriving through synapses Lij at cell i.',\n",
       "  'Expanding Cj in (1), the response of cell i becomes',\n",
       "  'ci',\n",
       "  '',\n",
       "  '=mi d + l: ~j mj d + l: ~jL Ljk mk d + 2: ~j 2Ljk L Lkn mn d +... (2)',\n",
       "  'J',\n",
       "  '',\n",
       "  'J',\n",
       "  '',\n",
       "  'K',\n",
       "  '',\n",
       "  'J',\n",
       "  '',\n",
       "  \"K' n\",\n",
       "  '',\n",
       "  'Note that each term contains a factor of the form',\n",
       "  '',\n",
       "  'This factor describes the first order effect, on cell q, of the',\n",
       "  'cortical transformation of the signal d.',\n",
       "  'The mean field',\n",
       "  'approximation consists of estimating this factor to be a constant,',\n",
       "  'independant of cell location',\n",
       "  '(3)',\n",
       "  '',\n",
       "  '\\x0c685',\n",
       "  '',\n",
       "  'This assumption does not imply that each cell in the network is',\n",
       "  'selective to the same pattern, (and thus that mi = mj). Rather,',\n",
       "  'the assumption is that the vector sum is a constant',\n",
       "  '',\n",
       "  'This amounts to assuming that each cell in the network is',\n",
       "  'surrounded by a population of cells which represent, on average,',\n",
       "  'all possible pattern preferences.',\n",
       "  'Thus the vector sum of the',\n",
       "  'afferent synaptic states describing these pattern preferences is a',\n",
       "  'constant independent of location.',\n",
       "  'Finally, if we assume that the lateral connection strengths are',\n",
       "  'a function only of i-j then Lij becomes a circular matrix so that',\n",
       "  '',\n",
       "  'r. Lij ::: ~J Lji = Lo = constan t.',\n",
       "  '1',\n",
       "  '',\n",
       "  'Then the response of the cell i becomes',\n",
       "  '(4)',\n",
       "  '',\n",
       "  'for I',\n",
       "  '',\n",
       "  '~',\n",
       "  '',\n",
       "  'I <1',\n",
       "  '',\n",
       "  'where we define the spatial average of cortical cell activity C = in',\n",
       "  'd, and N is the average number of intracortical synapses.',\n",
       "  'Here, in a manner similar to that in the theory of magnetism,',\n",
       "  'we have replaced the effect of individual cortical cells by their',\n",
       "  'average effect (as though all other cortical cells can be replaced',\n",
       "  \"by an 'effective' cell, figure 2). Note that we have retained all\",\n",
       "  'orders of synaptic traversal of the signal d.',\n",
       "  'Thus, we now focus on the activity of the layer after',\n",
       "  \"'relaxation' to equilibrium. In the mean field approximation we\",\n",
       "  'can therefore write',\n",
       "  '(5)',\n",
       "  '',\n",
       "  'where the mean field',\n",
       "  '',\n",
       "  'a',\n",
       "  'with',\n",
       "  '',\n",
       "  '=am',\n",
       "  '',\n",
       "  '\\x0c686',\n",
       "  '',\n",
       "  'and we asume that',\n",
       "  'inhibitory).',\n",
       "  '',\n",
       "  'Afferent',\n",
       "  'Signals',\n",
       "  'd',\n",
       "  '',\n",
       "  'Lo < 0 (the network is,',\n",
       "  '',\n",
       "  'on',\n",
       "  '',\n",
       "  'average,',\n",
       "  '',\n",
       "  '>',\n",
       "  '',\n",
       "  'Figure 2: The single layer mean field network.',\n",
       "  'Detailed connectivity between all cells of the',\n",
       "  \"network is replaced with a single (nonmodifiable) synapse from an 'effective' cell.\",\n",
       "  'LEARNING IN THE CORTICAL NETWORK',\n",
       "  '',\n",
       "  'We will first consider evolution of the network according to a',\n",
       "  'synaptic modification rule that has been studied in detail, for',\n",
       "  'single cells, elsewhere!? 3.',\n",
       "  'We consider the LGN - cortical',\n",
       "  'synapses to be the site of plasticity and assume for maximum',\n",
       "  'simplicity that there is no modification of cortico-cortical',\n",
       "  'synapses. Then',\n",
       "  '(6)',\n",
       "  '',\n",
       "  '.',\n",
       "  '',\n",
       "  'Lij = O.',\n",
       "  'In what follows c denotes the spatial average over cortical cells,',\n",
       "  'while Cj denotes the time averaged activity of the i th cortical cell.',\n",
       "  'The function cj> has been discussed extensively elsewhere.',\n",
       "  'Here',\n",
       "  'we note that cj> describes a function of the cell response that has',\n",
       "  'both hebbian and anti-hebbian regions.',\n",
       "  '',\n",
       "  '\\x0c687',\n",
       "  '',\n",
       "  'This leads to a very complex set of non-linear stochastic',\n",
       "  'equations that have been analyzed partially elsewhere 2 . In',\n",
       "  'general, the afferent synaptic state has fixed points that are',\n",
       "  'stable and selective and unstable fixed points that are nonselective!, 2. These arguments may now be generalized for the',\n",
       "  'network. In the mean field approximation',\n",
       "  '(7)',\n",
       "  '',\n",
       "  'The mean field, a has a time dependent component m. This',\n",
       "  'varies as the average over all of the network modifiable',\n",
       "  'synapses and, in most environmental situations, should change',\n",
       "  'slowly compared to the change of the modifiable synapses to a',\n",
       "  'single cell. Then in this approximation we can write',\n",
       "  '',\n",
       "  '?',\n",
       "  '',\n",
       "  '(mi(a)-a) = cj>[mi(a) - a] d.',\n",
       "  '',\n",
       "  '(8)',\n",
       "  '',\n",
       "  'We see that there is a mapping',\n",
       "  \"mi' <-> mica) - a\",\n",
       "  '',\n",
       "  '(9)',\n",
       "  '',\n",
       "  'such that for every mj(a) there exists a corresponding (mapped)',\n",
       "  \"point mj' which satisfies\",\n",
       "  '',\n",
       "  'the original equation for the mean field zero theory. It can be',\n",
       "  'shown 2, 4 that for every fixed point of mj( a = 0), there exists a',\n",
       "  'corresponding fixed point mj( a) with the same selectivity and',\n",
       "  'stability properties.',\n",
       "  'The fixed points are available to the',\n",
       "  'neurons if there is sufficient inhibition in the network (ILo I is',\n",
       "  'sufficiently large).',\n",
       "  'APPLICATION OF THE MEAN FIELD NETWORK TO',\n",
       "  'LAYER IV OF VISUAL CORTEX',\n",
       "  'Neurons in the primary visual cortex of normal adult cats are',\n",
       "  'sharply tuned for the orientation of an elongated slit of light and',\n",
       "  'most are activated by stimulation of either eye. Both of these',\n",
       "  'properties--orientation selectivity and binocularity--depend on',\n",
       "  'the type of visual environment experienced during a critical',\n",
       "  '',\n",
       "  '\\x0c688',\n",
       "  '',\n",
       "  'period of early postnatal development. For example, deprivation',\n",
       "  'of patterned input during this critical period leads to loss of',\n",
       "  'orientation selectivity while monocular deprivation (MD) results',\n",
       "  'in a dramatic shift in the ocular dominance of cortical neurons',\n",
       "  'such that most will be responsive exclusively to the open eye.',\n",
       "  'The ocular dominance shift after MD is the best known and most',\n",
       "  'intensively studied type of visual cortical plasticity.',\n",
       "  'The behavior of visual cortical cells in various rearing',\n",
       "  'conditions suggests that some cells respond more rapidly to',\n",
       "  'environmental changes than others.',\n",
       "  'In monocular deprivation,',\n",
       "  'for example, some cells remain responsive to the closed eye in',\n",
       "  'spite of the very large shift of most cells to the open eye- Singer',\n",
       "  'et. al. 5 found, using intracellular recording, that geniculo-cortical',\n",
       "  'synapses on inhibitory interneurons are more resistant to',\n",
       "  'monocular deprivation than are synapses on pyramidal cell',\n",
       "  'dendrites. Recent work suggests that the density of inhibitory',\n",
       "  'GABAergic synapses in kitten striate cortex is also unaffected by',\n",
       "  'MD during the cortical period 6, 7.',\n",
       "  'These results suggest that some LGN -cortical synapses modify',\n",
       "  'rapidly, while others modify relatively slowly, with slow',\n",
       "  'modification of some cortico-cortical synapses. Excitatory LGNcortical synapses into excitatory cells may be those that modify',\n",
       "  'primarily.',\n",
       "  'To embody these facts we introduce two types of',\n",
       "  'LGN -cortical synapses:',\n",
       "  'those (mj) that modify and those (Zk)',\n",
       "  'that remain relatively constant. In a simple limit we have',\n",
       "  '',\n",
       "  'and',\n",
       "  '',\n",
       "  '(10)',\n",
       "  '',\n",
       "  'We assume for simplicity and consistent with the above',\n",
       "  'physiological interpretation that these two types of synapses are',\n",
       "  'confined to two different classes of cells and that both left and',\n",
       "  'right eye have similar synapses (both m i or both Zk) on a given',\n",
       "  'cell. Then, for binocular cells, in the mean field approximation',\n",
       "  '(where binocular terms are in italics)',\n",
       "  '',\n",
       "  '\\x0c689',\n",
       "  '',\n",
       "  'where dl(r) are the explicit left (right) eye time averaged signals',\n",
       "  'arriving form the LGN.',\n",
       "  'Note that a1(r) contain terms from',\n",
       "  'modifiable and non-modifiable synapses:',\n",
       "  'al(r) =',\n",
       "  '',\n",
       "  'a (ml(r) + zl(r?).',\n",
       "  '',\n",
       "  'Under conditions of monocular deprivation, the animal is reared',\n",
       "  'with one eye closed. For the sake of analysis assume that the',\n",
       "  'right eye is closed and that only noise-like signals arrive at',\n",
       "  'cortex from the right eye. Then the environment of the cortical',\n",
       "  'cells is:',\n",
       "  'd = (di, n)',\n",
       "  '',\n",
       "  '(12)',\n",
       "  '',\n",
       "  'Further, assume that the left eye synapses have reached their',\n",
       "  '1',\n",
       "  '',\n",
       "  'r',\n",
       "  '',\n",
       "  \"selective fixed point, selective to pattern d 1 ? Then (mi' m i )\",\n",
       "  '(m:*, xi) with IXil ?lm!*1.',\n",
       "  'linear analysis of the',\n",
       "  'the closed eye',\n",
       "  '',\n",
       "  '<I> -',\n",
       "  '',\n",
       "  '=',\n",
       "  '',\n",
       "  'Following the methods of BCM, a local',\n",
       "  'function is employed to show that for',\n",
       "  '',\n",
       "  'Xi =',\n",
       "  '',\n",
       "  'a (1 - }..a)-li.r.',\n",
       "  '',\n",
       "  '(13)',\n",
       "  '',\n",
       "  'where A. = NmIN is the ratio of the number modifiable cells to the',\n",
       "  'total number of cells in the network. That is, the asymptotic',\n",
       "  'state of the closed eye synapses is a scaled function of the meanfield due to non-modifiable (inhibitory) cortical cells. The scale',\n",
       "  'of this state is set not only by the proportion of non-modifiable',\n",
       "  'cells, but in addition, by the averaged intracortical synaptic',\n",
       "  'strength Lo.',\n",
       "  'Thus contrasted with the mean field zero theory the deprived',\n",
       "  'eye LGN-cortical synapses do not go to zero.',\n",
       "  'Rather they',\n",
       "  'approach the constant value dependent on the average inhibition',\n",
       "  'produced by the non-modifiable cells in such a way that the',\n",
       "  'asymptotic output of the cortical cell is zero (it cannot be driven',\n",
       "  'by the deprived eye). However lessening the effect of inhibitory',\n",
       "  'synapses (e.g. by application of an inhibitory blocking agent such',\n",
       "  'as bicuculine) reduces the magnitude of a so that one could once',\n",
       "  'more obtain a response from the deprived eye.',\n",
       "  '',\n",
       "  '\\x0c690',\n",
       "  '',\n",
       "  'We find, consistent with previous theory and experiment,',\n",
       "  'that most learning can occur in the LGN-cortical synapse, for',\n",
       "  'inhibitory (cortico-cortical) synapses need not modify.',\n",
       "  'Some',\n",
       "  'non-modifiable LGN-cortical synapses are required.',\n",
       "  'THE MEAN FIELD APPROXIMATION AND',\n",
       "  'ARTIFICIAL NEURAL NETWORKS',\n",
       "  'The mean field approximation may be applied to networks in',\n",
       "  'which the cortico-cortical feedback is a general function of cell',\n",
       "  'activity. In particular, the feedback may measure the difference',\n",
       "  'between the network activity and memories of network activity.',\n",
       "  'In this way, a network may be used as a content addressable',\n",
       "  'memory.',\n",
       "  'We have been discussing the properties of a mean',\n",
       "  'field network after equilibrium has been reached. We now focus',\n",
       "  'on the detailed time dependence of the relaxation of the cell',\n",
       "  'activity to a state of equilibrium.',\n",
       "  'Hopfield8 introduced a simple formalism for the analysis of',\n",
       "  'the time dependence of network activity.',\n",
       "  'In this model,',\n",
       "  'network activity is mapped onto a physical system in which the',\n",
       "  \"state of neuron activity is considered as a 'particle' on a potential\",\n",
       "  'energy surface.',\n",
       "  'Identification of the pattern occurs when the',\n",
       "  \"activity 'relaxes' to a nearby minima of the energy.\",\n",
       "  'Thus',\n",
       "  'mlmma are employed as the sites of memories. For a Hopfield',\n",
       "  'network of N neurons, the intra-layer connectivity required is of',\n",
       "  'order N2. This connectivity is a significant constraint on the',\n",
       "  'practical implementation of such systems for large scale',\n",
       "  'problems. Further, the Hopfield model allows a storage capacity',\n",
       "  'which is limited to m < N memories 8, 9. This is a result of the',\n",
       "  \"proliferation of unwanted local minima in the 'energy' surface.\",\n",
       "  'Recently, Bachmann et al. l 0, have proposed a model for the',\n",
       "  'relaxation of network activity in which memories of activity',\n",
       "  \"patterns are the sites of negative 'charges', and the activity\",\n",
       "  \"caused by a test pattern is a positive test 'charge'. Then in this\",\n",
       "  'model, the energy function is the electrostatic energy of the',\n",
       "  '(unit) test charge with the collection of charges at the memory',\n",
       "  'sites',\n",
       "  '',\n",
       "  'E = -IlL ~ Qj I J-l- Xj I - L,',\n",
       "  'J',\n",
       "  '',\n",
       "  '(14)',\n",
       "  '',\n",
       "  '\\x0c691',\n",
       "  '',\n",
       "  'where Jl (0) is a vector describing the initial network activity',\n",
       "  \"caused by a test pattern, and Xj' the site of the jth memory. L is\",\n",
       "  'a parameter related to the network size.',\n",
       "  'This model has the advantage that storage density is not',\n",
       "  'restricted by the the network size as it is in the Hopfield model,',\n",
       "  'and in addition, the architecture employs a connectivity of order',\n",
       "  'm x N.',\n",
       "  'Note that at each stage in the settling of Jl (t) to a memory',\n",
       "  \"(of network activity) Xj' the only feedback from the network to\",\n",
       "  'each cell is the scalar',\n",
       "  '~',\n",
       "  '',\n",
       "  'J',\n",
       "  '',\n",
       "  'Q. I Jl- X? I - L',\n",
       "  'J',\n",
       "  '',\n",
       "  'J',\n",
       "  '',\n",
       "  '(15)',\n",
       "  '',\n",
       "  'This quantity is an integrated measure of the distance of the',\n",
       "  'current network state from stored memories.',\n",
       "  'Importantly, this',\n",
       "  'measure is the same for all cells; it is as if a single virtual cell',\n",
       "  'was computing the distance in activity space between the',\n",
       "  'current state and stored states. The result of the computation is',\n",
       "  'This is a',\n",
       "  'then broadcast to all of the cells in the network.',\n",
       "  'generalization of the idea that the detailed activity of each cell in',\n",
       "  'the network need not be fed back to each cell.',\n",
       "  'Rather some',\n",
       "  \"global measure, performed by a single 'effective' cell is all that is\",\n",
       "  'sufficient in the feedback.',\n",
       "  'DISCUSSION',\n",
       "  '',\n",
       "  'We have been discussing a formalism for the analysis of',\n",
       "  'networks of ideal neurons based on a mean field approximation',\n",
       "  'of the detailed activity of the cells in the network. We find that',\n",
       "  'a simple assumption concerning the spatial distribution of the',\n",
       "  'pattern preferences of the cells allows a great simplification of',\n",
       "  'the analysis. In particular, the detailed activity of the cells of',\n",
       "  'the network may be replaced with a mean field that in effect is',\n",
       "  \"computed by a single 'effective' cell.\",\n",
       "  'Further, the application of this formalism to the cortical layer',\n",
       "  'IV of visual cortex allows the prediction that much of learning in',\n",
       "  'cortex may be localized to the LGN-cortical synaptic states, and',\n",
       "  'that cortico-cortical plasticity is relatively unimportant. We find,',\n",
       "  'in agreement with experiment, that monocular deprivation of',\n",
       "  'the cortical cells will drive closed-eye responses to zero, but',\n",
       "  'chemical blockage of the cortical inhibitory pathways would',\n",
       "  'reveal non-zero closed-eye synaptic states.',\n",
       "  '',\n",
       "  '\\x0c692',\n",
       "  '',\n",
       "  'Finally, the mean field approximation allows the development',\n",
       "  'of single layer models of memory storage that are unrestricted',\n",
       "  'in storage density, but require a connectivity of order mxN. This',\n",
       "  'is significant for the fabrication of practical content addressable',\n",
       "  'memories.',\n",
       "  'ACKNOWLEOOEMENTS',\n",
       "  'I would like to thank Leon Cooper for many helpful discussions',\n",
       "  'and the contributions he made to this work.',\n",
       "  '',\n",
       "  '*This work was supported by the Office of Naval Research and',\n",
       "  'the Army Research Office under contracts #NOOOI4-86-K-0041',\n",
       "  'and #DAAG-29-84-K-0202.',\n",
       "  '',\n",
       "  'REFERENCES',\n",
       "  '[1] Bienenstock, E. L., Cooper, L. N & Munro, P. W. (1982) 1.',\n",
       "  'Neuroscience 2, 32-48.',\n",
       "  '[2] Scofield, C. L. (I984) Unpublished Dissertation.',\n",
       "  '[3] Cooper, L. N, Munro, P. W. & Scofield, C. L. (1985) in Synaptic',\n",
       "  'Modification, Neuron Selectivity and Nervous System',\n",
       "  'Organization, ed. C. Levy, J. A. Anderson & S. Lehmkuhle,',\n",
       "  '(Erlbaum Assoc., N. J.).',\n",
       "  '[4] Cooper, L. N & Scofield, C. L. (to be published) Proc. Natl. Acad.',\n",
       "  'Sci. USA ..',\n",
       "  '[5] Singer, W. (1977) Brain Res. 134, 508-000.',\n",
       "  '[6] Bear, M. F., Schmechel D. M., & Ebner, F. F. (1985) 1. Neurosci.',\n",
       "  '5, 1262-0000.',\n",
       "  '[7] Mower, G. D., White, W. F., & Rustad, R. (1986) Brain Res. 380,',\n",
       "  '253-000.',\n",
       "  '[8] Hopfield, J. J. (1982) Proc. Natl. A cad. Sci. USA 79, 2554-2558.',\n",
       "  '[9] Hopfield, J. J., Feinstein, D. 1., & Palmer, R. O. (1983) Nature',\n",
       "  '304, 158-159.',\n",
       "  '[10] Bachmann, C. M., Cooper, L. N, Dembo, A. & Zeitouni, O. (to be',\n",
       "  'published) Proc. Natl. Acad. Sci. USA.',\n",
       "  '',\n",
       "  '\\x0c'],\n",
       " ['394',\n",
       "  '',\n",
       "  'STORING COVARIANCE BY THE ASSOCIATIVE',\n",
       "  'LONG?TERM POTENTIATION AND DEPRESSION',\n",
       "  'OF SYNAPTIC STRENGTHS IN THE HIPPOCAMPUS',\n",
       "  'Patric K. Stanton? and Terrence J. Sejnowski t',\n",
       "  'Department of Biophysics',\n",
       "  'Johns Hopkins University',\n",
       "  'Baltimore, MD 21218',\n",
       "  'ABSTRACT',\n",
       "  '',\n",
       "  'In modeling studies or memory based on neural networks, both the selective',\n",
       "  'enhancement and depression or synaptic strengths are required ror effident storage',\n",
       "  'or inrormation (Sejnowski, 1977a,b; Kohonen, 1984; Bienenstock et aI, 1982;',\n",
       "  'Sejnowski and Tesauro, 1989). We have tested this assumption in the hippocampus,',\n",
       "  'a cortical structure or the brain that is involved in long-term memory. A brier,',\n",
       "  'high-frequency activation or excitatory synapses in the hippocampus produces an',\n",
       "  'increase in synaptic strength known as long-term potentiation, or LTP (BUss and',\n",
       "  'Lomo, 1973), that can last ror many days. LTP is known to be Hebbian since it',\n",
       "  'requires the simultaneous release or neurotransmitter from presynaptic terminals',\n",
       "  'coupled with postsynaptic depolarization (Kelso et al, 1986; Malinow and Miller,',\n",
       "  '1986; Gustatrson et al, 1987). However, a mechanism ror the persistent reduction or',\n",
       "  'synaptic strength that could balance LTP has not yet been demonstrated. We studied the associative interactions between separate inputs onto the same dendritic',\n",
       "  'trees or hippocampal pyramidal cells or field CAl, and round that a low-frequency',\n",
       "  'input which, by itselr, does not persistently change synaptic strength, can either',\n",
       "  'increase (associative LTP) or decrease in strength (associative long-term depression',\n",
       "  'or LTD) depending upon whether it is positively or negatively correlated in time',\n",
       "  'with a second, high-frequency bursting input. LTP or synaptic strength is Hebbian,',\n",
       "  'and LTD is anti-Hebbian since it is elicited by pairing presynaptic firing with postsynaptic hyperpolarization sufficient to block postsynaptic activity. Thus, associative LTP and associative LTO are capable or storing inrormation contained in the',\n",
       "  'covariance between separate, converging hippocampal inputs?',\n",
       "  '',\n",
       "  \"?Present address: Dep~ents of NeW'Oscience and Neurology, Albert Einstein College\",\n",
       "  'of Medicine, 1410 Pelham Parkway South, Bronx, NY 10461 USA.',\n",
       "  'tPresent address: Computational Neurobiology Laboratory, The Salk Institute, P.O. Box',\n",
       "  '85800, San Diego, CA 92138 USA.',\n",
       "  '',\n",
       "  '\\x0cStoring Covariance by Synaptic Strengths in the Hippocampus',\n",
       "  '',\n",
       "  'INTRODUCTION',\n",
       "  'Associative LTP can be produced in some hippocampal neuroos when lowfrequency. (Weak) and high-frequency (Strong) inputs to the same cells are simultaneously activated (Levy and Steward, 1979; Levy and Steward, 1983; Barrionuevo and',\n",
       "  'Brown, 1983). When stimulated alone, a weak input does not have a long-lasting effect',\n",
       "  'on synaptic strength; however, when paired with stimulation of a separate strong input',\n",
       "  'sufficient to produce homo synaptic LTP of that pathway, the weak pathway is associatively potentiated. Neural network modeling studies have predicted that, in addition to',\n",
       "  'this Hebbian form of plasticity, synaptic strength should be weakened when weak and',\n",
       "  'strong inputs are anti-correlated (Sejnowski, 1977a,b; Kohonen, 1984; Bienenstock et al,',\n",
       "  '1982; Sejnowski and Tesauro, 1989). Evidence for heterosynaptic depression in the hippocampus has been found for inputs that are inactive (Levy and Steward, 1979; Lynch et',\n",
       "  'al, 1977) or weakly active (Levy and Steward, 1983) during the stimulation of a strong',\n",
       "  'input, but this depression did not depend on any pattern of weak input activity and was',\n",
       "  'not typically as long-lasting as LTP.',\n",
       "  'Therefore, we searched for conditions under which stimulation of a hippocampal',\n",
       "  'pathway, rather than its inactivity, could produce either long-term depression or potentiation of synaptic strengths, depending on the pattern of stimulation. The stimulus paradigm that we used, illustrated in Fig. I, is based on the finding that bursts of stimuli at 5',\n",
       "  \"Hz are optimal in eliciting LTP in the hippocampus (Larson and Lynch, 1986). A highfrequency burst (S'IRONG) stimulus was applied to Schaffer collateral axons and a lowfrequency (WEAK) stimulus given to a separate subicular input coming from the opposite side of the recording site, but terminating on dendrites of the same population of CAl\",\n",
       "  'pyramidal neurons. Due to the rhythmic nature of the strong input bursts, each weak',\n",
       "  'input shock could be either superimposed on the middle of each burst of the strong input',\n",
       "  '(IN PHASE), or placed symmetrically between bursts (OUT OF PHASE).',\n",
       "  '',\n",
       "  'RESULTS',\n",
       "  'Extracellular evoked field potentials were recorded from the apical dendritic and',\n",
       "  'somatic layers of CAl pyramidal cells. The weak stimulus train was first applied alone',\n",
       "  'and did not itself induce long-lasting changes. The strong site was then stimulated alone,',\n",
       "  'which elicited homosynaptic LTP of the strong pathway but did not significantly alter',\n",
       "  'amplitude of responses to the weak input. When weak and strong inputs were activated',\n",
       "  'IN PHASE, there was an associative LTP of the weak input synapses, as shown in Fig.',\n",
       "  '2a. Both the synaptic excitatory post-synaptic potential (e.p.s.p.) (Ae.p.s.p. = +49.8 ?',\n",
       "  '7.8%, n=20) and population action potential (&Pike = +65.4 ? 16.0%, n=14) were',\n",
       "  'significantly enhanced for at least 60 min up to 180 min following stimulation.',\n",
       "  'In contrast, when weak and strong inputs were applied OUT OF PHASE, they elicited an associative long-term depression (LTO) of the weak input synapses, as shown in',\n",
       "  'Fig. 2b. There was a marked reduction in the population spike (-46.5 ? 11.4%, n=10)',\n",
       "  'with smaller decreases in the e.p.s.p. (-13.8 ? 3.5%, n=13). Note that the stimulus patterns applied to each input were identical in these two experiments, and only the relative',\n",
       "  '',\n",
       "  '395',\n",
       "  '',\n",
       "  '\\x0c396',\n",
       "  '',\n",
       "  'Stanton and Sejnowski',\n",
       "  '',\n",
       "  'phase of the weak and strong stimuli was altered. With these stimulus patterns. synaptic',\n",
       "  'strength could be repeatedly enhanced and depressed in a single slice. as illustrated in Fig',\n",
       "  '2c. As a control experiment to determine whether information concerning covariance',\n",
       "  'between the inputs was actually a determinant of plasticity. we combined the in phase',\n",
       "  'and out of phase conditions, giving both the weak input shocks superimposed on the',\n",
       "  'bursts plus those between the bursts. for a net frequency of 10 Hz. This pattern. which',\n",
       "  'resulted in zero covariance between weak and strong inputs. produced no net change in',\n",
       "  'weak input synaptic strength measmed by extracellular evoked potentials. Thus. the assoa',\n",
       "  '',\n",
       "  'b',\n",
       "  'A.SSOCIA.TIVE STIMULUS PA.RA.DIGMS',\n",
       "  'POSJTIVE.LY CORKELA TED ? \"IN PHASE\"',\n",
       "  '',\n",
       "  '~K~~ _I~__~I____~I____~I_',\n",
       "  'SI1IONG,NJO\\\\IT',\n",
       "  '',\n",
       "  '. u.Jj1l 11l. -1---1&1111.....',\n",
       "  '11 ---1&1',\n",
       "  '111.....',\n",
       "  '11 ---,I~IIII',\n",
       "  '',\n",
       "  'NEGATIVELY CORRELATED? \\'our OF PHASE\"',\n",
       "  \"W[AKIN'lTf\",\n",
       "  '',\n",
       "  \"STIONG 'N''''\",\n",
       "  '',\n",
       "  '~I',\n",
       "  '',\n",
       "  '11111',\n",
       "  '',\n",
       "  '--,-;',\n",
       "  '',\n",
       "  '11111',\n",
       "  '',\n",
       "  '11111',\n",
       "  '',\n",
       "  'Figure 1. Hippocampal slice preparation and stimulus paradigms. a: The in vitro hippocampal slice showing recording sites in CAl pyramidal cell somatic (stratum pyramidale) and dendritic (stratum radiatum) layers. and stimulus sites activating Schaffer collateral (STRONG) and commissural (WEAK) afferents. Hippocampal slices (400 Jlm',\n",
       "  'thick) were incubated in an interface slice chamber at 34-35 0 C. Extracellular (1-5 M!l',\n",
       "  'resistance, 2M NaCI filled) and intracellular (70-120 M 2M K-acetate filled) recording electrodes. and bipolar glass-insulated platinum wire stimulating electrodes (50 Jlm',\n",
       "  'tip diameter). were prepared by standard methods (Mody et al, 1988). b: Stimulus paradigms used. Strong input stimuli (STRONG INPUT) were four trains of 100 Hz bursts.',\n",
       "  'Each burst had 5 stimuli and the interburst interval was 200 msec. Each train lasted 2',\n",
       "  'seconds for a total of 50 stimuli. Weak input stimuli (WEAK INPUT) were four trains of',\n",
       "  'shocks at 5 Hz frequency. each train lasting for 2 seconds. When these inputs were IN',\n",
       "  'PHASE. the weak single shocks were superimposed on the middle of each burst of the',\n",
       "  'strong input. When the weak input was OUT OF PHASE. the single shocks were placed',\n",
       "  'symmetrically between the bursts.',\n",
       "  '',\n",
       "  'n.',\n",
       "  '',\n",
       "  '\\x0cStoring Covariance by Synaptic Strengths in the Hippocampus',\n",
       "  '',\n",
       "  'ciative LTP and LTD mechanisms appear to be balanced in a manner ideal for the',\n",
       "  'storage of temporal covariance relations.',\n",
       "  'The simultaneous depolarization of the postsynaptic membrane and activation of',\n",
       "  'glutamate receptors of the N-methyl-D-aspartate (NMDA) subtype appears to be necessary for LTP induction (Collingridge et ai, 1983; Harris et al, 1984; Wigstrom and Gustaffson, 1984). The SJ?read of current from strong to weak synapses in the dendritic tree,',\n",
       "  'd',\n",
       "  '',\n",
       "  'ASSOCIATIVE',\n",
       "  '',\n",
       "  'LON(;.TE~',\n",
       "  '',\n",
       "  \"I'OTENTIATION\",\n",
       "  '',\n",
       "  'LONG-TE~',\n",
       "  '',\n",
       "  'DE,/tESSION',\n",
       "  '',\n",
       "  '-',\n",
       "  '',\n",
       "  '!!Ll!!!!.',\n",
       "  '',\n",
       "  'b',\n",
       "  '',\n",
       "  'ASSOCIATIVE',\n",
       "  '',\n",
       "  'I',\n",
       "  '',\n",
       "  '11111',\n",
       "  '',\n",
       "  '?',\n",
       "  '',\n",
       "  '11111.',\n",
       "  'I',\n",
       "  '',\n",
       "  'c',\n",
       "  '',\n",
       "  'e...',\n",
       "  '',\n",
       "  'I',\n",
       "  '',\n",
       "  'I',\n",
       "  '',\n",
       "  'I',\n",
       "  '',\n",
       "  'I',\n",
       "  '',\n",
       "  'Figure 2. mustration of associative long-term potentiation (LTP) and associative longterm depression (LTD) using extracellular recordings. a: Associative LTP of evoked',\n",
       "  \"excitatory postsynaptic potentials (e.p.s.p.'s) and population action potential responses in\",\n",
       "  'the weak inpuL Test responses are shown before (Pre) and 30 min after (post) application of weak stimuli in phase with the coactive strong input. b: Associative LTD of',\n",
       "  \"evoked e.p.s.p.'s and population spike responses in the weak input. Test responses are\",\n",
       "  'shown before (Pre) and 30 min after (post) application of weak stimuli out of phase with',\n",
       "  'the coactive strong input. c: Time course of the changes in population spike amplitude',\n",
       "  'observed at each input for a typical experiment. Test responses from the strong input (S,',\n",
       "  'open circles), show that the high-frequency bursts (5 pulses/l00 Hz, 200 msec interburst',\n",
       "  'interval as in Fig. 1) elicited synapse-specific LTP independent of other input activity.',\n",
       "  'Test responses from the weak input (W. filled circles) show that stimulation of the weak',\n",
       "  'pathway out of phase with the strong one produced associative LTD (Assoc LTD) of this',\n",
       "  'input. Associative LTP (Assoc LTP) of the same pathway was then elicited following in',\n",
       "  'phase stimulation. Amplitude and duration of associative LTD or LTP could be increased',\n",
       "  'by stimulating input pathways with more trains of shocks.',\n",
       "  '',\n",
       "  '397',\n",
       "  '',\n",
       "  '\\x0c398',\n",
       "  '',\n",
       "  'Stanton and Sejnowski',\n",
       "  '',\n",
       "  'coupled with release of glutamate from the weak inputs, could account for the ability of',\n",
       "  'the strong pathway to associatively potentiate a weak one (Kelso et al, 1986; Malinow',\n",
       "  'and Miller, 1986; Gustaffson et al, 1987). Consistent with this hypothesis, we find that',\n",
       "  'the NMDA receptor antagonist 2-amino-S-phosphonovaleric acid (APS, 10 J.1M) blocks',\n",
       "  'induction of associative LTP in CAl pyramidal neurons (data not shown, n=S). In contrast, the application of APS to the bathing solution at this same concentration had no',\n",
       "  'significant effect on associative LTD (data not shown, n=6). Thus, the induction of LTD',\n",
       "  'seems to involve cellular mechanisms different from associative LTP.',\n",
       "  'The conditions necessary for LTD induction were explored in another series of',\n",
       "  'experiments using intracellular recordings from CAl pyramidal neurons made using',\n",
       "  'standard techniques (Mody et al, 1988). Induction of associative LTP (Fig 3; WEAK',\n",
       "  'S+W IN PHASE) produced an increase in amplitude of the single cell evoked e.p.s.p. and',\n",
       "  'a lowered action potential threshold in the weak pathway, as reported previously (Barrionuevo and Brown, 1983). Conversely, the induction of associative LTD (Fig. 3;',\n",
       "  'WEAK S+W OUT OF PHASE) was accompanied by a long-lasting reduction of e.p.s.p.',\n",
       "  'amplitude and reduced ability to elicit action potential firing. As in control extracellular',\n",
       "  'experiments, the weak input alone produced no long-lasting alterations in intracellular',\n",
       "  \"e.p.s.p.'s or firing properties, while the strong input alone yielded specific increases of\",\n",
       "  \"the strong pathway e.p.s.p. without altering e.p.s.p. 's elicited by weak input stimulation.\",\n",
       "  '',\n",
       "  'PRE',\n",
       "  '',\n",
       "  '30 min POST',\n",
       "  'S+W OUT OF PHASE',\n",
       "  '',\n",
       "  '30 min POST',\n",
       "  'S+W IN PHASE',\n",
       "  '',\n",
       "  'Figure 3. Demonstration of associative LTP and LTD using intracellular recordings from',\n",
       "  \"a CAl pyramidal neuron. Intracellular e.p.s.p.'s prior to repetitive stimulation (pre), 30\",\n",
       "  'min after out of phase stimulation (S+W OUT OF PHASE), and 30 min after subsequent in phase stimuli (S+W IN PHASE). The strong input (Schaffer collateral side,',\n",
       "  'lower traces) exhibited LTP of the evoked e.p.s.p. independent of weak input activity.',\n",
       "  'Out of phase stimulation of the weak (Subicular side, upper traces) pathway produced a',\n",
       "  'marked, persistent reduction in e.p.s.p. amplitude. In the same cell, subsequent in phase',\n",
       "  'stimuli resulted in associative LTP of the weak input that reversed the LTD and enhanced',\n",
       "  'amplitude of the e.p.s.p. past the original baseline. (RMP = -62 mY, RN = 30 MO)',\n",
       "  '',\n",
       "  '\\x0cStoring Covariance by Synaptic Strengths in the Hippocampus',\n",
       "  '',\n",
       "  'A weak stimulus that is out of phase with a strong one anives when the postsynaptic neuron is hyperpolarized as a consequence of inhibitory postsynaptic potentials and',\n",
       "  'afterhyperpolarization from mechanisms intrinsic to pyramidal neurons. This suggests',\n",
       "  \"that postsynaptic hyperpolarization coupled with presynaptic activation may trigger L'ID.\",\n",
       "  'To test this hypothesis, we injected current with intracellular microelectrodes to hyperpolarize or depolarize the cell while stimulating a synaptic input. Pairing the injection of',\n",
       "  'depolarizing current with the weak input led to LTP of those synapses (Fig. 4a; STIM;',\n",
       "  '',\n",
       "  'a',\n",
       "  '',\n",
       "  'PRE',\n",
       "  '',\n",
       "  '? ?IDPOST',\n",
       "  \"S'I1M ? DEPOL\",\n",
       "  '',\n",
       "  '~l\"V',\n",
       "  'lS.,.c',\n",
       "  '',\n",
       "  'r',\n",
       "  ',\" i',\n",
       "  '',\n",
       "  \"COI'ITROL\",\n",
       "  '',\n",
       "  '-Jj',\n",
       "  '',\n",
       "  'b',\n",
       "  '',\n",
       "  'I',\n",
       "  '',\n",
       "  '--\" \\\\',\n",
       "  '',\n",
       "  '\"----',\n",
       "  '',\n",
       "  '(W.c:ULVllj',\n",
       "  '',\n",
       "  'PRE',\n",
       "  '',\n",
       "  'lOlIIin POST',\n",
       "  'STlM ? HYPERPOL',\n",
       "  '',\n",
       "  'Figure 4. Pairing of postsynaptic hyperpolarization with stimulation of synapses on CAl',\n",
       "  \"hippocampal pyramidal neurons produces L'ID specific to the activated pathway, while\",\n",
       "  \"pairing of postsynaptic depolarization with synaptic stimulation produces synapsespecific LTP. a: Intracellular evoked e.p.s.p.'s are shown at stimulated (STIM) and\",\n",
       "  'unstimulated (CONTROL) pathway synapses before (Pre) and 30 min after (post) pairing a 20 mY depolarization (constant current +2.0 nA) with 5 Hz synaptic stimulation.',\n",
       "  'The stimulated pathway exhibited associative LTP of the e.p.s.p., while the control,',\n",
       "  'unstimulated input showed no change in synaptic strength. (RMP = -65 mY; RN = 35',\n",
       "  \"Mfl) b: Intracellular e.p.s.p. 's are shown evoked at stimulated and control pathway\",\n",
       "  'synapses before (Pre) and 30 min after (post) pairing a 20 mV hyperpolarization (constant current -1.0 nA) with 5 Hz synaptic stimulation. The input (STIM) activated during',\n",
       "  \"the hyperpolarization showed associative LTD of synaptic evoked e.p.s.p.'s, while\",\n",
       "  'synaptic strength of the silent input (CONTROL) was unaltered. (RMP =-62 mV; RN =',\n",
       "  '38M!l)',\n",
       "  '',\n",
       "  '399',\n",
       "  '',\n",
       "  '\\x0c400',\n",
       "  '',\n",
       "  'Stanton and Sejnowski',\n",
       "  '',\n",
       "  '+64.0 -9.7%, n=4), while a control input inactive during the stimulation did not change',\n",
       "  '(CONTROL), as reported previously (Kelso et al, 1986; Malinow and Miller, 1986; Gustaffson et al, 1987). Conversely, prolonged hyperpolarizing current injection paired with',\n",
       "  'the same low-frequency stimuli led to induction of LTD in the stimulated pathway (Fig.',\n",
       "  '4b; STIM; -40.3 ? 6.3%, n=6). but not in the unstimulated pathway (CONTROL). The',\n",
       "  'application of either depolarizing current, hyperpolarizing current, or the weak 5 Hz',\n",
       "  'synaptic stimulation alone did not induce long-term alterations in synaptic strengths.',\n",
       "  'Thus. hyperpolarization and simultaneous presynaptic activity supply sufficient conditions for the induction of LTD in CAl pyramidal neurons.',\n",
       "  '',\n",
       "  'CONCLUSIONS',\n",
       "  'These experiments identify a novel fono of anti-Hebbian synaptic plasticity in the',\n",
       "  'hippocampus and confirm predictions made from modeling studies of information storage',\n",
       "  'in neural networks. Unlike previous reports of synaptic depression in the hippocampus,',\n",
       "  'the plasticity is associative, long-lasting, and is produced when presynaptic activity',\n",
       "  'occurs while the postsynaptic membrane is hyperpolarized. In combination with Hebbian',\n",
       "  'mechanisms also present at hippocampal synapses. associative LTP and associative LTD',\n",
       "  'may allow neurons in the hippocampus to compute and store covariance between inputs',\n",
       "  '(Sejnowski, 1977a,b; Stanton and Sejnowski. 1989). These finding make temporal as',\n",
       "  'well as spatial context an important feature of memory mechanisms in the hippocampus.',\n",
       "  'Elsewhere in the brain, the receptive field properties of cells in cat visual cortex',\n",
       "  'can be altered by visual experience paired with iontophoretic excitation or depression of',\n",
       "  'cellular activity (Fregnac et al, 1988; Greuel et al, 1988). In particular, the chronic hyperpolarization of neurons in visual cortex coupled with presynaptic transmitter release leads',\n",
       "  'to a long-teno depression of the active. but not inactive, inputs from the lateral geniculate',\n",
       "  'nucleus (Reiter and Stryker, 1988). Thus. both Hebbian and anti-Hebbian mechanisms',\n",
       "  'found in the hippocampus seem to also be present in other brain areas, and covariance of',\n",
       "  'firing patterns between converging inputs a likely key to understanding higher cognitive',\n",
       "  'function.',\n",
       "  'This research was supported by grants from the National Science Foundation and',\n",
       "  'the Office of Naval research to TJS. We thank Drs. Charles Stevens and Richard Morris',\n",
       "  'for discussions about related experiments.',\n",
       "  '',\n",
       "  'Rererences',\n",
       "  'Bienenstock, E., Cooper. LN. and Munro. P. Theory for the development of neuron',\n",
       "  'selectivity: orientation specificity and binocular interaction in visual cortex. J. Neurosci. 2. 32-48 (1982).',\n",
       "  'Barrionuevo, G. and Brown, T.H. Associative long-teno potentiation in hippocampal',\n",
       "  'slices. Proc. Nat. Acad. Sci. (USA) 80, 7347-7351 (1983).',\n",
       "  'Bliss. T.V.P. and Lomo, T. Long-lasting potentiation of synaptic ttansmission in the dentate area of the anaesthetized rabbit following stimulation of the perforant path. J.',\n",
       "  'Physiol. (Lond.) 232. 331-356 (1973).',\n",
       "  '',\n",
       "  '\\x0cStoring Covariance by Synaptic Strengths in the Hippocampus',\n",
       "  '',\n",
       "  'Collingridge, GL., Kehl, SJ. and McLennan, H. Excitatory amino acids in synaptic',\n",
       "  'transmission in the Schaffer collateral-commissural pathway of the rat hippocampus. J.',\n",
       "  'Physiol. (Lond.) 334, 33-46 (1983).',\n",
       "  'Fregnac, Y., Shulz, D., Thorpe, S. and Bienenstock, E. A cellular analogue of visual cortical plasticity. Nature (Lond.) 333, 367-370 (1988).',\n",
       "  'Greuel. J.M.. Luhmann. H.J. and Singer. W. Pharmacological induction of usedependent receptive field modifications in visual cortex. Science 242,74-77 (1988).',\n",
       "  'Gustafsson, B., Wigstrom, H., Abraham, W.C. and Huang. Y.Y. Long-term potentiation',\n",
       "  'in the hippocampus using depolarizing current pulses as the conditioning stimulus to',\n",
       "  'single volley synaptic potentials. J. Neurosci. 7, 774-780 (1987).',\n",
       "  'Harris. E.W., Ganong, A.H. and Cotman, C.W. Long-term potentiation in the hippocampus involves activation of N-metbyl-D-aspartate receptors. Brain Res. 323, 132137 (1984).',\n",
       "  'Kelso, S.R.. Ganong, A.H. and Brown, T.H. Hebbian synapses in hippocampus. Proc.',\n",
       "  'Natl. Acad. Sci. USA 83, 5326-5330 (1986).',\n",
       "  'Kohonen. T. Self-Organization and Associative Memory. (Springer-Verlag. Heidelberg,',\n",
       "  '1984).',\n",
       "  'Larson. J. and Lynch. G. Synaptic potentiation in hippocampus by patterned stimulation',\n",
       "  'involves two events. Science 232, 985-988 (1986).',\n",
       "  'Levy. W.B. and Steward, O. Synapses as associative memory elements in the hippocampal formation. Brain Res. 175,233-245 (1979).',\n",
       "  'Levy. W.B. and Steward, O. Temporal contiguity requirements for long-term associative',\n",
       "  'potentiation/depression in the hippocampus. Neuroscience 8, 791-797 (1983).',\n",
       "  'Lynch. G.S., Dunwiddie. T. and Gribkoff. V. Heterosynaptic depression: a postsynaptic',\n",
       "  'correlate oflong-term potentiation. Nature (Lond.) 266. 737-739 (1977).',\n",
       "  'Malinow. R. and Miller, J.P. Postsynaptic hyperpolarization during conditioning reversibly blocks induction of long-term potentiation Nature (Lond.)32.0. 529-530 (1986).',\n",
       "  'Mody. I.. Stanton. PK. and Heinemann. U. Activation of N-methyl-D-aspartate',\n",
       "  '(NMDA) receptors parallels changes in cellular and synaptic properties of dentate',\n",
       "  'gyrus granule cells after kindling. J. Neurophysiol. 59. 1033-1054 (1988).',\n",
       "  'Reiter, H.O. and Stryker, M.P. Neural plasticity without postsynaptic action potentials:',\n",
       "  'Less-active inputs become dominant when kitten visual cortical cells are pharmacologically inhibited. Proc. Natl. Acad. Sci. USA 85, 3623-3627 (1988).',\n",
       "  'Sejnowski, T J. and Tesauro, G. Building network learning algorithms from Hebbian',\n",
       "  'synapses, in: Brain Organization and Memory JL. McGaugh, N.M. Weinberger, and',\n",
       "  'G. Lynch, Eds. (Oxford Univ. Press, New York, in press).',\n",
       "  'Sejnowski, TJ. Storing covariance with nonlinearly interacting neurons. J. Math. Biology 4, 303-321 (1977).',\n",
       "  'Sejnowski, T. J. Statistical constraints on synaptic plasticity. J. Theor. Biology 69, 385389 (1977).',\n",
       "  'Stanton, P.K. and Sejnowski, TJ. Associative long-term depression in the hippocampus:',\n",
       "  'Evidence for anti-Hebbian synaptic plasticity. Nature (Lond.), in review.',\n",
       "  'Wigstrom, H. and Gustafsson, B. A possible correlate of the postsynaptic condition for',\n",
       "  'long-lasting potentiation in the guinea pig hippocampus in vitro. Neurosci. Lett. 44,',\n",
       "  '327?332 (1984).',\n",
       "  '',\n",
       "  '401',\n",
       "  '',\n",
       "  '\\x0c'],\n",
       " ['Bayesian Query Construction for Neural',\n",
       "  'Network Models',\n",
       "  'Gerhard Paass',\n",
       "  'Jorg Kindermann',\n",
       "  'German National Research Center for Computer Science (GMD)',\n",
       "  'D-53757 Sankt Augustin, Germany',\n",
       "  'paass@gmd.de',\n",
       "  'kindermann@gmd.de',\n",
       "  '',\n",
       "  'Abstract',\n",
       "  'If data collection is costly, there is much to be gained by actively selecting particularly informative data points in a sequential way. In',\n",
       "  'a Bayesian decision-theoretic framework we develop a query selection criterion which explicitly takes into account the intended use',\n",
       "  'of the model predictions. By Markov Chain Monte Carlo methods',\n",
       "  'the necessary quantities can be approximated to a desired precision. As the number of data points grows, the model complexity',\n",
       "  'is modified by a Bayesian model selection strategy. The properties of two versions of the criterion ate demonstrated in numerical',\n",
       "  'experiments.',\n",
       "  '',\n",
       "  '1',\n",
       "  '',\n",
       "  'INTRODUCTION',\n",
       "  '',\n",
       "  'In this paper we consider the situation where data collection is costly, as when',\n",
       "  'for example, real measurements or technical experiments have to be performed. In',\n",
       "  \"this situation the approach of query learning ('active data selection', 'sequential\",\n",
       "  \"experimental design', etc.) has a potential benefit. Depending on the previously\",\n",
       "  \"seen examples, a new input value ('query') is selected in a systematic way and\",\n",
       "  'the corresponding output is obtained. The motivation for query learning is that',\n",
       "  'random examples often contain redundant information, and the concentration on',\n",
       "  'non-redundant examples must necessarily improve generalization performance.',\n",
       "  'We use a Bayesian decision-theoretic framework to derive a criterion for query construction. The criterion reflects the intended use of the predictions by an appropriate',\n",
       "  '',\n",
       "  '\\x0c444',\n",
       "  '',\n",
       "  'Gerhard Paass. Jorg Kindermann',\n",
       "  '',\n",
       "  'loss function. We limit our analysis to the selection of the next data point, given a',\n",
       "  'set of data already sampled. The proposed procedure derives the expected loss for',\n",
       "  'candidate inputs and selects a query with minimal expected loss.',\n",
       "  'There are several published surveys of query construction methods [Ford et al. 89,',\n",
       "  'Plutowski White 93, Sollich 94]. Most current approaches, e.g. [Cohn 94], rely',\n",
       "  'on the information matrix of parameters. Then however, all parameters receive',\n",
       "  'equal attention regardless of their influence on the intended use of the model',\n",
       "  '[Pronzato Walter 92]. In addition, the estimates are valid only asymptotically. Bayesian approaches have been advocated by [Berger 80], and applied to neural networks',\n",
       "  '[MacKay 92]. In [Sollich Saad 95] their relation to maximum information gain is',\n",
       "  'discussed. In this paper we show that by using Markov Chain Monte Carlo methods it is possible to determine all quantities necessary for the selection of a query.',\n",
       "  \"This approach is valid in small sample situations, and the procedure's precision can\",\n",
       "  'be increased with additional computational effort. With the square loss function,',\n",
       "  'the criterion is reduced to a variant of the familiar integrated mean square error',\n",
       "  '[Plutowski White 93].',\n",
       "  '',\n",
       "  'In the next section we develop the query selection criterion from a decision-theoretic',\n",
       "  'point of view. In the third section we show how the criterion can be calculated using',\n",
       "  'Markov Chain Monte Carlo methods and we discuss a strategy for model selection.',\n",
       "  'In the last section, the results of two experiments with MLPs are described.',\n",
       "  '2',\n",
       "  '',\n",
       "  'A DECISION-THEORETIC FRAMEWORK',\n",
       "  '',\n",
       "  'Assume we have an input vector x and a scalar output y distributed as y \"\" p(y I x, w)',\n",
       "  'where w is a vector of parameters. The conditional expected value is a deterministic',\n",
       "  'function !(x, w) := E(y I x, w) where y = !(x, w)+? and ? is a zero mean error term.',\n",
       "  'Suppose we have iteratively collected observations D(n) := ((Xl, iii), .. . , (Xn, Yn)).',\n",
       "  'We get the Bayesian posterior p(w I D(n)) = p(D(n) Iw) p(w)/ J p(D(n) Iw) p(w) dw',\n",
       "  'and the predictive distribution p(y I x, D(n)) = p(y I x, w)p(w I D(n)) dw if p(w) is',\n",
       "  'the prior distribution.',\n",
       "  '',\n",
       "  'J',\n",
       "  '',\n",
       "  'We consider the situation where, based on some data x, we have to perform an',\n",
       "  'action a whose result depends on the unknown output y. Some decisions may have',\n",
       "  'more severe effects than others. The loss function L(y, a) E [0,00) measures the',\n",
       "  'loss if y is the true value and we have taken the action a E A. In this paper we',\n",
       "  'consider real-valued actions, e.g. setting the temperature a in a chemical process.',\n",
       "  'We have to select an a E A only knowing the input x. According to the Bayes',\n",
       "  'Principle [Berger 80, p.14] we should follow a decision rule d : x --t a such that',\n",
       "  'the average risk J R(w, d) p(w I D(n)) dw is minimal, where the risk is defined as',\n",
       "  'R(w, d) := J L(y, d(x)) p(y I x, w) p(x) dydx. Here p(x) is the distribution of future',\n",
       "  'inputs, which is assumed to be known.',\n",
       "  'For the square loss function L(y, a) = (y - a)2, the conditional expectation',\n",
       "  'd(x) := E(y I x, D(n)) is the optimal decision rule. In a control problem the loss',\n",
       "  'may be larger at specific critical points. This can be addressed with a weighted square loss function L(y, a) := h(y)(y - a)2, where h(y) 2: a [Berger 80,',\n",
       "  'p.1U]. The expected loss for an action is J(y - a)2h(y) p(y I x, D(n)) dy. Replacing the predictive density p(y I x, D(n)) with the weighted predictive density',\n",
       "  '',\n",
       "  '\\x0cBayesian Query Construction for Neural Network Models',\n",
       "  '',\n",
       "  '445',\n",
       "  '',\n",
       "  'p(y I x, Den) := h(y) p(y I x, Den)/G(x), where G(x) := I h(y) p(y I x, Den) dy,',\n",
       "  'we get the optimal decision rule d(x) := I yp(y I x, Den) dy and the average loss',\n",
       "  'G(x) I(y - E(y I x, D(n))2 p(y I x, Den) dy for a given input x. With these modifications, all later derIvations for the square loss function may be applied to the',\n",
       "  'weighted square loss.',\n",
       "  'The aim of query sampling is the selection of a new observation x in such a way',\n",
       "  'that the average risk will be maximally reduced. Together with its still unknown',\n",
       "  'y-value, x defines a new observation (x, y) and new data Den) U (x, y). To determine',\n",
       "  'this risk for some given x we have to perform the following conceptual steps for a',\n",
       "  'candidate query x:',\n",
       "  \"1. Future Data: Construct the possible sets of 'future' observations Den) U\",\n",
       "  '',\n",
       "  '(x, y), where y \"\"\\' p(y I x, Den).',\n",
       "  \"2. Future posterior: Determine a 'future' posterior distribution of parameters\",\n",
       "  'p(w I Den) U (x, y? that depends on y in the same way as though it had',\n",
       "  'actually been observed.',\n",
       "  '3. Future Loss: Assuming d~,x(x) is the optimal decision rule for given values',\n",
       "  'of x, y, and x, compute the resulting loss as',\n",
       "  '',\n",
       "  \"1';,x(x):=\",\n",
       "  '',\n",
       "  'J',\n",
       "  '',\n",
       "  'L(y,d;,x(x?p(ylx,w)p(wIDen)U(x,y?dydw',\n",
       "  '',\n",
       "  '(1)',\n",
       "  '',\n",
       "  '4. Averaging: Integrate this quantity over the future trial inputs x distributed',\n",
       "  'as p(x) and the different possible future outputs y, yielding',\n",
       "  '',\n",
       "  \"1';:= Ir;,x(x)p(x)p(ylx,Den)dxdy.\",\n",
       "  'This procedure is repeated until an x with minimal average risk is found. Since local',\n",
       "  'optima are typical, a global optimization method is required. Subsequently we then',\n",
       "  'try to determine whether the current model is still adequate or whether we have to',\n",
       "  'increase its complexity (e.g. by adding more hidden units).',\n",
       "  '',\n",
       "  '3',\n",
       "  '',\n",
       "  'COMPUTATIONAL PROCEDURE',\n",
       "  '',\n",
       "  'Let us assume that the real data Den) was generated according to a regression model',\n",
       "  'y = !(x, w)+{ with i.i.d. Gaussian noise {\"\"\\' N(O, (T2(w?. For example !(x, w) may',\n",
       "  'be a multilayer perceptron or a radial basis function network. Since the error terms',\n",
       "  'are independent, the posterior density is p( w I Den) ex: p( w) rr~=l P(Yi I Xi, w) even',\n",
       "  'in the case of query sampling [Ford et al. 89].',\n",
       "  'As the analytic derivation of the posterior is infeasible except in trivial cases, we',\n",
       "  'have to use approximations. One approach is to employ a normal approximation',\n",
       "  '[MacKay 92], but this is unreliable if the number of observations is small compared to the number of parameters. We use Markov Chain Monte Carlo procedures',\n",
       "  '[PaaB 91, Neal 93] to generate a sample WeB) := {WI, .. .WB} of parameters distributed according to p( w I Den). If the number of sampling steps approaches infinity,',\n",
       "  'the distribution of the simulated Wb approximates the posterior arbitrarily well.',\n",
       "  'To take into account the range of future y-values, we create a set of them by simulation. For each Wb E WeB) a number of y \"\"\\' p(y I x, Wb) is generated. Let',\n",
       "  '',\n",
       "  '\\x0c446',\n",
       "  '',\n",
       "  'y(x.R)',\n",
       "  '',\n",
       "  'Gerhard Paass. JiJrg Kindermann',\n",
       "  '',\n",
       "  '{YI, ... , YR} be the resulting set. Instead of performing a new Markov',\n",
       "  'Monte Carlo run to generate a new sample according to p(w I DCn) U (x, y)), we',\n",
       "  ':=',\n",
       "  '',\n",
       "  'use the old set WCB) of parameters and reweight them (importance sampling).',\n",
       "  'In this way we may approximate integrals of some function g( w) with respect to',\n",
       "  'p(w I DCn) U (x, y)) [Kalos Whitlock 86, p.92]:',\n",
       "  '',\n",
       "  '- -))d',\n",
       "  'j 9 (w ) P(W IDCn) U( X,',\n",
       "  'Y',\n",
       "  'W',\n",
       "  '',\n",
       "  '__',\n",
       "  '',\n",
       "  '--',\n",
       "  '',\n",
       "  'L~-lg(Wb)P(ylx,Wb)',\n",
       "  'B',\n",
       "  '',\n",
       "  'Lb=l p(Y I x, Wb)',\n",
       "  '',\n",
       "  '(2)',\n",
       "  '',\n",
       "  'The approximation error approaches zero as the size of WCB) increases.',\n",
       "  '',\n",
       "  '3.1',\n",
       "  '',\n",
       "  'APPROXIMATION OF FUTURE LOSS',\n",
       "  '',\n",
       "  'Consider the future loss f;,x(x) given new observation (x, y) and trial input Xt. In',\n",
       "  'the case of the square loss function, (1) can be transformed to',\n",
       "  '',\n",
       "  'f~,.t(Xt)',\n",
       "  '',\n",
       "  '=',\n",
       "  '',\n",
       "  'j[!(Xt,w)-E(yIXt,Dcn)U(X,y)Wp(wIDcn)U(x,y))dw (3)',\n",
       "  '',\n",
       "  '+ j ?T2(w) p(w I DCn) U (x, y)) dw',\n",
       "  'where ?T2(w) := Var(y I x, w) is independent of x. Assume a set XT = {Xl, ... , XT}',\n",
       "  'is given, which is representative of trial inputs for the distribution p(x). Define',\n",
       "  'S(x, y) := L~=i p(Y I x, Wb) for y E YCx,R) . Then from equations (2) and (3) we get',\n",
       "  'E(ylxt,DCn)U(x,y)):= 1/S(x,Y)L~=1!(Xt,Wb)P(Ylx,Wb) and',\n",
       "  '1',\n",
       "  '',\n",
       "  'B',\n",
       "  '',\n",
       "  'S(x -) L?T 2(Wb)P(Ylx,Wb)',\n",
       "  ',y b=l',\n",
       "  '1',\n",
       "  '',\n",
       "  '+ S(x',\n",
       "  '',\n",
       "  '(4)',\n",
       "  '',\n",
       "  'B',\n",
       "  '',\n",
       "  '-) I)!(Xt, Wb) - E(y I Xt, DCn) U (x, y))]2 p(Y I x, Wb)',\n",
       "  '',\n",
       "  ',y',\n",
       "  '',\n",
       "  'b=l',\n",
       "  '',\n",
       "  'The final value of f; is obtained by averaging over the different y E YCx,R) and',\n",
       "  'different trial inputs Xt E XT. To reduce the variance, the trial inputs Xt should',\n",
       "  'be selected by importance sampling (2) to concentrate them on regions with high',\n",
       "  'current loss (see (5) below). To facilitate the search for an x with minimal f; we',\n",
       "  'reduce the extent of random fluctuations of the y values. Let (Vi, ... , VR) be a',\n",
       "  'vector of random numbers Vr -- N(O,1), and let jr be randomly selected from',\n",
       "  '{1, ... , B}. Then for each x the possible observations Yr E YCx,R) are defined as',\n",
       "  'Yr := !(x, wir) + V r?T2(wir). In this way the difference between neighboring inputs',\n",
       "  'is not affected by noise, and search procedures can exploit gradients.',\n",
       "  '',\n",
       "  '3.2',\n",
       "  '',\n",
       "  'CURRENT LOSS',\n",
       "  '',\n",
       "  'As a proxy for the future loss, we may use the current loss at',\n",
       "  '',\n",
       "  'x,',\n",
       "  '',\n",
       "  'rcurr(x) = p(x) j L(y, d*(x)) p(y I x, DCn)) dy',\n",
       "  '',\n",
       "  '(5)',\n",
       "  '',\n",
       "  '\\x0cBayesian Query Construction for Neural Network Models',\n",
       "  '',\n",
       "  '447',\n",
       "  '',\n",
       "  'where p(x) weights the inputs according to their relevance. For the square loss',\n",
       "  'function the average loss at x is the conditional variance Var(y I x, DCn?. We get',\n",
       "  '',\n",
       "  '=',\n",
       "  '',\n",
       "  'Tcurr(X)',\n",
       "  '',\n",
       "  'p(x) jU(x,w)-E(YIX,DCn?)2p(wIDcn?dw',\n",
       "  '',\n",
       "  '(6)',\n",
       "  '',\n",
       "  '+ p(x) j 0\"2(w) p(w I D(n? dw',\n",
       "  'If E(y I x,DCn?',\n",
       "  'fr~~=lf(x,wb) and the sample WCB):= {Wl, ... ,WB} is',\n",
       "  'representative of p(w I DCn? we can approximate the current loss with',\n",
       "  '',\n",
       "  'Tcurr(X)',\n",
       "  '',\n",
       "  '~',\n",
       "  '',\n",
       "  'p( x) ~',\n",
       "  '',\n",
       "  '13 L..tU(x, Wb) -',\n",
       "  '',\n",
       "  '2',\n",
       "  '',\n",
       "  'E(y I x, DCn?) +',\n",
       "  'A',\n",
       "  '',\n",
       "  'p( x) ~',\n",
       "  '',\n",
       "  '13 L..t 0\"',\n",
       "  '',\n",
       "  'b=l',\n",
       "  '',\n",
       "  '2',\n",
       "  '',\n",
       "  '(Wb)',\n",
       "  '',\n",
       "  '(7)',\n",
       "  '',\n",
       "  'b=l',\n",
       "  '',\n",
       "  'If the input distribution p( x) is uniform, the second term is independent of x.',\n",
       "  '3.3',\n",
       "  '',\n",
       "  'COMPLEXITY REGULARIZATION',\n",
       "  '',\n",
       "  'Neural network models can represent arbitrary mappings between finite-dimensional',\n",
       "  'spaces if the number of hidden units is sufficiently large [Hornik Stinchcombe 89].',\n",
       "  'As the number of observations grows, more and more hidden units are necessary to catch the details of the mapping. Therefore we use a sequential procedure to increase the capacity of our networks during query learning. White and',\n",
       "  'Wooldridge call this approach the \"method of sieves\" and provide some asymptotic results on its consistency [White Wooldridge 91]. Gelfand and Dey compare Bayesian approaches for model selection and prove that, in the case of nested models Ml and M2, model choice by the ratio of popular Bayes factors',\n",
       "  'p(DCn) I Mi) := J p(DCn) I W, Mi ) p(w I Mi) dw will always choose the full model',\n",
       "  'regardless of the data as n --t 00 [Gelfand Dey 94]. They show that the pseudoBayes factor, a Bayesian variant of crossvalidation, is not affected by this paradox',\n",
       "  '',\n",
       "  \"A(Ml' M2) :=\",\n",
       "  '',\n",
       "  'n',\n",
       "  '',\n",
       "  'n',\n",
       "  '',\n",
       "  ';=1',\n",
       "  '',\n",
       "  'j=1',\n",
       "  '',\n",
       "  'II p(y; I x;, DCn,j), Mt}j II p(Y; Ix;, DCn,j), M2)',\n",
       "  '',\n",
       "  '(8)',\n",
       "  '',\n",
       "  'Here DCn ,;) := D(n) \\\\ (x;, y;). As the difference between p(w I DCn? and p( wi D(n,j?',\n",
       "  'is usually small, we use the full posterior as the importance function (2) and get',\n",
       "  '',\n",
       "  'p(Y;',\n",
       "  '',\n",
       "  'I x;, DCn,j),Mi) =',\n",
       "  '',\n",
       "  'j p(Y; IXj,w,Mi)p(wIDCn,j),Mi)dw',\n",
       "  '',\n",
       "  '\\'\" B/(t,l/P(Y;li;,W\"M,))',\n",
       "  '4',\n",
       "  '',\n",
       "  '(9)',\n",
       "  '',\n",
       "  'NUMERICAL DEMONSTRATION',\n",
       "  '',\n",
       "  'In a first experiment we tested the approach for a small a 1-2-1 MLP target function with Gaussian noise N(0,0.05 2 ). We assumed the square loss function and a',\n",
       "  'uniform input distribution p(x) over [-5,5]. Using the \"true\" architecture for the',\n",
       "  'approximating model we started with a single randomly generated observation. We',\n",
       "  '',\n",
       "  '\\x0c448',\n",
       "  '',\n",
       "  'Gerhard Paass, JiJrg Kindermann',\n",
       "  '',\n",
       "  '~',\n",
       "  '',\n",
       "  '=~!?~',\n",
       "  '',\n",
       "  '--- ~tuo:io_',\n",
       "  '',\n",
       "  '~',\n",
       "  '',\n",
       "  \".. .' .\",\n",
       "  '',\n",
       "  \"1'01\",\n",
       "  '',\n",
       "  '..',\n",
       "  '',\n",
       "  'on',\n",
       "  '',\n",
       "  '~',\n",
       "  '',\n",
       "  \"I - '~ ' =~ I\",\n",
       "  '',\n",
       "  ':;',\n",
       "  '',\n",
       "  '\"',\n",
       "  '',\n",
       "  '. ..',\n",
       "  '',\n",
       "  'a:',\n",
       "  '0',\n",
       "  '',\n",
       "  '::::.:::::.::::\\\\....',\n",
       "  'd',\n",
       "  '',\n",
       "  ':;',\n",
       "  '',\n",
       "  '....',\n",
       "  '',\n",
       "  '\\\\~.',\n",
       "  '',\n",
       "  \"'\\\\ ------ -- - - - - - - -----\",\n",
       "  '',\n",
       "  '\\\\., 1\\\\l',\n",
       "  '',\n",
       "  '. . ......_. _-_._...........__................... _. ._......._..',\n",
       "  '',\n",
       "  '~',\n",
       "  '',\n",
       "  '\\\\!',\n",
       "  '',\n",
       "  '~',\n",
       "  '',\n",
       "  ',',\n",
       "  '',\n",
       "  '\\\\',\n",
       "  '',\n",
       "  ':.,.',\n",
       "  \"\\\\, '\",\n",
       "  '',\n",
       "  '\"',\n",
       "  '',\n",
       "  '\\\\!',\n",
       "  '',\n",
       "  '\\'\"',\n",
       "  '0',\n",
       "  '',\n",
       "  '..',\n",
       "  '',\n",
       "  '-2',\n",
       "  '',\n",
       "  '10',\n",
       "  '',\n",
       "  '15',\n",
       "  '20',\n",
       "  'No.d_',\n",
       "  '',\n",
       "  '25',\n",
       "  '',\n",
       "  '30',\n",
       "  '',\n",
       "  'Figure 1: Future loss exploration: predicted posterior mean, future loss and current',\n",
       "  'loss for 12 observations (left), and root mean square error of prediction (right) .',\n",
       "  'estimated the future loss by (4) for 100 different inputs and selected the input with',\n",
       "  'smallest future loss as the next query. B = 50 parameter vectors were generated requiring 200,000 Metropolis steps. Simultaneously we approximated the current loss',\n",
       "  'criterion by (7). The left side of figure 1 shows the typical relation of both measures.',\n",
       "  'In most situations the future loss is low in the same regions where the current loss',\n",
       "  '(posterior standard deviation of mean prediction) is high. The queries are concentrated in areas of high variation and the estimated posterior mean approximates',\n",
       "  'the target function quite well.',\n",
       "  'In the right part of figure 1 the RMSE of prediction averaged over 12 independent',\n",
       "  'experiments is shown. After a few observations the RMSE drops sharply. In our',\n",
       "  'example there is no marked difference between the prediction errors resulting from',\n",
       "  'the future loss and the current loss criterion (also averaged over 12 experiments).',\n",
       "  'Considering the substantial computing effort this favors the current loss criterion.',\n",
       "  'The dots indicate the RMSE for randomly generated data (averaged over 8 experiments) using the same Bayesian prediction procedure. Because only few data points',\n",
       "  'were located in the critical region of high variation the RMSE is much larger.',\n",
       "  'In the second experiment, a 2-3-1 MLP defined the target function I(x, wo) , to which',\n",
       "  'Gaussian noise of standard deviation 0.05 was added. I( x, wo) is shown in the left',\n",
       "  'part of figure 2. We used five MLPs with 2-6 hidden units as candidate models',\n",
       "  \"Ml, .. . , M5 and generated B = 45 samples WeB) of the posterior pew I D(n)' M.),\",\n",
       "  'where D(n) is the current data. We started with 30,000 Metropolis steps for small',\n",
       "  'values of n and increased this to 90,000 Metropolis steps for larger values of n.',\n",
       "  'For a network with 6 hidden units and n = 50 observations, 10,000 Metropolis',\n",
       "  'steps took about 30 seconds on a Sparc10 workstation. Next, we used equation (9)',\n",
       "  'to compare the different models, and then used the optimal model to calculate the',\n",
       "  'current loss (7) on a regular grid of 41 x 41 = 1681 query points x. Here we assumed',\n",
       "  'the square loss function and a uniform input distribution p(x) over [-5,5] x [-5,5].',\n",
       "  'We selected the query point with maximal current loss and determined the final',\n",
       "  'query point with a hillclimbing algorithm. In this way we were rather sure to get',\n",
       "  'close to the true global optimum.',\n",
       "  'The main result of the experiment is summarized in the right part of figure 2. It',\n",
       "  '',\n",
       "  '\\x0cBayesian Query Construct.ion for Neural Network Models',\n",
       "  '',\n",
       "  '449',\n",
       "  '',\n",
       "  '?',\n",
       "  'o',\n",
       "  '',\n",
       "  '\".',\n",
       "  '',\n",
       "  '.m',\n",
       "  '',\n",
       "  'eXDlorati~n',\n",
       "  'random a',\n",
       "  '',\n",
       "  ':2\"\\': \\\\',\n",
       "  '<:>',\n",
       "  '',\n",
       "  '\\\\',\n",
       "  '',\n",
       "  '~\\\\?{l? .',\n",
       "  '.,',\n",
       "  '.\"',\n",
       "  '',\n",
       "  'o .. o .. o ............. __ (). ...',\n",
       "  '',\n",
       "  '\\\\',\n",
       "  '',\n",
       "  '. . .......... 0 ... .. ........ --',\n",
       "  '',\n",
       "  '..',\n",
       "  '',\n",
       "  '~.',\n",
       "  '',\n",
       "  '20',\n",
       "  '',\n",
       "  '40',\n",
       "  '',\n",
       "  '0',\n",
       "  '',\n",
       "  '60',\n",
       "  '',\n",
       "  '80',\n",
       "  '',\n",
       "  '100',\n",
       "  '',\n",
       "  'No. of Observations',\n",
       "  '',\n",
       "  'Figure 2: Current loss exploration: MLP target function and root mean square error.',\n",
       "  'shows - averaged over 3 experiments - the root mean square error between the true',\n",
       "  'mean value and the posterior mean E(y I x) on the grid of 1681 inputs in relation to',\n",
       "  'the sample size. Three phases of the exploration can be distinguished (see figure 3).',\n",
       "  'In the beginning a search is performed with many queries on the border of the',\n",
       "  'input area. After about 20 observations the algorithm knows enough detail about',\n",
       "  'the true function to concentrate on the relevant parts of the input space. This leads',\n",
       "  'to a marked reduction ofthe mean square error. After 40 observations the systematic',\n",
       "  'part of the true function has been captured nearly perfectly. In the last phase of',\n",
       "  'the experiment the algorithm merely reduces the uncertainty caused by the random',\n",
       "  'noise. In contrast , the data generated randomly does not have sufficient information',\n",
       "  'on the details of f(x , w), and therefore the error only gradually decreases. Because',\n",
       "  'of space constraints we cannot report experiments with radial basis functions which',\n",
       "  'led to similar results.',\n",
       "  'Acknowledgements',\n",
       "  \"This work is part of the joint project 'REFLEX' of the German Fed. Department\",\n",
       "  'of Science and Technology (BMFT), grant number 01 IN 111Aj4. We would like to',\n",
       "  'thank Alexander Linden, Mark Ring, and Frank Weber for many fruitful discussions.',\n",
       "  '',\n",
       "  'References',\n",
       "  '[Berger 80] Berger, J. (1980): Statistical Decision Theory, Foundations, Concepts, and',\n",
       "  'Methods. Springer Verlag, New York.',\n",
       "  '[Cohn 94] Cohn, D. (1994): Neural Network Exploration Using Optimal Experimental',\n",
       "  'Design. In J. Cowan et al. (eds.): NIPS 5. Morgan Kaufmann, San Mateo.',\n",
       "  '[Ford et al. 89] Ford, I. , Titterington, D.M., Kitsos, C.P. (1989): Recent Advances in Nonlinear Design. Technometrics, 31, p.49-60.',\n",
       "  '[Gelfand Dey 94] Gelfand, A.E., Dey, D.K. (1994): Bayesian Model Choice: Asymptotics',\n",
       "  'and Exact Calculations. J. Royal Statistical Society B, 56, pp.501-514.',\n",
       "  '',\n",
       "  '\\x0c450',\n",
       "  '',\n",
       "  'Gerhard Paass, Jorg Kindermann',\n",
       "  '',\n",
       "  'Figure 3: Squareroot of current loss (upper row) and absolute deviation from true',\n",
       "  'function (lower row) for 10,25, and 40 observations (which are indicated by dots) .',\n",
       "  '[Hornik Stinchcombe 89] Hornik, K., Stinchcombe, M. (1989): Multilayer Feedforward',\n",
       "  'Networks are Universal Approximators. Neural Networks 2, p.359-366.',\n",
       "  '[Kalos Whitlock 86] Kalos, M.H., Whitlock, P.A. (1986): Monte Carlo Methods, Wiley,',\n",
       "  'New York.',\n",
       "  '[MacKay 92] MacKay, D. (1992): Information-Based Objective Functions for Active Data',\n",
       "  'Selection. Neural Computation 4, p .590-604.',\n",
       "  '[Neal 93] Neal, R.M. (1993): Probabilistic Inference using Markov Chain Monte Carlo',\n",
       "  'Methods. Tech. Report CRG-TR-93-1, Dep. of Computer Science, Univ. of Toronto.',\n",
       "  '[PaaB 91] PaaB, G. (1991): Second Order Probabilities for Uncertain and Conflicting Evidence. In: P.P. Bonissone et al. (eds.) Uncertainty in Artificial Intelligence 6. Elsevier,',\n",
       "  'Amsterdam, pp. 447-456.',\n",
       "  '[Plutowski White 93] Plutowski, M., White, H. (1993): Selecting Concise Training Sets',\n",
       "  'from Clean Data. IEEE Tr. on Neural Networks, 4, p.305-318.',\n",
       "  '[Pronzato Walter 92] Pronzato, L., Walter, E. (1992): Nonsequential Bayesian Experimental Design for Response Optimization. In V. Fedorov, W.G. Miiller, I.N. Vuchkov',\n",
       "  '(eds.): Model Oriented Data-Analysis. Physica Verlag, Heidelberg, p. 89-102.',\n",
       "  '[Sollich 94] Sollich, P. (1994): Query Construction, Entropy and Generalization in Neural',\n",
       "  'Network Models. To appear in Physical Review E.',\n",
       "  '[Sollich Saad 95] Sollich, P., Saad, D. (1995): Learning from Queries for Maximum Information Gain in Unlearnable Problems. This volume.',\n",
       "  '[White Wooldridge 91] White, H., Wooldridge, J. (1991): Some Results for Sieve Estimation with Dependent Observations. In W. Barnett et al. (eds.) : Nonparametric and',\n",
       "  'Semiparametric Methods in Econometrics and Statistics, New York, Cambridge Univ.',\n",
       "  'Press.',\n",
       "  '',\n",
       "  '\\x0c'],\n",
       " ['Neural Network Ensembles, Cross',\n",
       "  'Validation, and Active Learning',\n",
       "  '',\n",
       "  'Anders Krogh\"',\n",
       "  'Nordita',\n",
       "  'Blegdamsvej 17',\n",
       "  '2100 Copenhagen, Denmark',\n",
       "  '',\n",
       "  'Jesper Vedelsby',\n",
       "  'Electronics Institute, Building 349',\n",
       "  'Technical University of Denmark',\n",
       "  '2800 Lyngby, Denmark',\n",
       "  '',\n",
       "  'Abstract',\n",
       "  'Learning of continuous valued functions using neural network ensembles (committees) can give improved accuracy, reliable estimation of the generalization error, and active learning. The ambiguity',\n",
       "  'is defined as the variation of the output of ensemble members averaged over unlabeled data, so it quantifies the disagreement among',\n",
       "  'the networks. It is discussed how to use the ambiguity in combination with cross-validation to give a reliable estimate of the ensemble',\n",
       "  'generalization error, and how this type of ensemble cross-validation',\n",
       "  'can sometimes improve performance. It is shown how to estimate',\n",
       "  'the optimal weights of the ensemble members using unlabeled data.',\n",
       "  'By a generalization of query by committee, it is finally shown how',\n",
       "  'the ambiguity can be used to select new training data to be labeled',\n",
       "  'in an active learning scheme.',\n",
       "  '',\n",
       "  '1',\n",
       "  '',\n",
       "  'INTRODUCTION',\n",
       "  '',\n",
       "  'It is well known that a combination of many different predictors can improve predictions. In the neural networks community \"ensembles\" of neural networks has been',\n",
       "  'investigated by several authors, see for instance [1, 2, 3]. Most often the networks',\n",
       "  'in the ensemble are trained individually and then their predictions are combined.',\n",
       "  'This combination is usually done by majority (in classification) or by simple averaging (in regression), but one can also use a weighted combination of the networks .',\n",
       "  '.. Author to whom correspondence should be addressed. Email: kroghlnordita. elk',\n",
       "  '',\n",
       "  '\\x0c232',\n",
       "  '',\n",
       "  'Anders Krogh, Jesper Vedelsby',\n",
       "  '',\n",
       "  'At the workshop after the last NIPS conference (December, 1993) an entire session',\n",
       "  'was devoted to ensembles of neural networks ( \"Putting it all together\", chaired by',\n",
       "  'Michael Perrone) . Many interesting papers were given, and it showed that this area',\n",
       "  'is getting a lot of attention .',\n",
       "  'A combination of the output of several networks (or other predictors) is only useful',\n",
       "  'if they disagree on some inputs. Clearly, there is no more information to be gained',\n",
       "  'from a million identical networks than there is from just one of them (see also',\n",
       "  '[2]). By quantifying the disagreement in the ensemble it turns out to be possible',\n",
       "  'to state this insight rigorously for an ensemble used for approximation of realvalued functions (regression). The simple and beautiful expression that relates the',\n",
       "  'disagreement (called the ensemble ambiguity) and the generalization error is the',\n",
       "  'basis for this paper, so we will derive it with no further delay.',\n",
       "  '',\n",
       "  '2',\n",
       "  '',\n",
       "  'THE BIAS-VARIANCE TRADEOFF',\n",
       "  '',\n",
       "  'Assume the task is to learn a function J from RN to R for which you have a sample',\n",
       "  'of p examples, (xiJ , yiJ), where yiJ = J(xiJ) and J.t = 1, . . . ,p. These examples',\n",
       "  'are assumed to be drawn randomly from the distribution p(x) . Anything in the',\n",
       "  'following is easy to generalize to several output variables.',\n",
       "  'The ensemble consists of N networks and the output of network a on input x is',\n",
       "  'called va (x). A weighted ensemble average is denoted by a bar , like',\n",
       "  '',\n",
       "  'V(x) =',\n",
       "  '',\n",
       "  'L Wa Va(x).',\n",
       "  '',\n",
       "  '(1)',\n",
       "  '',\n",
       "  'a',\n",
       "  '',\n",
       "  'This is the final output of the ensemble. We think of the weight Wa as our belief in',\n",
       "  'network a and therefore constrain the weights to be positive and sum to one. The',\n",
       "  'constraint on the sum is crucial for some of the following results.',\n",
       "  'The ambiguity on input x of a single member of the ensemble is defined as aa (x)',\n",
       "  '(V a(x) - V(x))2 . The ensemble ambiguity on input x is',\n",
       "  '',\n",
       "  'a(x)',\n",
       "  '',\n",
       "  '= Lwaaa(x) = LWa(va(x) a',\n",
       "  '',\n",
       "  'V(x))2 .',\n",
       "  '',\n",
       "  '=',\n",
       "  '',\n",
       "  '(2)',\n",
       "  '',\n",
       "  'a',\n",
       "  '',\n",
       "  'It is simply the variance of the weighted ensemble around the weighed mean, and',\n",
       "  'it measures the disagreement among the networks on input x. The quadratic error',\n",
       "  'of network a and of the ensemble are',\n",
       "  '',\n",
       "  '(J(x) - V a(x))2',\n",
       "  '(J(x) - V(X))2',\n",
       "  '',\n",
       "  '(3)',\n",
       "  '(4)',\n",
       "  '',\n",
       "  'respectively. Adding and subtracting J( x) in (2) yields',\n",
       "  '',\n",
       "  'a(x)',\n",
       "  '',\n",
       "  '=L',\n",
       "  '',\n",
       "  'Wafa(X) - e(x)',\n",
       "  '',\n",
       "  '(5)',\n",
       "  '',\n",
       "  'a',\n",
       "  '',\n",
       "  '(after a little algebra using that the weights sum to one) . Calling the weighted',\n",
       "  'average of the individual errors ?( x) = La Wa fa (x) this becomes',\n",
       "  '',\n",
       "  'e(x)',\n",
       "  '',\n",
       "  '= ?(x) -',\n",
       "  '',\n",
       "  'a(x).',\n",
       "  '',\n",
       "  '(6)',\n",
       "  '',\n",
       "  '\\x0cNeural Network Ensembles, Cross Validation, and Active Learning',\n",
       "  '',\n",
       "  '233',\n",
       "  '',\n",
       "  'All these formulas can be averaged over the input distribution . Averages over the',\n",
       "  'input distribution will be denoted by capital letter, so',\n",
       "  '',\n",
       "  'J dxp(xVl! (x)',\n",
       "  'J dxp(x)aa(x)',\n",
       "  'J dxp(x)e(x).',\n",
       "  '',\n",
       "  'E',\n",
       "  '',\n",
       "  '(7)',\n",
       "  '(8)',\n",
       "  '(9)',\n",
       "  '',\n",
       "  'The first two of these are the generalization error and the ambiguity respectively',\n",
       "  'for network n , and E is the generalization error for the ensemble. From (6) we then',\n",
       "  'find for the ensemble generalization error',\n",
       "  '(10)',\n",
       "  'The first term on the right is the weighted average of the generalization errors of',\n",
       "  'the individual networks (E = La waEa), and the second is the weighted average',\n",
       "  'of the ambiguities (A = La WaAa), which we refer to as the ensemble ambiguity.',\n",
       "  'The beauty of this equation is that it separates the generalization error into a term',\n",
       "  'that depends on the generalization errors of the individual networks and another',\n",
       "  'term that contain all correlations between the networks . Furthermore, the correlation term A can be estimated entirely from unlabeled data, i. e., no knowledge is',\n",
       "  'required of the real function to be approximated. The term \"unlabeled example\" is',\n",
       "  'borrowed from classification problems, and in this context it means an input x for',\n",
       "  'which the value of the target function f( x) is unknown.',\n",
       "  'Equation (10) expresses the tradeoff between bias and variance in the ensemble ,',\n",
       "  'but in a different way than the the common bias-variance relation [4] in which the',\n",
       "  'averages are over possible training sets instead of ensemble averages. If the ensemble',\n",
       "  'is strongly biased the ambiguity will be small , because the networks implement very',\n",
       "  'similar functions and thus agree on inputs even outside the training set. Therefore',\n",
       "  'the generalization error will be essentially equal to the weighted average of the',\n",
       "  'generalization errors of the individual networks. If, on the other hand , there is a',\n",
       "  'large variance , the ambiguity is high and in this case the generalization error will',\n",
       "  'be smaller than the average generalization error . See also [5].',\n",
       "  'From this equation one can immediately see that the generalization error of the',\n",
       "  'ensemble is always smaller than the (weighted) average of the ensemble errors,',\n",
       "  'E < E. In particular for uniform weights:',\n",
       "  '',\n",
       "  'E',\n",
       "  '',\n",
       "  \"~ ~ 'fEcx\",\n",
       "  '',\n",
       "  '(11)',\n",
       "  '',\n",
       "  'which has been noted by several authors , see e.g. [3] .',\n",
       "  '',\n",
       "  '3',\n",
       "  '',\n",
       "  'THE CROSS-VALIDATION ENSEMBLE',\n",
       "  '',\n",
       "  'From (10) it is obvious that increasing the ambiguity (while not increasing individual',\n",
       "  'generalization errors) will improve the overall generalization. We want the networks',\n",
       "  'to disagree! How can we increase the ambiguity of the ensemble? One way is to',\n",
       "  'use different types of approximators like a mixture of neural networks of different',\n",
       "  'topologies or a mixture of completely different types of approximators. Another',\n",
       "  '',\n",
       "  '\\x0c234',\n",
       "  '',\n",
       "  'Anders Krogh, Jesper Vedelsby',\n",
       "  '',\n",
       "  '.',\n",
       "  '',\n",
       "  ':~',\n",
       "  '',\n",
       "  '1. -',\n",
       "  '',\n",
       "  't',\n",
       "  '',\n",
       "  '-',\n",
       "  '',\n",
       "  \",',\",\n",
       "  '',\n",
       "  '.. ,',\n",
       "  '',\n",
       "  \"E o...... -' '.- .. ' ........ ....,.\",\n",
       "  '',\n",
       "  \".'\",\n",
       "  '',\n",
       "  '..... , ...',\n",
       "  '',\n",
       "  \"v '. --:\",\n",
       "  '',\n",
       "  ',',\n",
       "  '',\n",
       "  '.~.--c??',\n",
       "  '',\n",
       "  '__ .. -.tI\"',\n",
       "  '',\n",
       "  '.',\n",
       "  '',\n",
       "  '. -- - -\\\\\\\\',\n",
       "  '',\n",
       "  \"'1\",\n",
       "  '',\n",
       "  '-',\n",
       "  '',\n",
       "  '.~',\n",
       "  '',\n",
       "  '~.',\n",
       "  '',\n",
       "  ', . _ ? .\" ?',\n",
       "  '',\n",
       "  '.. - .....',\n",
       "  '',\n",
       "  \"_._ ..... .'-._._.1\",\n",
       "  '',\n",
       "  ',',\n",
       "  '',\n",
       "  '-',\n",
       "  '',\n",
       "  '>',\n",
       "  '',\n",
       "  '-',\n",
       "  '',\n",
       "  '-1.k!',\n",
       "  '~',\n",
       "  '',\n",
       "  '-4',\n",
       "  '',\n",
       "  '.t.',\n",
       "  '',\n",
       "  'f.',\n",
       "  '',\n",
       "  '1\\\\.1',\n",
       "  '',\n",
       "  \":\\\\,'. - ?-.l\",\n",
       "  '',\n",
       "  ':--,____',\n",
       "  '..',\n",
       "  '',\n",
       "  '~~',\n",
       "  '.',\n",
       "  '',\n",
       "  '~.',\n",
       "  '',\n",
       "  ',',\n",
       "  '',\n",
       "  \",'\",\n",
       "  '',\n",
       "  '-2',\n",
       "  '',\n",
       "  '.~',\n",
       "  '',\n",
       "  'If',\n",
       "  '',\n",
       "  'o',\n",
       "  '',\n",
       "  '2',\n",
       "  '',\n",
       "  '\\\\.',\n",
       "  '~',\n",
       "  ':',\n",
       "  '?',\n",
       "  '',\n",
       "  \"' 0'\",\n",
       "  '',\n",
       "  '~:',\n",
       "  '',\n",
       "  '4',\n",
       "  '',\n",
       "  'x',\n",
       "  '',\n",
       "  'Figure 1: An ensemble of five networks were trained to approximate the square',\n",
       "  'wave target function f(x). The final ensemble output (solid smooth curve) and',\n",
       "  'the outputs of the individual networks (dotted curves) are shown. Also the square',\n",
       "  'root of the ambiguity is shown (dash-dot line) _ For training 200 random examples',\n",
       "  'were used, but each network had a cross-validation set of size 40, so they were each',\n",
       "  'trained on 160 examples.',\n",
       "  '',\n",
       "  'obvious way is to train the networks on different training sets. Furthermore, to be',\n",
       "  'able to estimate the first term in (10) it would be desirable to have some kind of',\n",
       "  'cross-validation. This suggests the following strategy.',\n",
       "  'Chose a number K :::; p. For each network in the ensemble hold out K examples for',\n",
       "  'testing, where the N test sets should have minimal overlap, i. e., the N training sets',\n",
       "  'should be as different as possible. If, for instance, K :::; piN it is possible to choose',\n",
       "  'the K test sets with no overlap. This enables us to estimate the generalization error',\n",
       "  'E(X of the individual members of the ensemble, and at the same time make sure',\n",
       "  'that the ambiguity increases . When holding out examples the generalization errors',\n",
       "  'for the individual members of the ensemble, E(X, will increase, but the conjecture',\n",
       "  'is that for a good choice of the size of the ensemble (N) and the test set size',\n",
       "  '(K), the ambiguity will increase more and thus one will get a decrease in overall',\n",
       "  'generalization error.',\n",
       "  'This conjecture has been tested experimentally on a simple square wave function',\n",
       "  'of one variable shown in Figure 1. Five identical feed-forward networks with one',\n",
       "  'hidden layer of 20 units were trained independently by back-propagation using 200',\n",
       "  'random examples. For each network a cross-validation set of K examples was held',\n",
       "  'out for testing as described above. The \"true\" generalization and the ambiguity were',\n",
       "  'estimated from a set of 1000 random inputs. The weights were uniform, w(X',\n",
       "  '1/5',\n",
       "  '(non-uniform weights are addressed later).',\n",
       "  '',\n",
       "  '=',\n",
       "  '',\n",
       "  'In Figure 2 average results over 12 independent runs are shown for some values of',\n",
       "  '',\n",
       "  '\\x0cNeural Network Ensembles, Cross Validation, and Active Learning',\n",
       "  '',\n",
       "  'Figure 2: The solid line shows the generalization error for uniform weights as',\n",
       "  'a function of K, where K is the size',\n",
       "  'of the cross-validation sets. The dotted',\n",
       "  'line is the error estimated from equation (10) . The dashed line is for the',\n",
       "  'optimal weights estimated by the use of',\n",
       "  'the generalization errors for the individual networks estimated from the crossvalidation sets as described in the text.',\n",
       "  'The bottom solid line is the generalization error one would obtain if the individual generalization errors were known',\n",
       "  'exactly (the best possible weights).',\n",
       "  '',\n",
       "  '0.08',\n",
       "  '',\n",
       "  '235',\n",
       "  '',\n",
       "  ',-----r----,--~---r-----,',\n",
       "  '',\n",
       "  'o',\n",
       "  '',\n",
       "  't=',\n",
       "  'w',\n",
       "  '0.06',\n",
       "  '',\n",
       "  'c',\n",
       "  '',\n",
       "  'o',\n",
       "  '~',\n",
       "  '',\n",
       "  '.!::!',\n",
       "  '',\n",
       "  'co...',\n",
       "  '',\n",
       "  '~ 0.04',\n",
       "  '',\n",
       "  'Q)',\n",
       "  '',\n",
       "  '(!)',\n",
       "  '',\n",
       "  \"0 .02 '---_---1_ _---'-_ _--'-_ _-----'\",\n",
       "  'o',\n",
       "  '20',\n",
       "  '40',\n",
       "  '60',\n",
       "  '80',\n",
       "  'Size of CV set',\n",
       "  '',\n",
       "  'K (top solid line) . First, one should note that the generalization error is the same',\n",
       "  'for a cross-validation set of size 40 as for size 0, although not lower, so it supports',\n",
       "  'the conjecture in a weaker form. However, we have done many experiments, and',\n",
       "  'depending on the experimental setup the curve can take on almost any form, sometimes the error is larger at zero than at 40 or vice versa. In the experiments shown,',\n",
       "  'only ensembles with at least four converging networks out of five were used . If all',\n",
       "  'the ensembles were kept, the error would have been significantly higher at ]{ = a',\n",
       "  'than for K > a because in about half of the runs none of the networks in the ensemble converged - something that seldom happened when a cross-validation set',\n",
       "  'was used. Thus it is still unclear under which circumstances one can expect a drop',\n",
       "  'in generalization error when using cross-validation in this fashion.',\n",
       "  '',\n",
       "  'The dotted line in Figure 2 is the error estimated from equation (10) using the',\n",
       "  'cross-validation sets for each of the networks to estimate Ea, and one notices a',\n",
       "  'good agreement.',\n",
       "  '',\n",
       "  '4',\n",
       "  '',\n",
       "  'OPTIMAL WEIGHTS',\n",
       "  '',\n",
       "  'The weights Wa can be estimated as described in e.g. [3]. We suggest instead',\n",
       "  'to use unlabeled data and estimate them in such a way that they minimize the',\n",
       "  'generalization error given in (10) .',\n",
       "  'There is no analytical solution for the weights , but something can be said about',\n",
       "  'the minimum point of the generalization error. Calculating the derivative of E as',\n",
       "  'given in (10) subject to the constraints on the weights and setting it equal to zero',\n",
       "  'shows that',\n",
       "  'Ea - Aa',\n",
       "  'E or Wa = O.',\n",
       "  '(12)',\n",
       "  '',\n",
       "  '=',\n",
       "  '',\n",
       "  '(The calculation is not shown because of space limitations, but it is easy to do.)',\n",
       "  'That is, Ea - Aa has to be the same for all the networks. Notice that Aa depends',\n",
       "  'on the weights through the ensemble average of the outputs. It shows that the',\n",
       "  'optimal weights have to be chosen such that each network contributes exactly waE',\n",
       "  '',\n",
       "  '\\x0c236',\n",
       "  '',\n",
       "  'Anders Krogh, Jesper Vedelsby',\n",
       "  '',\n",
       "  'to the generalization error. Note, however, that a member of the ensemble can have',\n",
       "  'such a poor generalization or be so correlated with the rest of the ensemble that it',\n",
       "  'is optimal to set its weight to zero.',\n",
       "  'The weights can be \"learned\" from unlabeled examples, e.g. by gradient descent',\n",
       "  'minimization of the estimate of the generalization error (10). A more efficient',\n",
       "  'approach to finding the optimal weights is to turn it into a quadratic optimization',\n",
       "  'problem. That problem is non-trivial only because of the constraints on the weights',\n",
       "  '(L:a Wa = 1 and Wa 2:: 0). Define the correlation matrix,',\n",
       "  'C af3',\n",
       "  '',\n",
       "  '=',\n",
       "  '',\n",
       "  'f',\n",
       "  '',\n",
       "  'dxp(x)V a (x)V f3 (x) .',\n",
       "  '',\n",
       "  '(13)',\n",
       "  '',\n",
       "  'Then, using that the weights sum to one, equation (10) can be rewritten as',\n",
       "  'E',\n",
       "  '',\n",
       "  '=',\n",
       "  '',\n",
       "  'L',\n",
       "  'a',\n",
       "  '',\n",
       "  'wa Ea',\n",
       "  '',\n",
       "  '+ L w a C af3 w f3 - L',\n",
       "  'af3',\n",
       "  '',\n",
       "  'waCaa .',\n",
       "  '',\n",
       "  '(14)',\n",
       "  '',\n",
       "  'a',\n",
       "  '',\n",
       "  'Having estimates of E a and C af3 the optimal weights can be found by linear programming or other optimization techniques. Just like the ambiguity, the correlation',\n",
       "  'matrix can be estimated from unlabeled data to any accuracy needed (provided that',\n",
       "  'the input distribution p is known).',\n",
       "  'In Figure 2 the results from an experiment with weight optimization are shown.',\n",
       "  'The dashed curve shows the generalization error when the weights are optimized as',\n",
       "  'described above using the estimates of Ea from the cross-validation (on K exampies). The lowest solid curve is for the idealized case, when it is assumed that the',\n",
       "  'errors Ea are known exactly, so it shows the lowest possible error. The performance',\n",
       "  'improvement is quite convincing when the cross-validation estimates are used.',\n",
       "  'It is important to notice that any estimate of the generalization error of the individual networks can be used in equation (14). If one is certain that the individual',\n",
       "  'networks do not overfit, one might even use the training errors as estimates for',\n",
       "  'Ea (see [3]). It is also possible to use some kind of regularization in (14), if the',\n",
       "  'cross-validation sets are small.',\n",
       "  '',\n",
       "  '5',\n",
       "  '',\n",
       "  'ACTIVE LEARNING',\n",
       "  '',\n",
       "  'In some neural network applications it is very time consuming and/or expensive',\n",
       "  'to acquire training data, e.g., if a complicated measurement is required to find the',\n",
       "  'value of the target function for a certain input. Therefore it is desirable to only use',\n",
       "  'examples with maximal information about the function. Methods where the learner',\n",
       "  'points out good examples are often called active learning.',\n",
       "  'We propose a query-based active learning scheme that applies to ensembles of networks with continuous-valued output. It is essentially a generalization of query by',\n",
       "  'committee [6, 7] that was developed for classification problems. Our basic assumption is that those patterns in the input space yielding the largest error are those',\n",
       "  'points we would benefit the most from including in the training set.',\n",
       "  'Since the generalization error is always non-negative, we see from (6) that the',\n",
       "  'weighted average of the individual network errors is always larger than or equal to',\n",
       "  'the ensemble ambiguity,',\n",
       "  'f(X) 2:: a(x),',\n",
       "  '(15)',\n",
       "  '',\n",
       "  '\\x0cNeural Network Ensembles. Cross Validation. and Active Learning',\n",
       "  '',\n",
       "  '237',\n",
       "  '',\n",
       "  '2.5 r\"\\':\":\\'T---r--\"T\"\"--.-----r---,',\n",
       "  '',\n",
       "  '.',\n",
       "  '',\n",
       "  '.',\n",
       "  '',\n",
       "  '.',\n",
       "  '',\n",
       "  ':',\n",
       "  '',\n",
       "  '0.5',\n",
       "  '',\n",
       "  'o',\n",
       "  '',\n",
       "  '10',\n",
       "  '',\n",
       "  '20',\n",
       "  '',\n",
       "  '30',\n",
       "  '',\n",
       "  'Training set size',\n",
       "  '',\n",
       "  '40',\n",
       "  '',\n",
       "  '50',\n",
       "  '',\n",
       "  'o',\n",
       "  '',\n",
       "  '10',\n",
       "  '',\n",
       "  '20',\n",
       "  '',\n",
       "  '30',\n",
       "  '',\n",
       "  '40',\n",
       "  '',\n",
       "  '50',\n",
       "  '',\n",
       "  'Training set size',\n",
       "  '',\n",
       "  'Figure 3: In both plots the full line shows the average generalization for active',\n",
       "  'learning, and the dashed line for passive learning as a function of the number of',\n",
       "  'training examples. The dots in the left plot show the results of the individual',\n",
       "  'experiments contributing to the mean for the active learning. The dots in right plot',\n",
       "  'show the same for passive learning.',\n",
       "  '',\n",
       "  'which tells us that the ambiguity is a lower bound for the weighted average of the',\n",
       "  'squared error. An input pattern that yields a large ambiguity will always have a',\n",
       "  'large average error. On the other hand, a low ambiguity does not necessarily imply',\n",
       "  'a low error. If the individual networks are trained to a low training error on the',\n",
       "  'same set of examples then both the error and the ambiguity are low on the training',\n",
       "  'points. This ensures that a pattern yielding a large ambiguity cannot be in the close',\n",
       "  'neighborhood of a training example. The ambiguity will to some extent follow the',\n",
       "  'fluctuations in the error. Since the ambiguity is calculated from unlabeled examples',\n",
       "  'the input-space can be scanned for these areas to any detail. These ideas are well',\n",
       "  'illustrated in Figure 1, where the correlation between error and ambiguity is quite',\n",
       "  'strong, although not perfect.',\n",
       "  'The results of an experiment with the active learning scheme is shown in Figure 3.',\n",
       "  'An ensemble of 5 networks was trained to approximate the square-wave function',\n",
       "  'shown in Figure 1, but in this experiments the function was restricted to the interval',\n",
       "  'from - 2 to 2. The curves show the final generalization error of the ensemble in a',\n",
       "  'passive (dashed line) and an active learning test (solid line). For each training set',\n",
       "  'size 2x40 independent tests were made, all starting with the same initial training',\n",
       "  'set of a single example. Examples were generated and added one at a time. In the',\n",
       "  'passive test examples were generated at random, and in the active one each example',\n",
       "  'was selected as the input that gave the largest ambiguity out of 800 random ones.',\n",
       "  'Figure 3 also shows the distribution of the individual results of the active and',\n",
       "  'passive learning tests. Not only do we obtain significantly better generalization by',\n",
       "  'active learning, there is also less scatter in the results. It seems to be easier for the',\n",
       "  'ensemble to learn from the actively generated set.',\n",
       "  '',\n",
       "  '\\x0c238',\n",
       "  '',\n",
       "  '6',\n",
       "  '',\n",
       "  'Anders Krogh. Jesper Vedelsby',\n",
       "  '',\n",
       "  'CONCLUSION',\n",
       "  '',\n",
       "  'The central idea in this paper was to show that there is a lot to be gained from',\n",
       "  'using unlabeled data when training in ensembles. Although we dealt with neural',\n",
       "  'networks, all the theory holds for any other type of method used as the individual',\n",
       "  'members of the ensemble.',\n",
       "  'It was shown that apart from getting the individual members of the ensemble to',\n",
       "  'generalize well, it is important for generalization that the individuals disagrees as',\n",
       "  'much as possible, and we discussed one method to make even identical networks',\n",
       "  'disagree. This was done by training the individuals on different training sets by',\n",
       "  'holding out some examples for each individual during training. This had the added',\n",
       "  'advantage that these examples could be used for testing, and thereby one could',\n",
       "  'obtain good estimates of the generalization error.',\n",
       "  'It was discussed how to find the optimal weights for the individuals of the ensemble.',\n",
       "  'For our simple test problem the weights found improved the performance of the',\n",
       "  'ensemble significantly.',\n",
       "  '',\n",
       "  'Finally a method for active learning was described, which was based on the method',\n",
       "  'of query by committee developed for classification problems. The idea is that if the',\n",
       "  'ensemble disagrees strongly on an input, it would be good to find the label for that',\n",
       "  'input and include it in the training set for the ensemble. It was shown how active',\n",
       "  'learning improves the learning curve a lot for a simple test problem.',\n",
       "  'Acknowledgements',\n",
       "  '',\n",
       "  'We would like to thank Peter Salamon for numerous discussions and for his implementation of linear programming for optimization of the weights. We also thank',\n",
       "  'Lars Kai Hansen for many discussions and great insights, and David Wolpert for',\n",
       "  'valuable comments.',\n",
       "  '',\n",
       "  'References',\n",
       "  '[1] L.K. Hansen and P Salamon. Neural network ensembles. IEEE Transactions on',\n",
       "  'Pattern Analysis and Machine Intelligence, 12(10):993- 1001, Oct. 1990.',\n",
       "  '[2] D.H Wolpert. Stacked generalization. Neural Networks, 5(2):241-59, 1992.',\n",
       "  '[3] Michael P. Perrone and Leon N Cooper. When networks disagree: Ensemble method',\n",
       "  'for neural networks. In R. J. Mammone, editor, Neural Networks for Speech and Image',\n",
       "  'processing. Chapman-Hall, 1993.',\n",
       "  '[4] S. Geman , E . Bienenstock, and R Doursat. Neural networks and the bias/variance',\n",
       "  'dilemma. Neural Computation, 4(1):1-58, Jan. 1992.',\n",
       "  '[5] Ronny Meir. Bias, variance and the combination of estimators; the case of linear least',\n",
       "  'squares. Preprint (In Neuroprose), Technion, Heifa, Israel, 1994.',\n",
       "  '[6] H.S. Seung, M. Opper, and H. Sompolinsky. Query by committee. In Proceedings of',\n",
       "  'the Fifth Workshop on Computational Learning Theory, pages 287-294, San Mateo,',\n",
       "  'CA, 1992. Morgan Kaufmann.',\n",
       "  '[7] Y. Freund, H.S. Seung, E. Shamir, and N. Tishby. Information, prediction, and query',\n",
       "  'by committee. In Advances in Neural Information Processing Systems, volume 5, San',\n",
       "  'Mateo, California, 1993. Morgan Kaufmann.',\n",
       "  '',\n",
       "  '\\x0c']]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** **\n",
    "#### Step 2: Data Cleaning <a class=\"anchor\\\" id=\"clean_data\"></a>\n",
    "** **\n",
    "\n",
    "Since the goal of this analysis is to perform topic modeling, let's focus only on the text data from each paper, and drop other metadata columns. Also, for the demonstration, we'll only look at 100 papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>paper_text</th>\n",
       "      <th>symbols</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5301</th>\n",
       "      <td>1987</td>\n",
       "      <td>Analysis of Distributed Representation of Cons...</td>\n",
       "      <td>Abstract Missing</td>\n",
       "      <td>730\\n\\nAnalysis of distributed representation ...</td>\n",
       "      <td>35404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2171</th>\n",
       "      <td>2006</td>\n",
       "      <td>Learning to Rank with Nonsmooth Cost Functions</td>\n",
       "      <td>Abstract Missing</td>\n",
       "      <td>Learning to Rank with Nonsmooth Cost Functions...</td>\n",
       "      <td>30924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1229</th>\n",
       "      <td>1989</td>\n",
       "      <td>Time Dependent Adaptive Neural Networks</td>\n",
       "      <td>Abstract Missing</td>\n",
       "      <td>710\\n\\nPineda\\n\\nTime DependentAdaptive Neural...</td>\n",
       "      <td>22289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2251</th>\n",
       "      <td>2006</td>\n",
       "      <td>Statistical Modeling of Images with Fields of ...</td>\n",
       "      <td>Abstract Missing</td>\n",
       "      <td>Statistical Modeling of Images with\\nFields of...</td>\n",
       "      <td>29856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2305</th>\n",
       "      <td>2006</td>\n",
       "      <td>iLSTD: Eligibility Traces and Convergence Anal...</td>\n",
       "      <td>Abstract Missing</td>\n",
       "      <td>iLSTD: Eligibility Traces and Convergence Anal...</td>\n",
       "      <td>26711</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      year                                              title  \\\n",
       "5301  1987  Analysis of Distributed Representation of Cons...   \n",
       "2171  2006     Learning to Rank with Nonsmooth Cost Functions   \n",
       "1229  1989            Time Dependent Adaptive Neural Networks   \n",
       "2251  2006  Statistical Modeling of Images with Fields of ...   \n",
       "2305  2006  iLSTD: Eligibility Traces and Convergence Anal...   \n",
       "\n",
       "              abstract                                         paper_text  \\\n",
       "5301  Abstract Missing  730\\n\\nAnalysis of distributed representation ...   \n",
       "2171  Abstract Missing  Learning to Rank with Nonsmooth Cost Functions...   \n",
       "1229  Abstract Missing  710\\n\\nPineda\\n\\nTime DependentAdaptive Neural...   \n",
       "2251  Abstract Missing  Statistical Modeling of Images with\\nFields of...   \n",
       "2305  Abstract Missing  iLSTD: Eligibility Traces and Convergence Anal...   \n",
       "\n",
       "      symbols  \n",
       "5301    35404  \n",
       "2171    30924  \n",
       "1229    22289  \n",
       "2251    29856  \n",
       "2305    26711  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove the columns\n",
    "papers = papers.drop(columns=['id', 'event_type', 'pdf_name'], axis=1).sample(1000)\n",
    "\n",
    "# Print out the first rows of papers\n",
    "papers.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Remove punctuation/lower casing\n",
    "\n",
    "Next, let’s perform a simple preprocessing on the content of `paper_text` column to make them more amenable for analysis, and reliable results. To do that, we’ll use a regular expression to remove any punctuation, and then lowercase the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5301    730\\n\\nanalysis of distributed representation ...\n",
       "2171    learning to rank with nonsmooth cost functions...\n",
       "1229    710\\n\\npineda\\n\\ntime dependentadaptive neural...\n",
       "2251    statistical modeling of images with\\nfields of...\n",
       "2305    ilstd: eligibility traces and convergence anal...\n",
       "Name: paper_text_processed, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the regular expression library\n",
    "import re\n",
    "\n",
    "# Remove punctuation\n",
    "papers['paper_text_processed'] = \\\n",
    "papers['paper_text'].map(lambda x: re.sub('[,\\.!?]', '', x))\n",
    "\n",
    "# Convert the titles to lowercase\n",
    "papers['paper_text_processed'] = \\\n",
    "papers['paper_text_processed'].map(lambda x: x.lower())\n",
    "\n",
    "# Print out the first rows of papers\n",
    "papers['paper_text_processed'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** **\n",
    "#### Step 3: Exploratory Analysis <a class=\"anchor\\\" id=\"eda\"></a>\n",
    "** **\n",
    "\n",
    "To verify whether the preprocessing, we’ll make a simple word cloud using the `wordcloud` package to get a visual representation of most common words. It is key to understanding the data and ensuring we are on the right track, and if any more preprocessing is necessary before training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=400x200 at 0x1C83AC66EB0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the wordcloud library\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Join the different processed titles together.\n",
    "long_string = ','.join(list(papers['paper_text_processed'].values))\n",
    "\n",
    "# Create a WordCloud object\n",
    "wordcloud = WordCloud(background_color=\"white\", max_words=1000, contour_width=3, contour_color='steelblue')\n",
    "\n",
    "# Generate a word cloud\n",
    "wordcloud.generate(long_string)\n",
    "\n",
    "# Visualize the word cloud\n",
    "wordcloud.to_image()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** **\n",
    "#### Step 4: Prepare text for LDA analysis <a class=\"anchor\\\" id=\"data_preparation\"></a>\n",
    "** **\n",
    "\n",
    "Next, let’s work to transform the textual data in a format that will serve as an input for training LDA model. We start by tokenizing the text and removing stopwords. Next, we convert the tokenized object into a corpus and dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Keller\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['analysis', 'distributed', 'representation', 'constituent', 'structure', 'connectionist', 'systems', 'paul', 'smolensky', 'department', 'computer', 'science', 'university', 'colorado', 'boulder', 'co', 'abstract', 'general', 'method', 'tensor', 'product', 'representation', 'described', 'distributed', 'representation', 'value', 'variable', 'bindings', 'method', 'allows']\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])\n",
    "\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        # deacc=True removes punctuations\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))\n",
    "\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) \n",
    "             if word not in stop_words] for doc in texts]\n",
    "\n",
    "\n",
    "data = papers.paper_text_processed.values.tolist()\n",
    "data_words = list(sent_to_words(data))\n",
    "\n",
    "# remove stop words\n",
    "data_words = remove_stopwords(data_words)\n",
    "\n",
    "print(data_words[:1][0][:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Keller\\anaconda3\\lib\\site-packages\\numpy\\core\\_asarray.py:102: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1000,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.shape(data_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'analysis'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_words[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 2), (6, 3), (7, 1), (8, 2), (9, 4), (10, 26), (11, 1), (12, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 3), (19, 1)]\n"
     ]
    }
   ],
   "source": [
    "import gensim.corpora as corpora\n",
    "\n",
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_words)\n",
    "\n",
    "# Create Corpus\n",
    "texts = data_words\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "# View\n",
    "print(corpus[0][:][:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(900, 2)\n",
      "(2922,)\n",
      "(1000,)\n",
      "(900,)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(corpus[0]))\n",
    "print(np.shape(data_words[0]))\n",
    "print(np.shape(data_words))\n",
    "print(np.shape(np.unique(data_words[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** **\n",
    "#### Step 5: LDA model tranining <a class=\"anchor\\\" id=\"train_model\"></a>\n",
    "** **\n",
    "\n",
    "To keep things simple, we'll keep all the parameters to default except for inputting the number of topics. For this tutorial, we will build a model with 10 topics where each topic is a combination of keywords, and each keyword contributes a certain weightage to the topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.007*\"model\" + 0.006*\"learning\" + 0.005*\"data\" + 0.004*\"function\" + '\n",
      "  '0.004*\"set\" + 0.004*\"one\" + 0.004*\"using\" + 0.004*\"number\" + 0.003*\"figure\" '\n",
      "  '+ 0.003*\"time\"'),\n",
      " (1,\n",
      "  '0.007*\"learning\" + 0.006*\"model\" + 0.006*\"algorithm\" + 0.006*\"set\" + '\n",
      "  '0.005*\"data\" + 0.004*\"number\" + 0.004*\"function\" + 0.003*\"using\" + '\n",
      "  '0.003*\"time\" + 0.003*\"models\"'),\n",
      " (2,\n",
      "  '0.006*\"model\" + 0.006*\"learning\" + 0.005*\"data\" + 0.005*\"using\" + '\n",
      "  '0.005*\"algorithm\" + 0.004*\"matrix\" + 0.004*\"one\" + 0.004*\"function\" + '\n",
      "  '0.004*\"set\" + 0.004*\"two\"'),\n",
      " (3,\n",
      "  '0.005*\"model\" + 0.005*\"data\" + 0.005*\"algorithm\" + 0.004*\"learning\" + '\n",
      "  '0.004*\"function\" + 0.004*\"problem\" + 0.004*\"using\" + 0.004*\"time\" + '\n",
      "  '0.003*\"two\" + 0.003*\"also\"'),\n",
      " (4,\n",
      "  '0.007*\"model\" + 0.006*\"data\" + 0.005*\"algorithm\" + 0.005*\"learning\" + '\n",
      "  '0.004*\"using\" + 0.004*\"set\" + 0.004*\"figure\" + 0.004*\"function\" + '\n",
      "  '0.004*\"one\" + 0.004*\"models\"'),\n",
      " (5,\n",
      "  '0.008*\"learning\" + 0.006*\"model\" + 0.006*\"data\" + 0.005*\"algorithm\" + '\n",
      "  '0.005*\"set\" + 0.004*\"function\" + 0.004*\"time\" + 0.004*\"one\" + '\n",
      "  '0.004*\"results\" + 0.003*\"problem\"'),\n",
      " (6,\n",
      "  '0.006*\"data\" + 0.006*\"model\" + 0.005*\"algorithm\" + 0.005*\"using\" + '\n",
      "  '0.005*\"learning\" + 0.004*\"set\" + 0.004*\"time\" + 0.004*\"training\" + '\n",
      "  '0.003*\"figure\" + 0.003*\"one\"'),\n",
      " (7,\n",
      "  '0.007*\"learning\" + 0.006*\"model\" + 0.006*\"function\" + 0.005*\"set\" + '\n",
      "  '0.004*\"data\" + 0.004*\"using\" + 0.004*\"algorithm\" + 0.004*\"time\" + '\n",
      "  '0.003*\"two\" + 0.003*\"one\"'),\n",
      " (8,\n",
      "  '0.007*\"model\" + 0.006*\"learning\" + 0.005*\"data\" + 0.004*\"using\" + '\n",
      "  '0.004*\"set\" + 0.004*\"algorithm\" + 0.004*\"one\" + 0.003*\"figure\" + '\n",
      "  '0.003*\"function\" + 0.003*\"number\"'),\n",
      " (9,\n",
      "  '0.007*\"learning\" + 0.006*\"data\" + 0.005*\"model\" + 0.005*\"problem\" + '\n",
      "  '0.005*\"algorithm\" + 0.004*\"two\" + 0.004*\"time\" + 0.004*\"using\" + '\n",
      "  '0.004*\"one\" + 0.004*\"set\"')]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "# number of topics\n",
    "num_topics = 10\n",
    "\n",
    "# Build LDA model\n",
    "lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                       id2word=id2word,\n",
    "                                       num_topics=num_topics)\n",
    "\n",
    "# Print the Keyword in the 10 topics\n",
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** **\n",
    "#### Step 6: Analyzing our LDA model <a class=\"anchor\\\" id=\"results\"></a>\n",
    "** **\n",
    "\n",
    "Now that we have a trained model let’s visualize the topics for interpretability. To do so, we’ll use a popular visualization package, pyLDAvis which is designed to help interactively with:\n",
    "\n",
    "1. Better understanding and interpreting individual topics, and\n",
    "2. Better understanding the relationships between the topics.\n",
    "\n",
    "For (1), you can manually select each topic to view its top most frequent and/or “relevant” terms, using different values of the λ parameter. This can help when you’re trying to assign a human interpretable name or “meaning” to each topic.\n",
    "\n",
    "For (2), exploring the Intertopic Distance Plot can help you learn about how topics relate to each other, including potential higher-level structure between groups of topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim_models as gensimvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Keller\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el1850019598491286725129869264\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el1850019598491286725129869264_data = {\"mdsDat\": {\"x\": [0.007117513152997866, 0.0013918255319595137, 0.0005801266360562446, -0.0003244962870941262, 0.0011399721685622912, 0.00014773926216038607, -0.00471330735355362, -0.0008224754663002352, -0.0017224162468790004, -0.0027944813979093293], \"y\": [0.0005732191085977341, -0.0018163089694047737, -0.0018297403635166851, -0.0006638663710019314, 0.003767005654250397, -0.0014930949501422196, 0.0035617615274482926, 0.00275370884094968, 0.0006643766038257623, -0.005517061081006253], \"topics\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], \"cluster\": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], \"Freq\": [17.34491065903005, 13.200734174858125, 11.29855157201118, 10.86319288037611, 10.28970966951021, 8.498947597117834, 8.138272137956653, 8.010392949253745, 6.6493904654223615, 5.70589789446373]}, \"tinfo\": {\"Term\": [\"model\", \"learning\", \"set\", \"data\", \"function\", \"using\", \"figure\", \"models\", \"state\", \"number\", \"algorithm\", \"training\", \"one\", \"probability\", \"used\", \"neural\", \"space\", \"time\", \"network\", \"linear\", \"functions\", \"input\", \"results\", \"error\", \"given\", \"case\", \"image\", \"order\", \"parameters\", \"feature\", \"nci\", \"kgr\", \"kveh\", \"halting\", \"kvh\", \"tottering\", \"keh\", \"enzymes\", \"kvehg\", \"duchenne\", \"mise\", \"mutag\", \"labelers\", \"mahito\", \"weisfeiler\", \"hawk\", \"lehman\", \"maxgg\", \"labeler\", \"mmh\", \"parzen\", \"hamm\", \"greebles\", \"wildlife\", \"glad\", \"argyy\", \"mns\", \"optspace\", \"listnet\", \"distillation\", \"npn\", \"bob\", \"iml\", \"koopman\", \"matrix\", \"walk\", \"macbeth\", \"lsm\", \"xi\", \"images\", \"laplacian\", \"method\", \"documents\", \"random\", \"using\", \"multi\", \"labels\", \"distribution\", \"label\", \"one\", \"approach\", \"number\", \"based\", \"information\", \"model\", \"function\", \"objective\", \"two\", \"show\", \"log\", \"data\", \"methods\", \"image\", \"likelihood\", \"algorithms\", \"learning\", \"figure\", \"algorithm\", \"problem\", \"values\", \"example\", \"analysis\", \"set\", \"parameters\", \"also\", \"given\", \"network\", \"different\", \"used\", \"time\", \"models\", \"results\", \"lemasson\", \"pyloric\", \"axoclamp\", \"inters\", \"golowasch\", \"priigel\", \"hyperpolarized\", \"gmmmfcc\", \"demultiplexing\", \"stomatogastric\", \"llinas\", \"smrsdn\", \"mfccs\", \"yarom\", \"iifx\", \"stasheff\", \"bordeaux\", \"olivary\", \"subnets\", \"marder\", \"buchholtz\", \"tam\", \"arcachon\", \"macintosh\", \"smmfcc\", \"variabilities\", \"hscrfs\", \"hindmarsh\", \"crab\", \"ohi\", \"adex\", \"predominant\", \"spk\", \"mkl\", \"speaker\", \"mcm\", \"interspike\", \"speech\", \"pgas\", \"neuron\", \"problem\", \"delays\", \"kernel\", \"weights\", \"two\", \"neurons\", \"tensor\", \"learning\", \"time\", \"training\", \"log\", \"input\", \"graph\", \"regression\", \"information\", \"recognition\", \"first\", \"consider\", \"however\", \"based\", \"data\", \"local\", \"one\", \"results\", \"given\", \"used\", \"vector\", \"algorithm\", \"probability\", \"using\", \"function\", \"model\", \"set\", \"also\", \"error\", \"different\", \"linear\", \"figure\", \"distribution\", \"number\", \"matrix\", \"network\", \"ettt\", \"rtcr\", \"lyapnov\", \"convexokl\", \"lgt\", \"humphreys\", \"okl\", \"wiles\", \"bain\", \"krs\", \"kgt\", \"fmtl\", \"nline\", \"argminkl\", \"rescaledexp\", \"klmax\", \"rekl\", \"rtcrdgtgt\", \"ybt\", \"asymmetricity\", \"ygt\", \"alstr\", \"invex\", \"lmax\", \"prestojst\", \"fins\", \"noboru\", \"silverstein\", \"rtcrd\", \"adagrad\", \"wake\", \"mtrl\", \"anificial\", \"gt\", \"mcp\", \"sleeping\", \"party\", \"fmtlp\", \"sleep\", \"transcript\", \"wasserstein\", \"hpnn\", \"presynaptic\", \"accelerated\", \"loss\", \"learning\", \"results\", \"matrix\", \"synaptic\", \"rule\", \"spike\", \"network\", \"state\", \"actions\", \"action\", \"time\", \"data\", \"methods\", \"non\", \"represent\", \"set\", \"model\", \"neural\", \"problem\", \"models\", \"figure\", \"function\", \"algorithm\", \"since\", \"latent\", \"given\", \"also\", \"parameters\", \"one\", \"optimal\", \"large\", \"error\", \"algorithms\", \"used\", \"optimization\", \"training\", \"first\", \"two\", \"using\", \"based\", \"different\", \"number\", \"distribution\", \"nsr\", \"tachycardia\", \"sesame\", \"ahp\", \"ventricular\", \"atrial\", \"iwal\", \"gazefollow\", \"hops\", \"gaze\", \"arrhythmia\", \"insites\", \"defibrillators\", \"bourke\", \"war\", \"rva\", \"cardia\", \"mmnn\", \"mstd\", \"snq\", \"gpfa\", \"symplectic\", \"psp\", \"cardiologists\", \"asogawa\", \"pickard\", \"ftks\", \"perturbator\", \"svt\", \"hofsttter\", \"etric\", \"gater\", \"arrhythmias\", \"styles\", \"risks\", \"aifnn\", \"style\", \"nq\", \"eeg\", \"saliency\", \"cot\", \"melodic\", \"input\", \"classification\", \"motion\", \"cbmpi\", \"standard\", \"order\", \"algorithms\", \"space\", \"also\", \"neural\", \"information\", \"algorithm\", \"field\", \"implementation\", \"problem\", \"similar\", \"value\", \"first\", \"local\", \"decision\", \"image\", \"rate\", \"single\", \"paper\", \"results\", \"size\", \"examples\", \"different\", \"time\", \"models\", \"data\", \"functions\", \"function\", \"two\", \"model\", \"large\", \"figure\", \"based\", \"new\", \"using\", \"used\", \"network\", \"one\", \"distribution\", \"learning\", \"number\", \"set\", \"training\", \"given\", \"elo\", \"trueskill\", \"abuam\", \"peg\", \"suppb\", \"vlr\", \"wedge\", \"atda\", \"ald\", \"fhi\", \"teams\", \"ytrain\", \"uhi\", \"nhi\", \"nbnb\", \"bvalue\", \"hca\", \"bqu\", \"phi\", \"studios\", \"mxn\", \"gaifman\", \"hxl\", \"nbd\", \"chaperones\", \"bungie\", \"xbox\", \"fnew\", \"wts\", \"psx\", \"zell\", \"ndb\", \"heap\", \"wlrmf\", \"milsd\", \"postsynaptic\", \"mf\", \"qos\", \"ut\", \"tmax\", \"tree\", \"otherwise\", \"xk\", \"set\", \"proof\", \"node\", \"algorithm\", \"edge\", \"learning\", \"performance\", \"search\", \"number\", \"means\", \"probability\", \"define\", \"assumptions\", \"value\", \"optimization\", \"structure\", \"models\", \"xt\", \"clustering\", \"linear\", \"distribution\", \"data\", \"model\", \"prior\", \"inference\", \"rank\", \"samples\", \"given\", \"time\", \"systems\", \"method\", \"since\", \"network\", \"function\", \"algorithms\", \"different\", \"see\", \"one\", \"using\", \"training\", \"methods\", \"also\", \"two\", \"used\", \"space\", \"results\", \"state\", \"neural\", \"figure\", \"problem\", \"matrix\", \"stratum\", \"strata\", \"xki\", \"ulsif\", \"spotter\", \"alertness\", \"fom\", \"ntr\", \"tubes\", \"ocularity\", \"stratication\", \"fictive\", \"kliep\", \"logreg\", \"lmc\", \"retinotopy\", \"gaussain\", \"ioui\", \"straties\", \"odh\", \"nkkn\", \"wcorrect\", \"xte\", \"pte\", \"sirp\", \"biologische\", \"spotters\", \"tsoi\", \"pntr\", \"svar\", \"kmm\", \"drr\", \"raslie\", \"ulse\", \"deepid\", \"buyer\", \"volterra\", \"isb\", \"flops\", \"salinity\", \"bio\", \"verification\", \"uluru\", \"jhk\", \"fouling\", \"splines\", \"training\", \"uv\", \"network\", \"order\", \"edges\", \"using\", \"data\", \"case\", \"error\", \"networks\", \"tree\", \"algorithm\", \"model\", \"fixed\", \"stochastic\", \"space\", \"test\", \"random\", \"used\", \"time\", \"gaussian\", \"set\", \"number\", \"figure\", \"system\", \"shown\", \"distributions\", \"image\", \"problem\", \"let\", \"one\", \"linear\", \"learning\", \"two\", \"based\", \"first\", \"also\", \"models\", \"function\", \"different\", \"given\", \"okm\", \"lpe\", \"fscore\", \"npe\", \"bcpg\", \"pym\", \"alexanders\", \"nedelec\", \"mkd\", \"mathtauacil\", \"sjv\", \"wkr\", \"blumer\", \"razaviyayn\", \"compressibility\", \"pkq\", \"fjv\", \"cgd\", \"hkk\", \"balleine\", \"dre\", \"aaabbb\", \"pmax\", \"massed\", \"remodeling\", \"automorphism\", \"adwords\", \"erx\", \"brede\", \"massarts\", \"equivalences\", \"mbatch\", \"booster\", \"hearer\", \"clue\", \"epu\", \"learnability\", \"compression\", \"eus\", \"category\", \"function\", \"points\", \"sample\", \"error\", \"bound\", \"learning\", \"size\", \"log\", \"weight\", \"consider\", \"probability\", \"feature\", \"functions\", \"model\", \"set\", \"scheme\", \"step\", \"case\", \"xk\", \"time\", \"lemma\", \"rule\", \"two\", \"distribution\", \"optimal\", \"features\", \"using\", \"neural\", \"first\", \"optimization\", \"one\", \"test\", \"problem\", \"also\", \"information\", \"data\", \"figure\", \"algorithm\", \"number\", \"used\", \"given\", \"training\", \"results\", \"models\", \"pert\", \"poedges\", \"qmf\", \"fspyr\", \"hcj\", \"biv\", \"iwf\", \"laussanne\", \"iiab\", \"pgehler\", \"wwwcnsnyuedu\", \"coring\", \"perts\", \"elets\", \"decsaiugres\", \"dirl\", \"daubechies\", \"componential\", \"inwards\", \"magin\", \"kozintsev\", \"graduated\", \"uit\", \"heavysin\", \"iout\", \"gsm\", \"sendur\", \"rbfnn\", \"fingerprint\", \"lior\", \"wavelet\", \"softening\", \"hvc\", \"biorthogonal\", \"ranknet\", \"ctbns\", \"entro\", \"syrinx\", \"lst\", \"routing\", \"online\", \"pcs\", \"syllable\", \"matching\", \"bounds\", \"noise\", \"mixer\", \"models\", \"bound\", \"section\", \"linear\", \"however\", \"xt\", \"note\", \"algorithm\", \"model\", \"second\", \"figure\", \"distribution\", \"data\", \"experiments\", \"distributions\", \"used\", \"well\", \"using\", \"results\", \"order\", \"one\", \"performance\", \"set\", \"probability\", \"based\", \"function\", \"two\", \"learning\", \"state\", \"log\", \"given\", \"also\", \"time\", \"number\", \"training\", \"neural\", \"network\", \"first\", \"recirculation\", \"synunetry\", \"andsmooth\", \"perfonns\", \"itlr\", \"congested\", \"tovisible\", \"zloop\", \"nishida\", \"infonnal\", \"teon\", \"statuclaedu\", \"sfa\", \"schempp\", \"inunediately\", \"smail\", \"noool\", \"emanates\", \"morrone\", \"ofcalifomia\", \"hiddento\", \"petforms\", \"hongjing\", \"disneyland\", \"exploded\", \"lulits\", \"lyk\", \"blht\", \"fpnns\", \"sekuler\", \"crcn\", \"routing\", \"visible\", \"sentence\", \"traffic\", \"ccn\", \"probing\", \"pbayes\", \"topic\", \"gms\", \"hhb\", \"gratings\", \"stimulus\", \"motion\", \"loop\", \"saliency\", \"score\", \"class\", \"eus\", \"vext\", \"sum\", \"load\", \"ii\", \"following\", \"hidden\", \"model\", \"show\", \"number\", \"one\", \"units\", \"feature\", \"neural\", \"function\", \"test\", \"models\", \"points\", \"given\", \"figure\", \"let\", \"data\", \"log\", \"learning\", \"set\", \"network\", \"using\", \"time\", \"also\", \"networks\", \"case\", \"first\", \"state\", \"non\", \"two\", \"training\", \"used\", \"results\", \"algorithm\", \"based\", \"input\", \"problem\", \"distribution\", \"different\", \"algorithms\", \"asap\", \"defender\", \"skill\", \"striker\", \"sps\", \"phmm\", \"mankowitz\", \"defenders\", \"csibra\", \"goalkeeper\", \"bacon\", \"derin\", \"lifelong\", \"skills\", \"hxgoal\", \"teas\", \"gjtgkt\", \"gjt\", \"yagent\", \"talking\", \"eaton\", \"noulas\", \"dribbling\", \"optimas\", \"wilpon\", \"vgw\", \"exclusiveness\", \"interviews\", \"pieraccini\", \"gbnt\", \"intentional\", \"gergely\", \"vkm\", \"lsbp\", \"sprite\", \"infants\", \"mii\", \"hpnn\", \"misspecified\", \"nrsfm\", \"vmt\", \"segmented\", \"disparity\", \"state\", \"model\", \"representation\", \"correlation\", \"policy\", \"behavior\", \"image\", \"parameters\", \"images\", \"left\", \"stimuli\", \"using\", \"parameter\", \"space\", \"learning\", \"figure\", \"training\", \"probability\", \"non\", \"number\", \"large\", \"thus\", \"data\", \"used\", \"set\", \"see\", \"one\", \"models\", \"input\", \"given\", \"methods\", \"matrix\", \"function\", \"algorithm\", \"two\", \"results\", \"problem\", \"neural\", \"method\", \"based\", \"network\", \"first\"], \"Freq\": [13848.0, 13966.0, 9619.0, 12580.0, 9043.0, 9376.0, 7300.0, 6244.0, 4862.0, 6928.0, 10586.0, 6100.0, 8389.0, 4409.0, 6150.0, 5347.0, 4228.0, 7678.0, 5757.0, 4476.0, 3118.0, 4496.0, 5985.0, 4577.0, 6046.0, 4212.0, 4647.0, 3422.0, 3918.0, 3108.0, 15.802572918863039, 28.94077076690663, 7.302678963028377, 26.235982107474634, 10.670167822056806, 4.069874731574387, 4.603029063053737, 7.44150602678542, 3.2548703716250853, 8.553820404641378, 2.5855074859090212, 3.0282449383316226, 29.71619782844681, 1.4774521371787144, 2.454921222204088, 6.763930337773516, 2.3808560744273124, 2.3961404971085662, 15.14173098138602, 15.00391180085471, 24.694641824047636, 2.355292507213236, 2.7689462067252943, 5.923178715764157, 17.460477967957853, 2.3093760206875737, 4.086598368152169, 3.652248743908155, 4.990098487805374, 8.49143062672626, 38.36110775679236, 17.660744003662227, 35.05774772580401, 21.394655853750454, 1688.270728318919, 86.7650154037067, 10.156741991912098, 7.755802308906097, 1195.2671179469032, 806.3718711882802, 99.96483790541018, 1170.3380374270719, 174.2937042887986, 923.8838592390856, 2015.0729468839575, 417.67910853095515, 404.9005075229199, 1234.9172873969949, 415.34265747946336, 1671.716323643452, 836.3262011168475, 1413.7239271871695, 1175.5622104722052, 1026.0409956073152, 2420.8975106481053, 1670.9557713488302, 393.1513895090151, 1456.1713273346616, 725.0563091543297, 1057.6610682317394, 2143.229337793585, 939.8050560398841, 943.0406400782995, 471.39436281331376, 944.4622366095401, 2273.978302028531, 1342.2624372837045, 1813.3823206210411, 1341.2681890201495, 683.3775070273385, 717.6009268513913, 720.8346150578465, 1507.298031154641, 771.7928672722016, 1047.1357791077755, 1034.4102151548188, 983.5981269606058, 870.0722874451939, 987.8801177613805, 1098.3773426006537, 944.4536360060139, 935.028299337331, 4.724939770373027, 4.042574623186965, 3.69515085885292, 2.8331101869208903, 1.5959189779471707, 3.1436067112907415, 2.2874060980854036, 2.63445443694614, 2.4842581286770375, 3.2063864834268427, 2.1200071650252204, 2.53493465644403, 9.181905209384588, 2.086647910472868, 1.0509807928189496, 1.3896631647291493, 1.0336644131471788, 1.3665116064494354, 4.533852191034515, 5.33022516027633, 1.0114639968849775, 4.34101986614227, 0.6719525546600477, 1.3291079019336152, 2.404604418057685, 1.6972973276177683, 4.766209903619091, 0.9800937000259881, 0.9678383471668693, 0.9535740350009249, 14.369483536361862, 5.666049363483379, 6.694098753238837, 39.602831740169506, 94.5728940307945, 14.92387602365563, 11.089610437732583, 227.8605096729395, 10.472429105405887, 306.57799191767583, 1378.1922165886106, 32.81172777353137, 573.2169028405056, 416.52096703814755, 1355.6857665675338, 379.5312439580496, 93.54091965088381, 2168.257943300756, 1277.3374643438535, 1029.1423515979773, 889.234648946067, 765.8531761791008, 497.45949442648055, 363.9513227110346, 804.6999727743914, 377.2511996909316, 846.398575497731, 458.6910700638664, 590.2526754092943, 870.573048457468, 1709.2545456369778, 457.57362349710667, 1203.6440574774133, 890.6468271916165, 898.1754883864655, 891.3187098437305, 611.0120264221896, 1375.2593284560323, 669.5031205088883, 1225.352638051144, 1178.9982953663505, 1632.351783525294, 1179.4949226440915, 845.50259119421, 673.5225919625786, 691.4410111266781, 658.825422886401, 882.4468464141293, 719.2642446739118, 737.153876911056, 682.4052944795908, 680.9571501712684, 1.9812011762666575, 1.0956668076387972, 1.0802360093068248, 4.877927285098342, 1.3729147714470658, 5.1491750888451735, 5.158267809245751, 5.255261062591219, 4.296327332229775, 3.3236361558854393, 12.643617854633513, 2.20010277225812, 9.15314745767361, 0.9083363534765969, 10.569358426846916, 2.663130027192861, 4.187690031501766, 0.5896225607944638, 3.455663679759838, 0.575414655701148, 0.5670027127363598, 1.4482709270856964, 1.1419017848350839, 23.351190337946214, 0.5547271283875921, 1.3841703074066927, 0.8318683162672436, 0.8381240914305208, 0.5523429366195138, 3.7799603533509667, 6.968486779052308, 1.6758422578431589, 1.6471390056800959, 122.02512103768863, 11.160827652324183, 15.201222202264432, 21.23138918229822, 5.167317230977753, 7.699441724065876, 5.8514794931182905, 29.574363015700193, 10.915022993379399, 22.646382141099025, 22.12289559038483, 525.6963792145652, 1984.395059255363, 924.0394312501588, 859.2128152649688, 133.5915734410419, 211.62240079054777, 203.24199243433722, 820.7679349681621, 708.0174188598746, 142.51265038918737, 245.6942661654274, 1021.2432704940644, 1552.366094881255, 645.7723372669059, 540.2062976982006, 165.83405124315456, 1168.587178792555, 1569.7518280993518, 696.5483169238826, 894.3938645152639, 772.3660777856317, 873.4793575802012, 1034.6465503109762, 1170.4544282271638, 440.6246612406724, 238.9050650538097, 729.2616282131725, 728.9652394140281, 507.28527328300646, 927.9665225971945, 474.68274634364064, 490.7355432275368, 566.1313416210734, 574.3240822523786, 698.7130997485267, 410.60083733711343, 659.4014026974747, 597.578494418409, 747.2598982828338, 812.5810980161893, 606.299795074855, 548.0978904419201, 611.2922595749122, 572.6452083817786, 4.305275243725242, 1.8203829049244014, 5.123935017797026, 5.666319104107593, 3.7088211251384067, 1.4109590103966863, 2.634103722214609, 2.7712410820306204, 0.8230676620562953, 47.72949913335841, 3.546820763435762, 1.571722794610625, 2.957290619030039, 1.338346198328425, 16.762811397869914, 1.05604056912722, 1.0615597478408776, 1.5794593509984318, 2.2827424067849447, 4.0255279531853745, 12.808440441575584, 5.389585483341683, 5.759169545838471, 1.2569698743840694, 3.0570193321421546, 1.9758698257646157, 1.2288233978281748, 0.7434142997295169, 2.46972047987688, 1.8971866863663787, 10.568573369863783, 6.241483863005967, 2.4477136767309706, 19.7239817773567, 11.88000570972404, 5.099118849967691, 57.36858343945273, 23.153096337222454, 38.54783740860641, 41.40841637807056, 12.108333277877438, 5.417299480184896, 675.6697879142254, 471.5714007386941, 195.3925196526655, 13.639324583356597, 352.4787588416815, 503.35654145439753, 668.6453200637314, 600.7497184120758, 833.7080933501492, 735.3620103000525, 682.500048730445, 1297.7295146302463, 203.06298798184076, 117.14175317640952, 920.8639589765136, 354.31987396201373, 520.8751975216828, 683.0005966547509, 376.3918827857654, 218.82168986132498, 600.8427964628002, 357.7614160634218, 369.2875427311963, 380.54517947729533, 730.0095300601739, 387.17853996156896, 308.43833179387974, 592.3697399075053, 880.4705176181219, 738.7497733285934, 1303.2299814411272, 415.8736863225172, 973.5033904460072, 856.0858638601321, 1313.4596472822757, 485.69986112147683, 773.7728370496742, 636.1978598022704, 439.5857975876722, 896.9204268329087, 657.2296765208832, 623.4552024323252, 789.9958721358443, 619.2933335245494, 1087.191249999965, 692.4871440199861, 832.9785861635452, 638.8505552877234, 607.5118648613684, 8.680318655084884, 10.264855773612554, 2.076113176867081, 4.39313653329084, 1.1334954739917946, 1.9239845321677276, 2.7568973377524135, 4.405825671535525, 8.774906204691828, 1.081222696505688, 7.153249767150325, 2.386903197242189, 1.8477476924006049, 4.2805025902823, 19.941408723302427, 1.050324934194803, 19.915338662684466, 1.6103140066896502, 4.275948559076429, 1.0157629447988377, 0.7512015916934058, 17.66350503070547, 1.2749372540255894, 7.168971354409742, 1.2456178653538217, 0.9973117248114597, 2.2141967607128294, 0.9740224092246407, 1.7689533087082658, 1.7203376861646613, 7.98337829163258, 11.210833279981019, 5.445615792130208, 5.378447582222266, 5.969857005503081, 43.701650997975584, 19.44536264223869, 14.220930895247486, 82.36378063189166, 8.215067683110965, 301.2749883040766, 131.94328220349718, 180.0746558570885, 1326.1107899602837, 239.42765616607974, 274.9611727907837, 1344.3159833944105, 161.514720322305, 1672.3798980363526, 593.9668197625738, 243.40676418455618, 894.8573578237877, 274.7425175723849, 580.8796561901927, 261.9917993962729, 125.77155967469142, 504.46806506579884, 415.39076549077515, 364.5803236428155, 756.3188269034947, 286.5994129362642, 259.39687684014797, 551.3484461328303, 676.7491752254211, 1282.7907774927276, 1350.1481059227424, 301.9500461725468, 343.79235973658706, 223.64223617636324, 295.9819739186617, 651.55447612726, 770.7594528364074, 307.24045458057515, 533.1827538296147, 396.8005183242854, 602.2199227143162, 829.9803712389138, 516.9935445016935, 518.5503828237366, 407.1109718443267, 729.79831623644, 783.5590262306812, 583.315120361604, 485.1435631971537, 571.9640314927763, 643.6783346623118, 561.4593092164057, 449.6756897712489, 527.2160531427752, 478.28995239599095, 496.693748738311, 534.5256825552366, 527.6420070044358, 489.36140617557555, 15.664862488868517, 18.135695810067162, 2.099587547096094, 11.539583979926567, 2.8727973253721997, 6.64000336073836, 8.018601713554837, 12.191631290653648, 0.9248855862096231, 3.623763783651347, 2.9690140531886797, 3.0122833917794636, 4.826000882783629, 6.850155216233294, 6.747394940217075, 3.472560328312604, 0.6931310560722878, 0.6831718684657999, 1.0845120875376175, 2.1536177097234135, 0.6527624332614209, 0.9105361392275568, 2.9256916114394054, 2.205707305928958, 2.191464153769137, 0.6443843036937266, 0.8441068316190027, 3.9972875062426176, 1.299039466281649, 1.9194163188007272, 9.207841991657071, 7.39001891845947, 3.370476586535554, 7.9150753359721255, 13.65693435652724, 27.8799059000375, 12.52680143403561, 8.247461457764953, 6.227818060618536, 9.048311607045946, 14.936655336276328, 23.394473273706932, 6.495014037006188, 6.640256839814889, 10.70503669039378, 15.500255442305688, 707.5458266656975, 25.92593340889113, 646.182161749437, 410.3832479766569, 115.23088581206737, 964.6382969930315, 1224.9247283000361, 476.5573849960741, 503.08184339627076, 410.4732429614141, 222.89936073607774, 981.8430179662103, 1205.9049725235018, 239.1068412109928, 220.05008598579053, 439.3885953121439, 338.939170501643, 407.0595510244989, 595.2558305890296, 709.1603439425928, 359.2371858590049, 848.8282553897076, 643.9029232079882, 667.7222755608205, 276.7625146379863, 324.2280718931187, 242.11730033226758, 441.00238755391524, 613.183320951371, 362.6583547206356, 652.5750683252554, 404.1305450679275, 895.686278525686, 553.7878998230907, 451.59206931518304, 434.38607953053753, 473.931171833132, 476.6438001199069, 526.1384134840387, 398.57523531902353, 428.6890254149062, 6.613655628844641, 2.039195637481716, 2.7308030416379436, 2.1131002542973367, 6.108331440547785, 3.935165283894315, 3.948319732443178, 2.120427340446577, 3.3166024605788484, 1.0602730794880082, 1.056017562498883, 6.100372463560097, 1.5659353327734056, 1.3143483683428603, 1.9937611946095923, 6.9212549362425175, 1.0241941974456796, 1.8110284662483276, 2.0473581823065743, 1.508549542524976, 3.523845705727775, 1.247020806303056, 2.7688995504494374, 8.195622140668549, 0.7318695967774936, 1.4619557291230842, 4.78155831519563, 0.7198989265590253, 2.5858937598186618, 1.2192859173534387, 4.187636183604501, 5.440910302327838, 6.295511473581571, 15.578444180634422, 9.039675752079242, 9.39430414532352, 15.344538803273144, 58.44100726896767, 19.377649879104524, 108.25884705563918, 1038.5191559733187, 356.2869575364191, 352.81911891920157, 526.518783330379, 339.82742158491743, 1321.7316534530435, 326.71683114180286, 555.573178321709, 224.76531774917655, 303.37414354281475, 464.7031752425878, 344.65281122508014, 341.3304647954205, 1175.8312372937692, 846.4958505174437, 111.96166228146771, 314.9509477945481, 419.37365879338006, 136.94214584624092, 653.5321179273781, 165.33783396216913, 148.07517289093545, 643.8640253804748, 508.6761386766982, 356.6887207826458, 322.1997608391565, 717.5859104847254, 463.78539854737863, 457.8961542372339, 302.3593214323139, 635.5142144172862, 304.83518364041714, 567.5429373156206, 497.74934127543213, 418.39049088831416, 791.765967974429, 542.5555978708492, 658.8897277961393, 504.97114302068036, 450.3031803558777, 433.06702073216894, 434.08260219646706, 413.66826488546695, 417.342278906556, 5.272933591262289, 6.378105552442684, 1.105366157389585, 1.048684721744265, 1.0048376701871318, 0.989145231752185, 0.7267105767771473, 0.4660092981461204, 0.69218798800409, 0.9233805353583873, 0.46249232805348867, 1.15010602768682, 1.8034076011913318, 0.6860225086151641, 0.44789929208375484, 2.967210634529582, 3.5327907439419, 4.349183265613808, 0.43556733229451555, 0.4342857040479561, 0.4340771600088778, 1.3355798407332566, 2.2118634576471305, 1.309675167539878, 1.0903282969844972, 19.089650194532574, 0.4272137254299647, 0.8633030017454854, 1.0674017995043874, 1.0592337134170007, 47.47692906087423, 1.9133341672236555, 5.389159355121396, 2.2930318006905592, 5.174715354924745, 9.195307289793867, 1.4890493176961217, 2.446574268067102, 6.486515085717136, 31.72442393660465, 204.89822213654313, 7.378586403983008, 14.245031357808966, 123.4598969791121, 204.87037533442262, 339.3668633065678, 10.703489772316878, 661.1158407721641, 325.78787422836916, 389.0152129630083, 488.50081780622526, 391.229332700323, 238.22296431314697, 307.3750518774776, 993.8393838958557, 1254.4465003886876, 296.85929645217317, 714.950841810911, 574.1485266472336, 1126.4851486273767, 288.9687427740145, 239.09768276533876, 569.088139138577, 296.2555718028259, 793.7150090909346, 538.3480030259664, 335.64288739569, 694.9296744433738, 402.65356868989943, 761.9685442182573, 407.30441174221613, 487.5777304837858, 697.7736785942977, 616.5177777521385, 954.9372304942307, 429.6663761708249, 438.6337725205546, 466.55745231562736, 468.1479659827557, 523.95197510641, 477.8304528354051, 427.79068904087654, 392.2705553826786, 399.20457994996167, 388.70031346602286, 7.0880015444076445, 1.1686964548248917, 0.5964571656634255, 0.3785650370140556, 0.37361616148036014, 1.3054013211334385, 0.373127917944473, 4.788597319438653, 0.7281704671545052, 0.5437641173718293, 0.3617387771059946, 1.0632924238953942, 10.914269815944017, 0.3508050232500381, 0.3512021755405048, 0.3509231891263665, 0.3502757176780169, 0.3467284909393048, 0.7007994001981855, 0.34489278249254035, 0.3422460122764033, 0.34302072064110406, 1.1941216798294363, 1.5320928433170493, 0.6670871223092412, 0.3320910868376771, 0.6619397571860378, 4.702077047254769, 2.5332118741836687, 0.48627304286478024, 4.412775960430903, 30.156710502042706, 45.46541975959034, 50.64561456926185, 14.641438567855943, 8.547452945264952, 17.392811585883553, 2.41983162929086, 109.66482002171989, 2.2098036768738316, 4.815622094248757, 6.010470011236665, 107.2826026024194, 135.86975229570834, 51.972003922692465, 26.002430099539705, 124.33496573070956, 250.41873269728484, 13.297631709668142, 3.6282298797138393, 135.28727353347557, 17.175667229457137, 130.68304295591963, 290.078217087009, 185.18680156984738, 999.5646130166482, 288.7916018205184, 538.6353541126849, 625.0614891205489, 140.8998824667213, 264.0291549172938, 420.7252307028928, 650.2948966663677, 262.22105816240224, 470.3051124615148, 237.7385853780098, 449.93124303070164, 524.0239809876504, 298.1277300393145, 794.5126603433426, 394.9652364042549, 853.4018145027267, 630.3621008093661, 414.056691775755, 601.2935051821516, 509.0180489201716, 425.96138842842817, 293.62859168434284, 310.46619690142296, 363.8495715581767, 342.3283648557864, 285.8479559914046, 451.58960308926703, 384.03553889581394, 369.2170480295844, 357.9878416752661, 460.508287308799, 342.87136645183966, 309.6066689114311, 360.00144304960935, 321.6164489233822, 305.9043949710384, 299.80814200390915, 16.551036540414678, 1.975004708199576, 31.254994326312485, 1.8999200320731304, 8.759177574555887, 4.297365707293494, 1.2170486585619862, 1.597377463620709, 1.1657004706053797, 1.164032569257789, 1.1760560761222334, 0.7737220252756946, 1.3470723532199436, 19.120097558688805, 0.5683548237727267, 1.1181565475535875, 0.5403366361655512, 1.7865628728261338, 0.550536622386739, 8.080890879782803, 0.7197297353573342, 0.898024408317854, 0.5262278069714421, 0.3484211154792902, 0.8651233162204415, 0.8420072255557747, 0.3434366633081473, 1.0426413187671972, 3.205733009940935, 0.4990732797165013, 5.917429719047993, 3.3232675876539455, 9.424572645660524, 6.5626378955240225, 3.44434332135951, 4.530040024959868, 2.282148711648699, 6.86907112462616, 3.072699487179634, 4.7326015632697285, 2.52641431977758, 13.448065505356949, 22.717638242727755, 406.08833599118543, 925.9919877457436, 180.04520714397987, 98.72861745955913, 164.0400624148247, 82.87064259352161, 335.585985596269, 281.7813529400938, 228.1710605600108, 123.56719399446489, 57.21772234652605, 565.571943387111, 208.2577079320208, 289.0280426340728, 754.9008688782377, 445.1567856051305, 380.16654362966773, 289.31082173023367, 257.2377093742825, 413.3340521584213, 255.84772614410528, 206.4549949968337, 652.2635871791829, 369.95293991794784, 517.4094458160155, 231.74933416106563, 458.1762759267399, 361.39779985852664, 280.159122736265, 347.33837001630735, 283.8484240200997, 327.2522620254825, 442.19175779545424, 490.69529395374343, 382.76360875316146, 319.0086809465653, 353.8784314083465, 296.5693154359727, 264.4827562418824, 272.0847527743835, 267.9431874729556, 264.3627082253734], \"Total\": [13848.0, 13966.0, 9619.0, 12580.0, 9043.0, 9376.0, 7300.0, 6244.0, 4862.0, 6928.0, 10586.0, 6100.0, 8389.0, 4409.0, 6150.0, 5347.0, 4228.0, 7678.0, 5757.0, 4476.0, 3118.0, 4496.0, 5985.0, 4577.0, 6046.0, 4212.0, 4647.0, 3422.0, 3918.0, 3108.0, 26.026199491863085, 48.21868983356876, 12.72970455291457, 47.90174097082281, 20.46098902118051, 7.907827970689499, 9.013443284563621, 14.689312043239266, 6.692536604327863, 17.82414305987527, 5.507076107232501, 6.61297765235147, 65.90953216618274, 3.281441376136201, 5.4943849963108695, 15.258629152654098, 5.430743121924997, 5.469455437125338, 34.8058640975816, 34.51081695576093, 57.290673063598675, 5.4768887070281735, 6.5115471797879, 13.983934463717524, 41.37214603204001, 5.482349903859325, 9.708826333257882, 8.68134486737167, 11.865788970936007, 20.477857321444954, 94.33974065412718, 43.864974215855234, 89.43522920385863, 55.720444242927755, 5671.35756439725, 255.96089779091616, 25.557850365665047, 19.278521008881718, 4440.157143227741, 3070.3557112163753, 311.9098789611051, 4854.069801491549, 597.5215533299927, 3877.1311375619985, 9376.290801152834, 1627.5791259619107, 1580.131189571697, 5745.540934248234, 1679.262571821544, 8389.377814323547, 3759.6649948439767, 6928.1884908520915, 5612.9506526453915, 4917.29208889893, 13848.348186446117, 9043.002281224553, 1618.2550212826357, 7707.404105505607, 3365.695626032803, 5320.616481708518, 12580.822829670038, 4616.101862999405, 4647.8179064850865, 2013.6390651135985, 4713.380893017013, 13966.86029847489, 7300.896642718308, 10586.917286249642, 7322.270926693521, 3173.7262596401897, 3435.083124760596, 3517.682167202418, 9619.533705465908, 3918.268665866604, 6148.913449913634, 6046.496784252798, 5757.054049313957, 4758.764433061485, 6150.418051121945, 7678.955858015884, 6244.403484412527, 5985.63494744816, 9.795152397849812, 8.819941130001348, 8.76745768746003, 6.81738385138958, 3.903513446532998, 7.836131858045595, 5.815846530122351, 7.041672027493847, 6.750955206189562, 8.725628930382602, 5.792760961429021, 6.972525890482369, 25.423143330291392, 5.803239570333983, 2.92624448905143, 3.8722541349832538, 2.9146757468043765, 3.8747751609076664, 12.979811355725378, 15.370237603873266, 2.921083166749275, 12.624493452183108, 1.9560115401021327, 3.881427697594047, 7.103558680665096, 5.020521996740683, 14.099293811196187, 2.907940685056522, 2.9073167530890793, 2.904665787721266, 45.43354567655369, 17.75006498426912, 21.098571772330395, 135.33478966921209, 338.20543946358896, 49.60863966586981, 36.640025652697794, 933.925721162793, 34.53868218615231, 1364.9860281273761, 7322.270926693521, 123.43624035083548, 2937.757405739877, 2071.903948115614, 7707.404105505607, 1877.782377483714, 402.5417230953298, 13966.86029847489, 7678.955858015884, 6100.657026352582, 5320.616481708518, 4496.171014517669, 2779.574480587309, 1970.9798840625754, 4917.29208889893, 2069.0186353502663, 5286.088936941448, 2631.6981116971456, 3531.1341010258616, 5612.9506526453915, 12580.822829670038, 2669.1764553190883, 8389.377814323547, 5985.63494744816, 6046.496784252798, 6150.418051121945, 3881.762032555839, 10586.917286249642, 4409.860309827102, 9376.290801152834, 9043.002281224553, 13848.348186446117, 9619.533705465908, 6148.913449913634, 4577.146381424672, 4758.764433061485, 4476.283125132471, 7300.896642718308, 5745.540934248234, 6928.1884908520915, 5671.35756439725, 5757.054049313957, 4.8985902508743395, 2.9117098012254297, 2.9052627044225146, 13.807772739946866, 3.9032845281916813, 14.862505039287473, 15.007919553798207, 15.639805320605376, 12.820128966364138, 10.008748134269348, 39.62775300840124, 6.899684099495731, 29.3136780728749, 2.9156951998354055, 34.16598670462183, 8.62677596970725, 13.610566369546573, 1.9387780943845991, 11.631949009998024, 1.942911782811093, 1.9370164348312369, 4.9700074333729924, 3.945278299214297, 80.7711681916335, 1.9421795870818486, 4.8598470060410515, 2.9225432364614385, 2.9475267570051242, 1.9453062774128287, 13.328979362413524, 24.749175351515724, 5.971995123708389, 5.871768643866892, 478.5148897198632, 41.456825946538075, 57.042865685345674, 81.0257420551804, 18.913295370415263, 28.8273413899052, 21.768749012728243, 122.16284994685151, 42.498489547090955, 93.61136302557962, 93.22485559463502, 3039.7994140899445, 13966.86029847489, 5985.63494744816, 5671.35756439725, 710.3365280005836, 1198.72248054579, 1167.9894647416218, 5757.054049313957, 4862.29737860151, 783.8926614436921, 1478.3291621928809, 7678.955858015884, 12580.822829670038, 4616.101862999405, 3808.192676042234, 961.8293787408431, 9619.533705465908, 13848.348186446117, 5347.990485374531, 7322.270926693521, 6244.403484412527, 7300.896642718308, 9043.002281224553, 10586.917286249642, 3257.6420356515823, 1545.0009014143463, 6046.496784252798, 6148.913449913634, 3918.268665866604, 8389.377814323547, 3649.5597411300782, 3810.283770666114, 4577.146381424672, 4713.380893017013, 6150.418051121945, 3067.536619397426, 6100.657026352582, 5286.088936941448, 7707.404105505607, 9376.290801152834, 5612.9506526453915, 4758.764433061485, 6928.1884908520915, 5745.540934248234, 13.43112732111493, 5.755549181173829, 16.78449745588614, 18.910454122407522, 12.5254560927112, 4.767486619502516, 8.941310499883368, 9.558331553423924, 2.8402722239300324, 165.7256935980873, 12.57475108391712, 5.611028853518721, 10.566940983310468, 4.794221816914399, 60.50419022495407, 3.826954217252821, 3.8477820587884373, 5.753497930263201, 8.4760362117225, 14.948474620241823, 48.0919322149646, 20.252264057522257, 21.898030059896186, 4.794860079935269, 11.67128783984938, 7.607777859515769, 4.752477143757672, 2.875919374441014, 9.62282012135378, 7.405935830115549, 41.51125595214428, 24.52308261765301, 9.559278828393905, 83.15282669686054, 49.6396566215115, 20.54620228429646, 265.9900040079382, 104.84337457404101, 181.55022465043476, 197.02032512403227, 52.88580010756179, 22.16435899665772, 4496.171014517669, 3017.8282980573863, 1179.9392119384047, 61.32004184055724, 2279.230617034392, 3422.0239462581094, 4713.380893017013, 4228.279960931838, 6148.913449913634, 5347.990485374531, 4917.29208889893, 10586.917286249642, 1259.5766233798356, 681.0884253984609, 7322.270926693521, 2446.8220287917165, 3836.2799518353904, 5286.088936941448, 2669.1764553190883, 1421.7026364332085, 4647.8179064850865, 2544.5649713769594, 2650.0183159369785, 2751.6457965520576, 5985.63494744816, 2825.741093532933, 2163.350070652955, 4758.764433061485, 7678.955858015884, 6244.403484412527, 12580.822829670038, 3118.010520705225, 9043.002281224553, 7707.404105505607, 13848.348186446117, 3810.283770666114, 7300.896642718308, 5612.9506526453915, 3371.534582923211, 9376.290801152834, 6150.418051121945, 5757.054049313957, 8389.377814323547, 5745.540934248234, 13966.86029847489, 6928.1884908520915, 9619.533705465908, 6100.657026352582, 6046.496784252798, 24.908873263737263, 30.054500801204775, 6.709927269807405, 14.516538074620017, 3.864997024383268, 6.73820116837437, 9.68911252740333, 15.563886529602991, 31.216258985875562, 3.8469181367042995, 25.536011570879808, 8.600720987164504, 6.70750235353451, 15.554918342342004, 72.95432888906244, 3.851185165752431, 73.48085377110098, 5.955570219029222, 16.234061768997872, 3.8662828050607856, 2.871300005659634, 67.91959812426877, 4.911810526110511, 27.650076687671834, 4.814237053159471, 3.8584893940535263, 8.697654977542946, 3.827113377275464, 6.9654707154728985, 6.784960217670281, 32.454492385592545, 46.32200164387843, 22.273059601612605, 22.16991969331856, 24.772327783717852, 204.69387037044467, 88.61894921448857, 64.12681587754508, 432.8650428683065, 35.84925139349629, 1847.5054380115148, 748.5339264732517, 1060.5870807970562, 9619.533705465908, 1457.4523467050844, 1761.5448669110528, 10586.917286249642, 980.6851526612654, 13966.86029847489, 4318.142464143531, 1572.4927845533928, 6928.1884908520915, 1814.2757817993424, 4409.860309827102, 1773.417031302904, 761.7475091898515, 3836.2799518353904, 3067.536619397426, 2654.5838525012523, 6244.403484412527, 2029.964555551947, 1811.011593881059, 4476.283125132471, 5745.540934248234, 12580.822829670038, 13848.348186446117, 2208.6761021331786, 2603.1884565198957, 1518.4329835376009, 2173.62068165205, 6046.496784252798, 7678.955858015884, 2307.1319950867905, 4854.069801491549, 3257.6420356515823, 5757.054049313957, 9043.002281224553, 4713.380893017013, 4758.764433061485, 3408.83187284041, 8389.377814323547, 9376.290801152834, 6100.657026352582, 4616.101862999405, 6148.913449913634, 7707.404105505607, 6150.418051121945, 4228.279960931838, 5985.63494744816, 4862.29737860151, 5347.990485374531, 7300.896642718308, 7322.270926693521, 5671.35756439725, 50.08935408943655, 61.427428984069444, 7.422334815242845, 43.10708089536654, 11.258443110895511, 26.033941643758627, 31.705025159545357, 49.2344490163312, 3.7708599591005356, 14.78219348577033, 12.133415977672565, 12.327795265565648, 20.158504851625494, 29.08896344828284, 28.864926566065, 14.906552082920431, 2.9762524900673513, 2.9387868475762757, 4.678997598602718, 9.322265512751596, 2.8280433641103007, 3.9448929777068913, 12.686127925005147, 9.67740234256332, 9.680849196782507, 2.854674870928019, 3.762200504043636, 17.84659097822474, 5.821707049761027, 8.65883020508758, 41.62010562478084, 33.35163385294508, 15.253653947293333, 36.51696047289271, 64.26605905763596, 136.88203398660394, 59.55462595890209, 38.74956968110174, 29.072222929029685, 43.12344922842351, 74.02600993805098, 120.38996018485771, 30.634555637493463, 31.57947085368908, 53.238063369551476, 81.75978209348628, 6100.657026352582, 146.86711710492585, 5757.054049313957, 3422.0239462581094, 814.2905275897548, 9376.290801152834, 12580.822829670038, 4212.391412362317, 4577.146381424672, 3735.8440650653165, 1847.5054380115148, 10586.917286249642, 13848.348186446117, 2026.8864251592574, 1836.4043724590301, 4228.279960931838, 3112.938533757555, 3877.1311375619985, 6150.418051121945, 7678.955858015884, 3389.9058549223955, 9619.533705465908, 6928.1884908520915, 7300.896642718308, 2491.651371972303, 3063.024401410201, 2131.159364485692, 4647.8179064850865, 7322.270926693521, 3734.955711287695, 8389.377814323547, 4476.283125132471, 13966.86029847489, 7707.404105505607, 5612.9506526453915, 5286.088936941448, 6148.913449913634, 6244.403484412527, 9043.002281224553, 4758.764433061485, 6046.496784252798, 7.4864863353021995, 3.7684452099088834, 6.615577928030612, 5.775251890610252, 19.15742711816956, 12.670122224293928, 12.719176478260698, 6.831672152180668, 11.69943543825772, 3.758551414963947, 3.7645716312773287, 22.0538877619206, 5.663639732542292, 4.784517189149389, 7.321294803146525, 25.422935166219585, 3.762857188359526, 6.6881375306651405, 7.694653255701049, 5.749663663650468, 13.510799373998612, 4.786620214171515, 10.632412283342946, 31.593791709649736, 2.827563809873682, 5.658261267585017, 18.79131434551031, 2.836320696094758, 10.2590595705371, 4.841908187231177, 16.639078732371953, 21.817513726284485, 25.483935659970403, 66.20127074918167, 39.3881701343856, 42.99265793945813, 74.41622340504935, 332.83867836695583, 96.4517135045525, 684.4131492453749, 9043.002281224553, 2795.423123084745, 2796.896815853972, 4577.146381424672, 2811.0622716599264, 13966.86029847489, 2825.741093532933, 5320.616481708518, 1821.8400292486285, 2631.6981116971456, 4409.860309827102, 3108.266094434272, 3118.010520705225, 13848.348186446117, 9619.533705465908, 816.2921296938243, 2901.1863529462144, 4212.391412362317, 1060.5870807970562, 7678.955858015884, 1369.0062840373625, 1198.72248054579, 7707.404105505607, 5745.540934248234, 3649.5597411300782, 3270.8513006411745, 9376.290801152834, 5347.990485374531, 5286.088936941448, 3067.536619397426, 8389.377814323547, 3112.938533757555, 7322.270926693521, 6148.913449913634, 4917.29208889893, 12580.822829670038, 7300.896642718308, 10586.917286249642, 6928.1884908520915, 6150.418051121945, 6046.496784252798, 6100.657026352582, 5985.63494744816, 6244.403484412527, 14.793781335037734, 20.48542476523793, 3.7748961692388416, 3.739598972356604, 3.7437556132144882, 3.761579700484444, 2.8321762258341963, 1.8863715452367047, 2.815897923886046, 3.767321091336391, 1.890671702621098, 4.756583560315151, 7.508431746780299, 2.881519348136323, 1.8928668301845561, 12.55821114515464, 15.152042326588145, 18.714435407790425, 1.8902423270023412, 1.889404517939679, 1.8958785089628523, 5.83329754807581, 9.701649795296587, 5.749586290153602, 4.8065460277080785, 84.36696533061439, 1.8929345908564736, 3.829607944168232, 4.766895362434112, 4.737184056645789, 216.55277314816001, 8.648400872150393, 25.012232252991662, 10.486476938790254, 24.38047142146339, 44.38009165059301, 6.731776705478526, 11.449365188522288, 32.678093166840014, 186.8190615554082, 1472.413892101723, 38.42841024960498, 80.29685985944116, 868.7030525819015, 1556.4642301953527, 2775.8662774553645, 60.06849297264124, 6244.403484412527, 2811.0622716599264, 3445.79045655467, 4476.283125132471, 3531.1341010258616, 2029.964555551947, 2738.2331981721654, 10586.917286249642, 13848.348186446117, 2642.3488058388157, 7300.896642718308, 5745.540934248234, 12580.822829670038, 2650.736097543634, 2131.159364485692, 6150.418051121945, 2868.7639183112783, 9376.290801152834, 5985.63494744816, 3422.0239462581094, 8389.377814323547, 4318.142464143531, 9619.533705465908, 4409.860309827102, 5612.9506526453915, 9043.002281224553, 7707.404105505607, 13966.86029847489, 4862.29737860151, 5320.616481708518, 6046.496784252798, 6148.913449913634, 7678.955858015884, 6928.1884908520915, 6100.657026352582, 5347.990485374531, 5757.054049313957, 5286.088936941448, 25.076814852990076, 4.481369963725022, 2.7519191624976886, 1.8457603187179925, 1.846801800241254, 6.45984513610887, 1.8488516502566261, 23.99941755217099, 3.6571846313843244, 2.750394533695107, 1.848195917001941, 5.472803589020053, 56.70499660338678, 1.8459551261075164, 1.8482890647409496, 1.85325555973897, 1.8552549702992158, 1.8508031591676661, 3.7426201409545037, 1.8636250417089082, 1.8524595525925018, 1.8628930394828047, 6.569084001973341, 8.43979910661932, 3.720077294335886, 1.85390828839573, 3.7181848460338833, 26.42289804478333, 14.360486355985918, 2.7794801342576334, 25.371723020503904, 186.8190615554082, 294.5314826150217, 331.7138185606375, 93.31559182437188, 53.338283300588394, 114.46584621020496, 14.46280384193347, 875.3753244511038, 13.179479901825456, 30.528663627162096, 39.07471840169812, 889.4506368024007, 1179.9392119384047, 412.12090828706556, 197.02032512403227, 1130.6589914850776, 2565.9263717122412, 96.4517135045525, 22.755668355821165, 1346.1282275192566, 131.48946080744108, 1319.721244831961, 3276.6639285403894, 1994.1284002949637, 13848.348186446117, 3365.695626032803, 6928.1884908520915, 8389.377814323547, 1488.870376366591, 3108.266094434272, 5347.990485374531, 9043.002281224553, 3112.938533757555, 6244.403484412527, 2795.423123084745, 6046.496784252798, 7300.896642718308, 3734.955711287695, 12580.822829670038, 5320.616481708518, 13966.86029847489, 9619.533705465908, 5757.054049313957, 9376.290801152834, 7678.955858015884, 6148.913449913634, 3735.8440650653165, 4212.391412362317, 5286.088936941448, 4862.29737860151, 3808.192676042234, 7707.404105505607, 6100.657026352582, 6150.418051121945, 5985.63494744816, 10586.917286249642, 5612.9506526453915, 4496.171014517669, 7322.270926693521, 5745.540934248234, 4758.764433061485, 4713.380893017013, 67.15860034719715, 8.253309271493235, 136.21758612298828, 8.379413736341306, 38.937052549210364, 19.20768660455725, 5.537203091199424, 7.3862799410331865, 5.47251280000686, 5.567056511810131, 5.631570690409516, 3.7061314865206842, 6.583310082102964, 94.67172242038927, 2.820486978168816, 5.589734042800356, 2.729730076689849, 9.121608441626199, 2.8144721189471777, 41.60623998386279, 3.7314445234794995, 4.752199419380652, 2.8132802039129507, 1.8803167532282992, 4.676731105652372, 4.57770086810031, 1.870089791343757, 5.700347530204887, 17.56320784995977, 2.741024388768181, 32.906349921845674, 18.399682181680408, 53.92572399136683, 38.9490470826399, 19.962409205212456, 26.8249343995799, 13.059258082130558, 42.498489547090955, 18.01508834863793, 29.146819077662965, 14.786326671566579, 93.43867784314813, 175.5714116569211, 4862.29737860151, 13848.348186446117, 2087.6802656483087, 1030.1931356912887, 1875.0689682425632, 865.9947479644366, 4647.8179064850865, 3918.268665866604, 3070.3557112163753, 1469.837607083825, 584.6277662263932, 9376.290801152834, 2796.371034348694, 4228.279960931838, 13966.86029847489, 7300.896642718308, 6100.657026352582, 4409.860309827102, 3808.192676042234, 6928.1884908520915, 3810.283770666114, 2927.443920660612, 12580.822829670038, 6150.418051121945, 9619.533705465908, 3408.83187284041, 8389.377814323547, 6244.403484412527, 4496.171014517669, 6046.496784252798, 4616.101862999405, 5671.35756439725, 9043.002281224553, 10586.917286249642, 7707.404105505607, 5985.63494744816, 7322.270926693521, 5347.990485374531, 4854.069801491549, 5612.9506526453915, 5757.054049313957, 5286.088936941448], \"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\"], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -10.132, -9.5269, -10.9039, -9.6251, -10.5247, -11.4886, -11.3655, -10.8851, -11.712, -10.7458, -11.9423, -11.7842, -9.5005, -12.5019, -11.9941, -10.9806, -12.0247, -12.0183, -10.1747, -10.1839, -9.6856, -12.0355, -11.8737, -11.1133, -10.0322, -12.0552, -11.4845, -11.5968, -11.2847, -10.7531, -9.2451, -10.0208, -9.3352, -9.829, -5.4607, -8.429, -10.574, -10.8437, -5.8061, -6.1996, -8.2874, -5.8271, -7.7314, -6.0636, -5.2838, -6.8575, -6.8885, -5.7734, -6.8631, -5.4706, -6.1632, -5.6382, -5.8227, -5.9587, -5.1003, -5.471, -6.918, -5.6086, -6.3059, -5.9284, -5.2221, -6.0465, -6.0431, -6.7365, -6.0416, -5.1629, -5.6901, -5.3892, -5.6908, -6.3651, -6.3163, -6.3118, -5.5741, -6.2435, -5.9384, -5.9506, -6.001, -6.1236, -5.9966, -5.8906, -6.0416, -6.0516, -11.0663, -11.2223, -11.3121, -11.5778, -12.1517, -11.4738, -11.7917, -11.6505, -11.7092, -11.454, -11.8677, -11.689, -10.4019, -11.8836, -12.5694, -12.2901, -12.5861, -12.3069, -11.1076, -10.9458, -12.6078, -11.1511, -13.0167, -12.3347, -11.7418, -12.0901, -11.0576, -12.6393, -12.6519, -12.6667, -9.9541, -10.8847, -10.7179, -8.9403, -8.0698, -9.9162, -10.2132, -7.1904, -10.2704, -6.8937, -5.3906, -9.1284, -6.2679, -6.5872, -5.4071, -6.6802, -8.0808, -4.9375, -5.4666, -5.6827, -5.8288, -5.9782, -6.4096, -6.7221, -5.9287, -6.6862, -5.8782, -6.4908, -6.2386, -5.85, -5.1753, -6.4932, -5.5261, -5.8272, -5.8188, -5.8265, -6.204, -5.3928, -6.1126, -5.5082, -5.5467, -5.2214, -5.5463, -5.8792, -6.1066, -6.0804, -6.1287, -5.8365, -6.0409, -6.0164, -6.0935, -6.0957, -11.7799, -12.3722, -12.3864, -10.8788, -12.1466, -10.8247, -10.823, -10.8043, -11.0058, -11.2625, -9.9264, -11.6751, -10.2495, -12.5597, -10.1056, -11.4841, -11.0314, -12.9918, -11.2235, -13.0162, -13.031, -12.0932, -12.3309, -9.3129, -13.0528, -12.1385, -12.6476, -12.6402, -13.0571, -11.1338, -10.5222, -11.9472, -11.9645, -7.6593, -10.0512, -9.7422, -9.4081, -10.8212, -10.4224, -10.6969, -9.0767, -10.0734, -9.3436, -9.3669, -6.1988, -4.8705, -5.6348, -5.7075, -7.5688, -7.1088, -7.1492, -5.7533, -5.9011, -7.5041, -6.9595, -5.5348, -5.116, -5.9931, -6.1716, -7.3526, -5.4, -5.1049, -5.9174, -5.6674, -5.8141, -5.6911, -5.5217, -5.3984, -6.3754, -6.9875, -5.8715, -5.8719, -6.2345, -5.6306, -6.3009, -6.2677, -6.1247, -6.1104, -5.9143, -6.4459, -5.9722, -6.0707, -5.8471, -5.7633, -6.0562, -6.1571, -6.048, -6.1133, -10.9644, -11.8252, -10.7903, -10.6897, -11.1136, -12.08, -11.4557, -11.405, -12.619, -8.5587, -11.1582, -11.9721, -11.34, -12.1328, -9.6051, -12.3697, -12.3645, -11.9672, -11.5989, -11.0316, -9.8742, -10.7398, -10.6735, -12.1956, -11.3068, -11.7433, -12.2182, -12.7208, -11.5202, -11.7839, -10.0664, -10.5931, -11.5291, -9.4424, -9.9494, -10.7952, -8.3748, -9.2821, -8.7724, -8.7008, -9.9304, -10.7347, -5.9086, -6.2682, -7.1493, -9.8113, -6.5593, -6.203, -5.919, -6.0261, -5.6984, -5.8239, -5.8985, -5.2559, -7.1108, -7.6609, -5.599, -6.5541, -6.1688, -5.8978, -6.4936, -7.036, -6.0259, -6.5444, -6.5127, -6.4827, -5.8312, -6.4654, -6.6927, -6.0401, -5.6438, -5.8193, -5.2517, -6.3939, -5.5434, -5.6719, -5.2438, -6.2387, -5.773, -5.9688, -6.3384, -5.6253, -5.9362, -5.989, -5.7522, -5.9957, -5.4329, -5.884, -5.6993, -5.9646, -6.0149, -10.209, -10.0413, -11.6395, -10.89, -12.2447, -11.7156, -11.3559, -10.8871, -10.1981, -12.2919, -10.4025, -11.5, -11.7561, -10.916, -9.3772, -12.3209, -9.3785, -11.8936, -10.917, -12.3544, -12.6561, -9.4985, -12.1271, -10.4003, -12.1504, -12.3727, -11.5751, -12.3964, -11.7996, -11.8275, -10.2927, -9.9532, -10.6752, -10.6876, -10.5833, -8.5926, -9.4024, -9.7153, -7.9589, -10.2641, -6.662, -7.4877, -7.1767, -5.18, -6.8918, -6.7534, -5.1664, -7.2854, -4.948, -5.9832, -6.8753, -5.5734, -6.7542, -6.0055, -6.8017, -7.5356, -6.1465, -6.3408, -6.4713, -5.7416, -6.7119, -6.8117, -6.0577, -5.8527, -5.2132, -5.1621, -6.6598, -6.53, -6.96, -6.6797, -5.8907, -5.7227, -6.6424, -6.0912, -6.3866, -5.9694, -5.6486, -6.122, -6.119, -6.3609, -5.7773, -5.7062, -6.0013, -6.1856, -6.021, -5.9028, -6.0395, -6.2615, -6.1024, -6.1998, -6.1621, -6.0887, -6.1016, -6.1769, -9.4274, -9.2809, -11.4371, -9.733, -11.1235, -10.2857, -10.0971, -9.6781, -12.2569, -10.8913, -11.0906, -11.0761, -10.6048, -10.2546, -10.2697, -10.9339, -12.5454, -12.5598, -12.0977, -11.4117, -12.6054, -12.2726, -11.1053, -11.3878, -11.3943, -12.6183, -12.3483, -10.7932, -11.9172, -11.5268, -9.9588, -10.1787, -10.9638, -10.1101, -9.5646, -8.8509, -9.651, -10.0689, -10.3498, -9.9763, -9.475, -9.0263, -10.3078, -10.2857, -9.8081, -9.438, -5.617, -8.9236, -5.7077, -6.1617, -7.4319, -5.3071, -5.0682, -6.0122, -5.9581, -6.1615, -6.7721, -5.2894, -5.0838, -6.7019, -6.785, -6.0934, -6.353, -6.1699, -5.7898, -5.6147, -6.2948, -5.435, -5.7113, -5.675, -6.5557, -6.3974, -6.6894, -6.0898, -5.7602, -6.2854, -5.6979, -6.1771, -5.3812, -5.862, -6.0661, -6.1049, -6.0178, -6.0121, -5.9133, -6.1909, -6.1181, -10.2463, -11.4229, -11.1309, -11.3873, -10.3258, -10.7655, -10.7622, -11.3838, -10.9365, -12.0769, -12.081, -10.3271, -11.687, -11.8621, -11.4454, -10.2009, -12.1116, -11.5416, -11.4189, -11.7243, -10.8759, -11.9147, -11.117, -10.0319, -12.4476, -11.7557, -10.5707, -12.4641, -11.1854, -11.9372, -10.7033, -10.4415, -10.2956, -9.3896, -9.9338, -9.8954, -9.4047, -8.0674, -9.1713, -7.4509, -5.1899, -6.2597, -6.2695, -5.8692, -6.307, -4.9488, -6.3464, -5.8155, -6.7204, -6.4205, -5.9941, -6.2929, -6.3026, -5.0657, -5.3944, -7.4173, -6.383, -6.0967, -7.2159, -5.6531, -7.0275, -7.1377, -5.668, -5.9037, -6.2586, -6.3603, -5.5596, -5.996, -6.0088, -6.4239, -5.681, -6.4157, -5.7941, -5.9254, -6.0991, -5.4612, -5.8392, -5.6449, -5.911, -6.0255, -6.0646, -6.0622, -6.1104, -6.1016, -10.457, -10.2668, -12.0195, -12.0721, -12.1148, -12.1305, -12.4389, -12.8832, -12.4875, -12.1993, -12.8908, -11.9798, -11.53, -12.4965, -12.9228, -11.032, -10.8575, -10.6496, -12.9507, -12.9537, -12.9542, -11.8303, -11.3258, -11.8498, -12.0331, -9.1705, -12.9701, -12.2666, -12.0544, -12.0621, -8.2594, -11.4708, -10.4352, -11.2898, -10.4758, -9.9009, -11.7215, -11.2249, -10.2499, -8.6625, -6.7971, -10.121, -9.4632, -7.3037, -6.7973, -6.2925, -9.7491, -5.6257, -6.3334, -6.156, -5.9283, -6.1503, -6.6464, -6.3916, -5.2181, -4.9852, -6.4264, -5.5474, -5.7667, -5.0928, -6.4533, -6.6428, -5.7756, -6.4284, -5.4429, -5.8311, -6.3036, -5.5758, -6.1216, -5.4837, -6.1101, -5.9302, -5.5717, -5.6955, -5.258, -6.0566, -6.036, -5.9742, -5.9708, -5.8582, -5.9504, -6.061, -6.1477, -6.1302, -6.1568, -9.975, -11.7775, -12.4502, -12.9048, -12.9179, -11.6669, -12.9192, -10.3672, -12.2506, -12.5427, -12.9502, -11.872, -9.5433, -12.9809, -12.9798, -12.9806, -12.9824, -12.9926, -12.2889, -12.9979, -13.0056, -13.0034, -11.756, -11.5068, -12.3382, -13.0358, -12.346, -10.3854, -11.0039, -12.6544, -10.4489, -8.527, -8.1165, -8.0086, -9.2496, -9.7878, -9.0774, -11.0497, -7.236, -11.1405, -10.3615, -10.1399, -7.2579, -7.0217, -7.9827, -8.6752, -7.1104, -6.4103, -9.3458, -10.6447, -7.026, -9.0899, -7.0606, -6.2633, -6.712, -5.0261, -6.2677, -5.6444, -5.4956, -6.9854, -6.3574, -5.8914, -5.456, -6.3642, -5.78, -6.4622, -5.8243, -5.6719, -6.2359, -5.2557, -5.9546, -5.1842, -5.4871, -5.9074, -5.5343, -5.7009, -5.8791, -6.2511, -6.1953, -6.0367, -6.0976, -6.278, -5.8206, -5.9827, -6.022, -6.0529, -5.8011, -6.0961, -6.1981, -6.0473, -6.1601, -6.2101, -6.2303, -8.9739, -11.0998, -8.3382, -11.1386, -9.6103, -10.3224, -11.584, -11.312, -11.6271, -11.6285, -11.6182, -12.0369, -11.4825, -8.8296, -12.3454, -11.6687, -12.396, -11.2001, -12.3773, -9.6909, -12.1093, -11.8879, -12.4224, -12.8347, -11.9253, -11.9524, -12.8491, -11.7386, -10.6154, -12.4754, -10.0025, -10.5794, -9.5371, -9.899, -10.5437, -10.2697, -10.9553, -9.8534, -10.6578, -10.2259, -10.8536, -9.1816, -8.6572, -5.7738, -4.9495, -6.5872, -7.188, -6.6803, -7.3631, -5.9645, -6.1393, -6.3503, -6.9636, -7.7335, -5.4426, -6.4416, -6.1139, -5.1538, -5.682, -5.8398, -6.1129, -6.2304, -5.7561, -6.2358, -6.4503, -5.2999, -5.867, -5.5316, -6.3347, -5.6531, -5.8904, -6.145, -5.9301, -6.1319, -5.9897, -5.6886, -5.5846, -5.833, -6.0152, -5.9114, -6.0881, -6.2026, -6.1743, -6.1896, -6.2031], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 1.2529, 1.2414, 1.1962, 1.1499, 1.1008, 1.0876, 1.0799, 1.0718, 1.031, 1.0177, 0.9958, 0.9708, 0.9553, 0.9539, 0.9462, 0.9383, 0.9273, 0.9266, 0.9195, 0.9189, 0.9103, 0.908, 0.8968, 0.8928, 0.8892, 0.8873, 0.8865, 0.886, 0.8857, 0.8716, 0.852, 0.8421, 0.8154, 0.7947, 0.5401, 0.67, 0.8291, 0.8413, 0.4396, 0.4149, 0.614, 0.3293, 0.5198, 0.3176, 0.2143, 0.3917, 0.3902, 0.2145, 0.3549, 0.1388, 0.2488, 0.1625, 0.1885, 0.1848, 0.0078, 0.0633, 0.337, 0.0855, 0.2167, 0.1363, -0.018, 0.1602, 0.1568, 0.2999, 0.1443, -0.0633, 0.0582, -0.0126, 0.0546, 0.2163, 0.186, 0.1667, -0.1016, 0.1272, -0.0183, -0.0138, -0.0151, 0.0527, -0.0768, -0.1928, -0.137, -0.1047, 1.2959, 1.2448, 1.1609, 1.1468, 1.1305, 1.1115, 1.0917, 1.0417, 1.0252, 1.0238, 1.0197, 1.0131, 1.0065, 1.002, 1.0009, 1.0001, 0.9882, 0.9827, 0.9731, 0.9659, 0.9643, 0.9574, 0.9564, 0.9532, 0.9417, 0.9404, 0.9403, 0.9373, 0.925, 0.911, 0.8738, 0.883, 0.8769, 0.796, 0.7506, 0.8237, 0.8298, 0.6142, 0.8316, 0.5315, 0.3547, 0.7, 0.3908, 0.4206, 0.287, 0.426, 0.5655, 0.1621, 0.2312, 0.2452, 0.2359, 0.2549, 0.3044, 0.3356, 0.2149, 0.323, 0.1931, 0.2779, 0.2361, 0.1612, 0.0288, 0.2613, 0.0833, 0.1197, 0.118, 0.0933, 0.176, -0.0161, 0.1398, -0.0101, -0.0124, -0.1132, -0.0738, 0.0408, 0.1086, 0.0959, 0.1088, -0.0882, -0.0531, -0.2157, -0.0927, -0.1098, 1.2753, 1.2031, 1.1912, 1.14, 1.1356, 1.1205, 1.1125, 1.0899, 1.0872, 1.0781, 1.0381, 1.0375, 1.0165, 1.0142, 1.0072, 1.0051, 1.0018, 0.9902, 0.9668, 0.9636, 0.952, 0.9474, 0.9407, 0.9395, 0.9274, 0.9246, 0.924, 0.9229, 0.9215, 0.9203, 0.9131, 0.9097, 0.9094, 0.814, 0.8683, 0.8581, 0.8412, 0.883, 0.8603, 0.8667, 0.762, 0.8212, 0.7613, 0.7421, 0.4257, 0.2291, 0.3121, 0.2933, 0.5095, 0.4463, 0.4319, 0.2326, 0.2537, 0.4757, 0.3859, 0.163, 0.0881, 0.2136, 0.2275, 0.4226, 0.0725, 0.0032, 0.1422, 0.078, 0.0905, 0.0572, 0.0126, -0.0217, 0.1799, 0.3138, 0.0653, 0.0481, 0.1362, -0.0212, 0.1408, 0.1309, 0.0905, 0.0755, 0.0055, 0.1695, -0.0443, 0.0005, -0.153, -0.2652, -0.045, 0.0192, -0.2473, -0.1254, 1.0821, 1.0687, 1.0333, 1.0146, 1.0027, 1.0022, 0.9977, 0.9817, 0.9812, 0.975, 0.9542, 0.9472, 0.9463, 0.9438, 0.9362, 0.9322, 0.932, 0.9271, 0.9079, 0.9078, 0.8968, 0.896, 0.8842, 0.8809, 0.8801, 0.8716, 0.8672, 0.8669, 0.8598, 0.8579, 0.8517, 0.8514, 0.8574, 0.7809, 0.7899, 0.8262, 0.6858, 0.7095, 0.6702, 0.66, 0.7455, 0.8109, 0.3245, 0.3636, 0.4216, 0.7166, 0.3532, 0.3031, 0.2669, 0.2684, 0.2216, 0.2357, 0.245, 0.1208, 0.3948, 0.4595, 0.1464, 0.2874, 0.223, 0.1735, 0.2609, 0.3484, 0.174, 0.2579, 0.249, 0.2414, 0.1157, 0.2321, 0.2719, 0.1362, 0.054, 0.0853, -0.0475, 0.2052, -0.0091, 0.0222, -0.1357, 0.1599, -0.0247, 0.0425, 0.1825, -0.1272, -0.0165, -0.0031, -0.1429, -0.0078, -0.3333, -0.0833, -0.2268, -0.0367, -0.0781, 1.2199, 1.1997, 1.1009, 1.0788, 1.0474, 1.0206, 1.0171, 1.012, 1.005, 1.0048, 1.0015, 0.9922, 0.9848, 0.9837, 0.977, 0.9747, 0.9685, 0.9661, 0.9399, 0.9374, 0.9332, 0.9272, 0.9253, 0.9242, 0.9221, 0.9211, 0.9059, 0.9056, 0.9034, 0.9018, 0.8715, 0.8553, 0.8655, 0.8577, 0.851, 0.7299, 0.7573, 0.7679, 0.6147, 0.8007, 0.4605, 0.5383, 0.5008, 0.2925, 0.4678, 0.4167, 0.2103, 0.4704, 0.1516, 0.2903, 0.4083, 0.2273, 0.3864, 0.247, 0.3617, 0.4729, 0.2453, 0.2746, 0.2887, 0.163, 0.3163, 0.3307, 0.1798, 0.1351, -0.0091, -0.0539, 0.2841, 0.2496, 0.3586, 0.2802, 0.0462, -0.0248, 0.2579, 0.0653, 0.1687, 0.0165, -0.1143, 0.0639, 0.0573, 0.149, -0.1679, -0.2081, -0.0734, 0.0212, -0.1009, -0.2087, -0.1197, 0.033, -0.1555, -0.045, -0.1025, -0.3403, -0.3562, -0.1761, 1.3028, 1.2453, 1.2025, 1.1473, 1.0994, 1.0989, 1.0905, 1.0694, 1.0598, 1.0593, 1.0575, 1.0561, 1.0356, 1.0191, 1.0118, 1.0083, 1.008, 1.0062, 1.0033, 1.0, 0.9991, 0.9991, 0.9982, 0.9865, 0.9796, 0.9768, 0.9707, 0.969, 0.9653, 0.9587, 0.9567, 0.9583, 0.9555, 0.9362, 0.9164, 0.874, 0.9062, 0.918, 0.9245, 0.9037, 0.8646, 0.827, 0.9141, 0.9059, 0.8612, 0.8023, 0.3109, 0.7309, 0.2781, 0.3443, 0.5098, 0.191, 0.1359, 0.286, 0.2571, 0.2568, 0.3504, 0.0873, 0.0243, 0.3279, 0.3435, 0.2011, 0.2477, 0.2113, 0.1299, 0.0831, 0.2207, 0.0375, 0.0894, 0.0733, 0.2677, 0.2195, 0.2902, 0.1101, -0.0148, 0.1332, -0.0886, 0.0604, -0.2816, -0.1679, -0.0548, -0.0337, -0.0977, -0.1074, -0.379, -0.0146, -0.1813, 2.3846, 1.8945, 1.6238, 1.5032, 1.3656, 1.3393, 1.3388, 1.3386, 1.248, 1.2431, 1.2375, 1.2235, 1.223, 1.2165, 1.2078, 1.2075, 1.2073, 1.2022, 1.1846, 1.1706, 1.1647, 1.1635, 1.1631, 1.1592, 1.157, 1.1553, 1.14, 1.1374, 1.1305, 1.1295, 1.129, 1.1198, 1.1104, 1.0618, 1.0368, 0.9877, 0.9297, 0.769, 0.9037, 0.6646, 0.3444, 0.4486, 0.4383, 0.346, 0.3957, 0.1508, 0.3512, 0.2492, 0.416, 0.3482, 0.2584, 0.3093, 0.2965, 0.0424, 0.0781, 0.522, 0.2881, 0.2016, 0.4616, 0.0447, 0.3947, 0.4173, 0.0261, 0.0842, 0.1831, 0.191, -0.0615, 0.0635, 0.0624, 0.1916, -0.0717, 0.185, -0.0488, -0.0053, 0.0445, -0.2571, -0.0909, -0.2682, -0.1103, -0.1058, -0.1277, -0.1343, -0.1635, -0.1969, 1.4928, 1.3576, 1.2962, 1.253, 1.2092, 1.1887, 1.1642, 1.1262, 1.1213, 1.1184, 1.1164, 1.1048, 1.0981, 1.0893, 1.0832, 1.0817, 1.0684, 1.0651, 1.0566, 1.0541, 1.0502, 1.0502, 1.046, 1.0451, 1.0409, 1.0384, 1.0358, 1.0347, 1.028, 1.0265, 1.0068, 1.0159, 0.9895, 1.0042, 0.9744, 0.9503, 1.0157, 0.9812, 0.9075, 0.7514, 0.5523, 0.8742, 0.7951, 0.5733, 0.4966, 0.4228, 0.7995, 0.2789, 0.3694, 0.3431, 0.3092, 0.3243, 0.3819, 0.3374, 0.1586, 0.123, 0.3383, 0.2009, 0.2211, 0.1114, 0.3082, 0.3369, 0.1442, 0.254, 0.0552, 0.1158, 0.2025, 0.0335, 0.1519, -0.0112, 0.1424, 0.081, -0.0374, -0.0014, -0.1584, 0.0982, 0.0288, -0.0374, -0.0508, -0.1604, -0.1497, -0.1331, -0.0881, -0.1443, -0.0856, 1.4471, 1.3666, 1.1816, 1.1264, 1.1127, 1.1116, 1.1102, 1.0989, 1.0967, 1.0897, 1.0796, 1.0722, 1.0629, 1.0501, 1.05, 1.0465, 1.0436, 1.0358, 1.0353, 1.0236, 1.0219, 1.0185, 1.0057, 1.0043, 0.9921, 0.991, 0.9848, 0.9844, 0.9757, 0.9674, 0.9615, 0.8869, 0.8422, 0.8312, 0.8585, 0.8796, 0.8264, 0.9228, 0.6334, 0.9249, 0.8638, 0.8387, 0.5955, 0.5491, 0.64, 0.6855, 0.5031, 0.3837, 0.7292, 0.8746, 0.4131, 0.6752, 0.3982, 0.2862, 0.334, 0.082, 0.255, 0.1563, 0.1138, 0.3529, 0.2449, 0.1681, 0.0783, 0.2365, 0.1246, 0.2461, 0.1125, 0.0764, 0.1827, -0.0516, 0.1101, -0.0846, -0.0146, 0.0785, -0.0362, -0.0031, 0.041, 0.1672, 0.1029, 0.0346, 0.0571, 0.1212, -0.1265, -0.0548, -0.1022, -0.106, -0.4244, -0.0848, 0.035, -0.3019, -0.1722, -0.0338, -0.0444, 1.4631, 1.4336, 1.3916, 1.3797, 1.3718, 1.3664, 1.3486, 1.3324, 1.3173, 1.2987, 1.2974, 1.2971, 1.2771, 1.264, 1.2618, 1.2544, 1.2439, 1.2333, 1.232, 1.2249, 1.218, 1.1975, 1.1873, 1.1779, 1.1762, 1.1705, 1.1689, 1.1649, 1.1628, 1.1603, 1.1479, 1.1523, 1.1194, 1.0828, 1.1066, 1.0851, 1.1193, 1.0412, 1.095, 1.0458, 1.0968, 0.9252, 0.8188, 0.381, 0.1586, 0.4131, 0.5185, 0.4274, 0.5171, 0.2354, 0.2314, 0.2642, 0.3875, 0.5396, 0.0556, 0.2664, 0.1806, -0.0542, 0.0663, 0.0881, 0.1396, 0.1688, 0.0446, 0.1628, 0.2119, -0.0958, 0.0528, -0.059, 0.1752, -0.0438, 0.0142, 0.088, 0.0067, 0.0748, 0.0112, -0.1543, -0.2079, -0.1388, -0.0682, -0.1661, -0.0285, -0.0461, -0.163, -0.2037, -0.1318]}, \"token.table\": {\"Topic\": [1, 7, 8, 1, 3, 5, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 9, 1, 2, 3, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 3, 1, 2, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 3, 1, 2, 3, 4, 5, 6, 8, 9, 10, 2, 4, 3, 4, 6, 7, 1, 2, 3, 6, 7, 8, 1, 3, 8, 10, 1, 2, 3, 5, 6, 7, 8, 10, 1, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 6, 1, 2, 3, 4, 6, 7, 8, 10, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 3, 4, 5, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 4, 1, 2, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 2, 1, 5, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 5, 1, 4, 1, 2, 4, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 7, 8, 1, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 3, 4, 7, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 4, 5, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 10, 1, 2, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 3, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 5, 8, 9, 10, 1, 2, 3, 5, 6, 8, 10, 1, 2, 3, 4, 5, 6, 7, 8, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 3, 4, 6, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 3, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 7, 8, 1, 2, 3, 4, 5, 6, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 5, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 8, 1, 2, 3, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 1, 2, 3, 4, 5, 6, 9, 10, 5, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 4, 7, 8, 2, 3, 4, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 9, 10, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 3, 8, 10, 2, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 6, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 4, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 1, 3, 4, 5, 6, 7, 8, 9, 2, 3, 4, 7, 8, 9, 1, 3, 4, 5, 6, 7, 8, 9, 4, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 8, 9, 1, 2, 3, 5, 6, 7, 8, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 10, 1, 5, 6, 2, 3, 4, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 8, 2, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 4, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 3, 4, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 7, 10, 1, 2, 3, 1, 6, 1, 2, 5, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 8, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 5, 6, 10, 1, 2, 3, 4, 5, 6, 9, 1, 2, 5, 1, 2, 3, 4, 5, 6, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 1, 2, 3, 6, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 3, 1, 3, 5, 8, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 3, 4, 8, 1, 2, 3, 4, 5, 7, 9, 10, 2, 3, 4, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 3, 2, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 1, 1, 3, 5, 8, 10, 1, 2, 3, 4, 5, 6, 7, 8, 10, 1, 2, 5, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 3, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 3, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 4, 5, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 4, 5, 6, 1, 3, 4, 5, 6, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 4, 5, 6, 9, 1, 2, 3, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 5, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 5, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 9, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 3, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 10, 1, 2, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 4, 5, 6, 7, 8, 9, 10, 1, 2, 4, 5, 6, 7, 8, 9, 10, 2, 1, 2, 3, 4, 5, 6, 10, 7, 2, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 10, 1, 2, 3, 4, 6, 8, 10, 4, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 4, 6, 1, 2, 3, 4, 5, 6, 7, 8, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 3, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 3, 4, 5, 7, 10, 1, 2, 4, 5, 6, 7, 8, 9, 1, 2, 3, 6, 7, 8, 1, 2, 3, 4, 5, 7, 8, 9, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 3, 7, 1, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 8, 9, 10, 3, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 3, 3, 3, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 4, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 3, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 10, 1, 2, 3, 6, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 2, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 4, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 4, 5, 6, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 6, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 5, 6, 8, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 5, 1, 2, 3, 4, 5, 6, 7, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 9, 1, 2, 3, 4, 5, 6, 7, 8, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 4, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 3, 5, 8, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 3, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 3, 5, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 6, 1, 2, 3, 4, 5, 6, 8, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 4, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 5, 6, 9, 1, 2, 4, 5, 6, 7, 8, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 4, 5, 6, 7, 8, 9, 10, 2, 3, 4, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 3, 1, 2, 4, 5, 6, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10], \"Freq\": [0.20891567645984285, 0.20891567645984285, 0.20891567645984285, 0.14903291195117574, 0.14903291195117574, 0.2980658239023515, 0.14903291195117574, 0.1287210360741024, 0.10726753006175199, 0.2359885661358544, 0.0858140240494016, 0.10726753006175199, 0.053633765030875996, 0.0643605180370512, 0.09654077705557679, 0.0643605180370512, 0.053633765030875996, 0.13867006431498183, 0.12717059556691018, 0.1664040771779782, 0.1102596121138636, 0.09064287130832958, 0.06832037315030812, 0.08117272057462352, 0.09537794667518262, 0.044644996316042934, 0.07711408454589233, 0.13904964973043007, 0.11481163739209824, 0.18242293496744497, 0.09312499477359079, 0.08547088561411757, 0.08164383103438097, 0.052303079256400306, 0.11481163739209824, 0.044648970096927094, 0.0918493099136786, 0.07502449908654721, 0.15004899817309442, 0.30009799634618883, 0.07502449908654721, 0.07502449908654721, 0.07502449908654721, 0.07502449908654721, 0.07502449908654721, 0.07502449908654721, 0.07502449908654721, 0.08804067436154843, 0.30814236026541947, 0.044020337180774215, 0.11005084295193554, 0.11005084295193554, 0.06603050577116132, 0.08804067436154843, 0.06603050577116132, 0.06603050577116132, 0.044020337180774215, 0.053216075342751294, 0.10643215068550259, 0.10643215068550259, 0.15964822602825388, 0.15964822602825388, 0.26608037671375645, 0.10643215068550259, 0.053216075342751294, 0.053216075342751294, 0.052880803048250026, 0.10576160609650005, 0.15864240914475009, 0.31728481828950017, 0.052880803048250026, 0.052880803048250026, 0.10576160609650005, 0.052880803048250026, 0.052880803048250026, 0.052880803048250026, 0.09734159005767248, 0.2920247701730175, 0.04867079502883624, 0.24335397514418122, 0.14601238508650874, 0.04867079502883624, 0.04867079502883624, 0.04867079502883624, 0.04867079502883624, 0.04867079502883624, 0.06406917628742577, 0.06406917628742577, 0.16017294071856442, 0.12813835257485154, 0.2883112932934159, 0.09610376443113865, 0.032034588143712885, 0.06406917628742577, 0.06406917628742577, 0.032034588143712885, 0.0768227887796422, 0.0384113943898211, 0.1536455775592844, 0.19205697194910548, 0.0768227887796422, 0.2688797607287477, 0.0384113943898211, 0.0768227887796422, 0.0384113943898211, 0.0768227887796422, 0.15724288466461256, 0.15724288466461256, 0.07862144233230628, 0.07862144233230628, 0.15724288466461256, 0.3144857693292251, 0.07862144233230628, 0.07862144233230628, 0.1712490946117749, 0.1298772780425761, 0.11051375658895567, 0.12260415047219185, 0.12694913577397984, 0.09275599057295253, 0.06224663725822375, 0.09388946499950593, 0.04354430922009279, 0.046377995286476265, 0.20028086450610405, 0.11244582435194402, 0.12178094939248275, 0.14193633300273686, 0.10968771922633029, 0.0736201906606124, 0.05961750309980428, 0.07722694351718419, 0.0636485798218551, 0.03967428142228968, 0.17027398556320641, 0.13758528346367968, 0.11855753149529846, 0.13563371915923034, 0.0930245651787527, 0.07708679002574961, 0.08098991863464833, 0.07611100787352494, 0.06928053280795218, 0.041633371828252955, 0.20120694252590493, 0.20120694252590493, 0.20120694252590493, 0.20496450950638473, 0.1230924169435015, 0.11996535785255805, 0.11570118636490788, 0.11598546446408456, 0.07163808099252282, 0.07988214586864648, 0.07874503347193976, 0.04008321198391158, 0.0500329454550953, 0.3633827670622356, 0.17030643757473446, 0.17030643757473446, 0.3406128751494689, 0.2223602371877533, 0.10532853340472526, 0.09176349501169245, 0.11995749637760376, 0.09974292936053528, 0.0718149091395854, 0.07633658860392967, 0.08431602295277249, 0.06755921082020257, 0.06037771990624402, 0.5112444274985132, 0.3429713778231865, 0.36480706906213534, 0.18240353453106767, 0.18240353453106767, 0.15904887394216208, 0.15904887394216208, 0.07952443697108104, 0.31809774788432416, 0.07952443697108104, 0.07952443697108104, 0.07952443697108104, 0.07952443697108104, 0.07952443697108104, 0.07952443697108104, 0.10461040188823681, 0.10461040188823681, 0.10461040188823681, 0.20922080377647362, 0.10461040188823681, 0.10461040188823681, 0.10461040188823681, 0.10461040188823681, 0.10461040188823681, 0.10461040188823681, 0.1340111311652077, 0.05956050274009231, 0.14890125685023076, 0.04467037705506923, 0.10423087979516153, 0.05956050274009231, 0.04467037705506923, 0.10423087979516153, 0.029780251370046154, 0.2531321366453923, 0.08568034768071534, 0.25704104304214603, 0.08568034768071534, 0.25704104304214603, 0.17136069536143067, 0.08568034768071534, 0.19691564224414848, 0.10108336301866289, 0.10633444681184018, 0.09320673732889695, 0.16540913948508473, 0.0787662568976594, 0.07351517310448209, 0.07088963120789345, 0.05513637982836157, 0.05907469267324454, 0.5146914074262058, 0.12850260737868646, 0.06425130368934323, 0.12850260737868646, 0.06425130368934323, 0.2570052147573729, 0.12850260737868646, 0.06425130368934323, 0.06425130368934323, 0.06425130368934323, 0.20975412828832424, 0.20975412828832424, 0.17673273691492272, 0.17673273691492272, 0.17673273691492272, 0.17673273691492272, 0.11405814953977887, 0.4562325981591155, 0.11405814953977887, 0.11405814953977887, 0.11405814953977887, 0.11405814953977887, 0.1775703538096371, 0.1775703538096371, 0.1775703538096371, 0.1775703538096371, 0.15600467087713016, 0.15600467087713016, 0.3120093417542603, 0.07800233543856508, 0.07800233543856508, 0.07800233543856508, 0.07800233543856508, 0.07800233543856508, 0.17392321681736403, 0.17392321681736403, 0.34784643363472806, 0.17392321681736403, 0.20951547105545093, 0.15517684973579743, 0.10796460498265584, 0.11330938740753979, 0.0791027798882825, 0.08052805520158489, 0.05807996901707228, 0.08694179411144563, 0.06110867905783986, 0.04845936065228117, 0.10439815261534423, 0.052199076307672114, 0.15659722892301634, 0.052199076307672114, 0.052199076307672114, 0.052199076307672114, 0.31319445784603267, 0.10439815261534423, 0.052199076307672114, 0.052199076307672114, 0.13510474546758428, 0.14203319395310143, 0.09815302021149286, 0.09930776162574571, 0.10046250303999857, 0.08083189899769999, 0.07390345051218285, 0.10623621011126286, 0.06812974344091857, 0.09584353738298713, 0.14859641913977809, 0.10807012301074768, 0.06754382688171731, 0.10807012301074768, 0.05403506150537384, 0.2026314806451519, 0.06754382688171731, 0.09456135763440424, 0.08105259225806077, 0.06754382688171731, 0.3503025896868292, 0.09536091156610721, 0.09536091156610721, 0.09536091156610721, 0.09536091156610721, 0.09536091156610721, 0.09536091156610721, 0.19072182313221442, 0.09536091156610721, 0.26584575620482337, 0.1892298108831839, 0.11353788652991036, 0.037845962176636785, 0.11353788652991036, 0.07569192435327357, 0.037845962176636785, 0.07569192435327357, 0.11353788652991036, 0.1892298108831839, 0.07569192435327357, 0.17656490299942162, 0.17656490299942162, 0.17656490299942162, 0.35312980599884325, 0.41035017851426897, 0.11398616069840804, 0.06839169641904483, 0.11398616069840804, 0.045594464279363214, 0.045594464279363214, 0.045594464279363214, 0.045594464279363214, 0.022797232139681607, 0.09118892855872643, 0.15696162686060736, 0.11772122014545552, 0.11772122014545552, 0.15696162686060736, 0.07848081343030368, 0.03924040671515184, 0.23544244029091105, 0.03924040671515184, 0.03924040671515184, 0.03924040671515184, 0.3430913373799438, 0.15154413486131985, 0.10102942324087989, 0.10600974692176834, 0.09996221102354666, 0.11027859579110129, 0.0786179666768819, 0.12095071796443367, 0.11597039428354523, 0.07185895596710472, 0.04446717572221826, 0.11564673090971586, 0.1374911134148844, 0.10022716678842042, 0.11950162194003973, 0.10986439436423007, 0.07581285692970262, 0.11628921274810318, 0.1317087768693986, 0.055895919939696004, 0.03790642846485131, 0.20858442479067613, 0.20858442479067613, 0.20858442479067613, 0.16791003434143092, 0.16791003434143092, 0.33582006868286185, 0.09747482146140284, 0.09747482146140284, 0.09747482146140284, 0.09747482146140284, 0.09747482146140284, 0.09747482146140284, 0.2924244643842085, 0.09747482146140284, 0.09747482146140284, 0.3423387637103291, 0.2591687828768275, 0.2591687828768275, 0.2591687828768275, 0.24108350116443747, 0.05113892448942613, 0.05113892448942613, 0.10227784897885225, 0.1315000915442386, 0.2045556979577045, 0.0730556064134659, 0.08036116705481249, 0.029222242565386358, 0.03652780320673295, 0.2596603271358477, 0.25988997940150305, 0.25988997940150305, 0.20855665928285025, 0.20855665928285025, 0.20855665928285025, 0.16451462652929594, 0.13222892781647594, 0.09780667551236641, 0.09970583426017934, 0.10279196722537538, 0.11323734033834655, 0.09946843941670273, 0.06409660773868672, 0.07359240147775142, 0.05222686556485585, 0.18117711522158916, 0.08620524030704646, 0.09643298068245876, 0.07013307685997, 0.09643298068245876, 0.06574975955622188, 0.15779942293493252, 0.11688846143328334, 0.07013307685997, 0.05698312494872563, 0.21200246460695932, 0.0652315275713721, 0.09784729135705815, 0.22831034649980234, 0.09784729135705815, 0.048923645678529075, 0.048923645678529075, 0.08153940946421512, 0.03261576378568605, 0.09784729135705815, 0.14998607950908216, 0.20623085932498797, 0.11248955963181162, 0.05624477981590581, 0.07499303975454108, 0.07499303975454108, 0.03749651987727054, 0.07499303975454108, 0.16873433944771743, 0.05624477981590581, 0.1495184564334982, 0.1495184564334982, 0.1495184564334982, 0.1495184564334982, 0.2990369128669964, 0.1495184564334982, 0.20771723306473316, 0.20771723306473316, 0.14536660292052625, 0.13445436463158594, 0.10054848137666426, 0.09353347104805979, 0.12743935430298145, 0.07677539081861574, 0.0841801239432538, 0.07833428200275007, 0.09743069900839561, 0.06196592456933961, 0.21074757646397616, 0.11829698867553377, 0.07985875145883373, 0.15640386177829677, 0.11233243462466652, 0.09046240310481994, 0.06163372519229491, 0.07389419740796647, 0.053349622343868176, 0.04374006303969316, 0.0761650005512955, 0.2284950016538865, 0.1269416675854925, 0.050776667034197004, 0.0761650005512955, 0.050776667034197004, 0.2284950016538865, 0.0761650005512955, 0.050776667034197004, 0.050776667034197004, 0.1993361065272723, 0.14135746058443688, 0.06515695448813887, 0.06957437513140252, 0.14301399332566073, 0.07454397335507414, 0.11264422640322314, 0.08724405770445713, 0.0673656648097707, 0.03920460820896491, 0.10686937417131175, 0.05343468708565587, 0.10686937417131175, 0.10686937417131175, 0.10686937417131175, 0.10686937417131175, 0.05343468708565587, 0.2137387483426235, 0.05343468708565587, 0.05343468708565587, 0.136587861421756, 0.136587861421756, 0.136587861421756, 0.273175722843512, 0.136587861421756, 0.11116496490593371, 0.1261872574607896, 0.11416942341690488, 0.11116496490593371, 0.08712929681816425, 0.06609808724136598, 0.1742585936363285, 0.06008917021942362, 0.09614267235107779, 0.05107579468651008, 0.15480247264911318, 0.15480247264911318, 0.15480247264911318, 0.15480247264911318, 0.15480247264911318, 0.15480247264911318, 0.15480247264911318, 0.15465299693418533, 0.17441210219358985, 0.10449526819877387, 0.100695440264273, 0.07409664472276693, 0.09347576718872136, 0.1151347864153763, 0.08549612852626953, 0.0490177803550612, 0.048637797561611115, 0.14484595290403773, 0.21726892935605663, 0.36211488226009436, 0.07242297645201887, 0.07242297645201887, 0.07242297645201887, 0.07242297645201887, 0.21023492751039663, 0.21023492751039663, 0.21023492751039663, 0.15628137523161076, 0.11454162905173956, 0.11454162905173956, 0.10580540310711536, 0.08056741704486767, 0.11939508790986413, 0.066007040470494, 0.07765534172999293, 0.06988980755699363, 0.09609848539086625, 0.13236067121539333, 0.05672600194945429, 0.11345200389890858, 0.22690400779781716, 0.1512693385318781, 0.037817334632969525, 0.13236067121539333, 0.037817334632969525, 0.09454333658242381, 0.018908667316484763, 0.3439597694119435, 0.15765582797697422, 0.07882791398848711, 0.11824187098273066, 0.07882791398848711, 0.07882791398848711, 0.07882791398848711, 0.11824187098273066, 0.07882791398848711, 0.15765582797697422, 0.039413956994243556, 0.18273141362021966, 0.18273141362021966, 0.18273141362021966, 0.18273141362021966, 0.18273141362021966, 0.15772838089455513, 0.06759787752623792, 0.11266312921039652, 0.045065251684158605, 0.09013050336831721, 0.15772838089455513, 0.045065251684158605, 0.20279363257871375, 0.06759787752623792, 0.06759787752623792, 0.17033862005798592, 0.13584167133882313, 0.12336236039663749, 0.10357033221444502, 0.10198061107531309, 0.0973704197718305, 0.06295295710962429, 0.08950130013312746, 0.06319141528049407, 0.05182490913570081, 0.1319954074105566, 0.1319954074105566, 0.0659977037052783, 0.0659977037052783, 0.0659977037052783, 0.0659977037052783, 0.0659977037052783, 0.2639908148211132, 0.0659977037052783, 0.0659977037052783, 0.18076916607875607, 0.1097275871917741, 0.09636332977739136, 0.15404065124999056, 0.1097275871917741, 0.06682128707191372, 0.09425318386985725, 0.059084085410955285, 0.06893143297944783, 0.060490849349311364, 0.12448250472034285, 0.09336187854025714, 0.20228407017055713, 0.04668093927012857, 0.1089221916303, 0.2178443832606, 0.04668093927012857, 0.031120626180085712, 0.07780156545021427, 0.07780156545021427, 0.12116351963859878, 0.12116351963859878, 0.12116351963859878, 0.12116351963859878, 0.12116351963859878, 0.12116351963859878, 0.24232703927719756, 0.1353861494532146, 0.1353861494532146, 0.1353861494532146, 0.1353861494532146, 0.1353861494532146, 0.1353861494532146, 0.2707722989064292, 0.09463476720267577, 0.09463476720267577, 0.09463476720267577, 0.2839043016080273, 0.09463476720267577, 0.09463476720267577, 0.09463476720267577, 0.09463476720267577, 0.09463476720267577, 0.16747329858549873, 0.11503216468498903, 0.10995721624300422, 0.10431838464079887, 0.14773738797778002, 0.06484656342536146, 0.09022130563528552, 0.06992151186734627, 0.0687937455469052, 0.062027147624258795, 0.08101348495042943, 0.2673445003364171, 0.145824272910773, 0.145824272910773, 0.11341887893060121, 0.04860809097025766, 0.06481078796034355, 0.04860809097025766, 0.04050674247521471, 0.04050674247521471, 0.2962543727391815, 0.14812718636959074, 0.14812718636959074, 0.14812718636959074, 0.2698231305707936, 0.18282056450529063, 0.1452057587047768, 0.11515594178034398, 0.12440203929555409, 0.10906192296350097, 0.08384529337656432, 0.06325171254723273, 0.05946921810919224, 0.06430240544668843, 0.052534644972784664, 0.15925835112046702, 0.07962917556023351, 0.07962917556023351, 0.07962917556023351, 0.07962917556023351, 0.07962917556023351, 0.07962917556023351, 0.23888752668070054, 0.07962917556023351, 0.11848623259476658, 0.11848623259476658, 0.11848623259476658, 0.11848623259476658, 0.11848623259476658, 0.11848623259476658, 0.11848623259476658, 0.11848623259476658, 0.23697246518953316, 0.20504477158471868, 0.1253051381906614, 0.09682669769278382, 0.08543532149363278, 0.05126119289617967, 0.09682669769278382, 0.07973963339405726, 0.05695688099575519, 0.07404394529448174, 0.13100082629023693, 0.39066587262634106, 0.04883323407829263, 0.09766646815658526, 0.04883323407829263, 0.09766646815658526, 0.04883323407829263, 0.09766646815658526, 0.04883323407829263, 0.04883323407829263, 0.04883323407829263, 0.21494929966269424, 0.12514052344734994, 0.09972951312285329, 0.10773572185522894, 0.11783050677865911, 0.06056870954058105, 0.08859044010389584, 0.0999035611387745, 0.05604346112662959, 0.029588162706605685, 0.17502210590901318, 0.0999455993528681, 0.08868412336944635, 0.11261475983421759, 0.09243794869725361, 0.1135532161661694, 0.10510710917860308, 0.11214553166824168, 0.05443046725320517, 0.046453588431614755, 0.29120288469980127, 0.0903733090447659, 0.0937204686390165, 0.10710910701601885, 0.08033183026201414, 0.08033183026201414, 0.06526961208788648, 0.08535256965339003, 0.05522813330513472, 0.05188097371088413, 0.07401486561368746, 0.14802973122737492, 0.07401486561368746, 0.07401486561368746, 0.14802973122737492, 0.29605946245474984, 0.07401486561368746, 0.07401486561368746, 0.35545694972335656, 0.20988476998951802, 0.05996707713986229, 0.05996707713986229, 0.11993415427972458, 0.08995061570979343, 0.20988476998951802, 0.029983538569931144, 0.08995061570979343, 0.05996707713986229, 0.029983538569931144, 0.504933110655979, 0.1122073579235509, 0.05610367896177545, 0.1122073579235509, 0.05610367896177545, 0.05610367896177545, 0.05610367896177545, 0.05610367896177545, 0.05610367896177545, 0.05610367896177545, 0.2679927287428943, 0.2679927287428943, 0.20393905164900689, 0.11114678314870875, 0.08667409695082792, 0.11624525943993391, 0.16519063183569557, 0.07647714436837758, 0.05098476291225172, 0.08259531591784779, 0.049965067654006684, 0.05608323920347689, 0.20631456993275937, 0.10561341079891254, 0.0785960266410512, 0.10192922205011327, 0.10929759954771182, 0.14122723537063886, 0.05280670539945627, 0.0785960266410512, 0.06754346039465337, 0.05771895706452197, 0.11567047102494296, 0.09363800035352526, 0.14871917703206952, 0.21481658904632264, 0.07711364734996198, 0.09363800035352526, 0.06609741201425312, 0.09363800035352526, 0.044064941342835416, 0.04957305901068984, 0.3470391412248434, 0.12043900855071787, 0.08029267236714525, 0.04014633618357263, 0.08029267236714525, 0.3613170256521536, 0.12043900855071787, 0.04014633618357263, 0.08029267236714525, 0.04014633618357263, 0.04014633618357263, 0.14854919343747236, 0.14854919343747236, 0.14854919343747236, 0.14854919343747236, 0.14854919343747236, 0.14854919343747236, 0.14854919343747236, 0.47653695281269076, 0.13615341508934023, 0.06807670754467011, 0.06807670754467011, 0.06807670754467011, 0.06807670754467011, 0.06807670754467011, 0.06807670754467011, 0.06977935637811863, 0.046519570918745756, 0.16281849821561015, 0.06977935637811863, 0.046519570918745756, 0.06977935637811863, 0.2093380691343559, 0.11629892729686438, 0.13955871275623727, 0.046519570918745756, 0.06009948123236309, 0.06009948123236309, 0.12019896246472618, 0.12019896246472618, 0.06009948123236309, 0.06009948123236309, 0.24039792492945236, 0.06009948123236309, 0.12019896246472618, 0.06009948123236309, 0.14594245941334302, 0.14725331982723533, 0.12365783237717387, 0.10617969352527651, 0.08323963628216122, 0.1098937980313047, 0.11513723968687391, 0.06991255540758946, 0.053089846762638254, 0.04588011448623059, 0.3525694401824409, 0.21680866535032173, 0.07226955511677391, 0.16862896193913912, 0.26498836876150433, 0.048179703411182606, 0.048179703411182606, 0.07226955511677391, 0.024089851705591303, 0.07226955511677391, 0.024089851705591303, 0.20414036463276594, 0.20414036463276594, 0.4082807292655319, 0.10367882162640897, 0.051839410813204485, 0.11404670378904987, 0.08294305730112718, 0.051839410813204485, 0.06220729297584538, 0.19698976109017705, 0.13478246811433167, 0.13478246811433167, 0.06220729297584538, 0.20901968712912591, 0.14293686125404015, 0.11149657405355881, 0.09577643045331814, 0.10363650225343847, 0.0815118557049516, 0.07423401144558094, 0.06870284980845921, 0.06841173603808438, 0.04395817932659891, 0.18489841539112054, 0.11741049377336153, 0.12896664473530658, 0.1423717798511628, 0.07719508842579283, 0.07164813596405921, 0.09522268392642708, 0.0693369057716702, 0.06471444538689218, 0.04853583404016914, 0.14675168922341075, 0.11921216913778354, 0.11770315433857109, 0.11543963213975242, 0.09242715645176255, 0.07431897886121316, 0.09242715645176255, 0.10902631924309951, 0.07205545666239449, 0.060360591968497995, 0.2688116189205476, 0.2688116189205476, 0.17437374521136298, 0.11614192254852773, 0.09490821925710714, 0.11260297199995764, 0.10552507090281744, 0.07882208039997035, 0.11099435811424395, 0.05565804044569334, 0.08493481316568233, 0.06627489209140364, 0.1757952126675048, 0.15347686392884768, 0.11862355219998585, 0.11556624415359447, 0.09233070300101992, 0.09569374185205044, 0.09844531909380269, 0.0464710823051491, 0.040662197017005464, 0.0629805457556626, 0.25994834422359503, 0.08111750547912072, 0.08111750547912072, 0.16223501095824144, 0.08111750547912072, 0.16223501095824144, 0.2433525164373622, 0.08111750547912072, 0.14211124331577968, 0.10241536529461218, 0.09447618969037867, 0.1611652647659401, 0.12067546918434922, 0.09527010725080202, 0.061131652152597964, 0.07304041555894822, 0.09050660188826191, 0.057955981910904564, 0.18381303909273528, 0.12080707934410768, 0.11957435404467802, 0.10601437575095164, 0.07327867057720817, 0.0914956111132244, 0.07437442639892344, 0.09793317656580158, 0.07177200632234969, 0.06095141758291147, 0.20978014493050917, 0.20978014493050917, 0.20978014493050917, 0.20576779449166738, 0.20576779449166738, 0.20576779449166738, 0.1526648547965849, 0.16004271023285108, 0.11312711668941483, 0.12920705802486676, 0.08380487072476718, 0.08210228870101344, 0.0866425074310234, 0.07358937858224476, 0.06885998407181772, 0.049942406030109555, 0.10804749456190799, 0.11988831588376092, 0.13863628297669472, 0.12432862387945577, 0.12383525632437857, 0.1179148456634521, 0.06413778216003671, 0.07745870614712126, 0.07893880881235286, 0.046869917732334516, 0.2657555017218086, 0.24077966164087997, 0.17198547260062855, 0.06879418904025142, 0.10319128356037713, 0.06879418904025142, 0.20638256712075426, 0.03439709452012571, 0.06879418904025142, 0.03439709452012571, 0.03439709452012571, 0.14493417170694609, 0.14493417170694609, 0.28986834341389217, 0.15861857710384458, 0.21149143613845944, 0.2643642951730743, 0.05287285903461486, 0.10574571806922972, 0.05287285903461486, 0.05287285903461486, 0.05287285903461486, 0.2612935393912745, 0.2612935393912745, 0.15564611175342072, 0.13123103539994296, 0.10712114750088367, 0.11810793185994867, 0.08972540559903076, 0.07446598287810717, 0.09094615941670466, 0.09216691323437855, 0.08850465178135687, 0.05188203725114024, 0.03154074141142677, 0.18924444846856064, 0.15770370705713388, 0.09462222423428032, 0.06308148282285354, 0.2523259312914142, 0.06308148282285354, 0.09462222423428032, 0.03154074141142677, 0.03154074141142677, 0.13148487298288014, 0.16905197954941734, 0.05635065984980578, 0.13148487298288014, 0.05635065984980578, 0.2066190861159545, 0.05635065984980578, 0.07513421313307436, 0.05635065984980578, 0.09391776641634296, 0.20890657360984868, 0.06963552453661623, 0.06963552453661623, 0.13927104907323246, 0.06963552453661623, 0.06963552453661623, 0.06963552453661623, 0.06963552453661623, 0.20890657360984868, 0.06963552453661623, 0.15115837359619608, 0.15115837359619608, 0.4534751207885882, 0.26740835244422595, 0.21041658271065844, 0.21041658271065844, 0.21041658271065844, 0.1847837640679797, 0.1303770543603519, 0.11445313932397305, 0.10770759198217368, 0.09178367694579483, 0.05816652298010611, 0.1148954702972058, 0.0771867548291142, 0.07187878315032124, 0.048877572542218445, 0.13373912539130361, 0.12475904023313455, 0.11032676051464854, 0.13341840806422614, 0.09012156890876814, 0.059974140163486275, 0.10936460853341615, 0.09300802485246534, 0.07825502780690188, 0.067350638686268, 0.07361645442677346, 0.11778632708283752, 0.13250961796819222, 0.05889316354141876, 0.26501923593638443, 0.08833974531212814, 0.11778632708283752, 0.05889316354141876, 0.04416987265606407, 0.04416987265606407, 0.08155581544060429, 0.08155581544060429, 0.12233372316090643, 0.24466744632181286, 0.040777907720302146, 0.08155581544060429, 0.08155581544060429, 0.16311163088120859, 0.040777907720302146, 0.040777907720302146, 0.33599299902723323, 0.33599299902723323, 0.18761582982503205, 0.12301226578150687, 0.09616786245748499, 0.10826259362545089, 0.1174073903622056, 0.10590264608048194, 0.06696351158849415, 0.07315837389403766, 0.07020843946282647, 0.05073887221683257, 0.19309015581859856, 0.06637474106264325, 0.07240880843197446, 0.28963523372789785, 0.09051101053996807, 0.04827253895464964, 0.03620440421598723, 0.07240880843197446, 0.07240880843197446, 0.05430660632398085, 0.2092415385280889, 0.10462076926404446, 0.10462076926404446, 0.31386230779213337, 0.10462076926404446, 0.10462076926404446, 0.10462076926404446, 0.10462076926404446, 0.10869752967751228, 0.10869752967751228, 0.16304629451626843, 0.05434876483875614, 0.05434876483875614, 0.05434876483875614, 0.05434876483875614, 0.10869752967751228, 0.10869752967751228, 0.16304629451626843, 0.17100811211756523, 0.14851574920848507, 0.1205656805935252, 0.10055409300529948, 0.10783103394647246, 0.07095017417643663, 0.07161171426199782, 0.07723480498926785, 0.07442325962563283, 0.05738860242243243, 0.10962978803568553, 0.10962978803568553, 0.10962978803568553, 0.10962978803568553, 0.10962978803568553, 0.10962978803568553, 0.10962978803568553, 0.10962978803568553, 0.21925957607137106, 0.3663365871004467, 0.4109044763313611, 0.12085425774451798, 0.04834170309780719, 0.12085425774451798, 0.07251255464671079, 0.04834170309780719, 0.04834170309780719, 0.04834170309780719, 0.024170851548903596, 0.04834170309780719, 0.14201172620587146, 0.42603517861761436, 0.14201172620587146, 0.15175105655899102, 0.15175105655899102, 0.07587552827949551, 0.15175105655899102, 0.07587552827949551, 0.07587552827949551, 0.07587552827949551, 0.07587552827949551, 0.15175105655899102, 0.17962813883397233, 0.17962813883397233, 0.17962813883397233, 0.17962813883397233, 0.512358936992096, 0.10396754236553979, 0.1455545593117557, 0.06238052541932387, 0.27031561015040345, 0.12476105083864775, 0.06238052541932387, 0.041587016946215916, 0.06238052541932387, 0.041587016946215916, 0.06238052541932387, 0.1714296230834073, 0.1714296230834073, 0.1714296230834073, 0.1714296230834073, 0.2083052654439612, 0.1788043470218458, 0.08094764201190202, 0.07195345956613512, 0.1165646044971389, 0.0899418244576689, 0.06044090603555351, 0.06727648469433635, 0.0813074093097327, 0.044251377633173106, 0.12795997526069716, 0.10236798020855774, 0.10236798020855774, 0.12795997526069716, 0.10236798020855774, 0.0767759851564183, 0.0767759851564183, 0.10236798020855774, 0.1535519703128366, 0.05118399010427887, 0.4607199974396436, 0.1535733324798812, 0.1535733324798812, 0.15408874728462785, 0.0948238444828479, 0.10667682504320389, 0.10667682504320389, 0.05926490280177994, 0.11852980560355988, 0.03555894168106796, 0.22520663064676377, 0.04741192224142395, 0.04741192224142395, 0.14419613993703445, 0.15464513558464563, 0.25495549380171306, 0.05851437562662267, 0.07523276866280058, 0.056424576497100434, 0.06269397388566715, 0.09822055908754519, 0.0543347773675782, 0.03970618346092253, 0.542777767009277, 0.06262820388568581, 0.06262820388568581, 0.06262820388568581, 0.06262820388568581, 0.04175213592379054, 0.02087606796189527, 0.04175213592379054, 0.04175213592379054, 0.02087606796189527, 0.36517083091966357, 0.18258541545983178, 0.18258541545983178, 0.4587568077032932, 0.06553668681475618, 0.13107337362951235, 0.06553668681475618, 0.06553668681475618, 0.06553668681475618, 0.06553668681475618, 0.06553668681475618, 0.06553668681475618, 0.06553668681475618, 0.2993977188742505, 0.09526291055089788, 0.0680449361077842, 0.04082696166467052, 0.2721797444311368, 0.05443594888622736, 0.02721797444311368, 0.05443594888622736, 0.0680449361077842, 0.02721797444311368, 0.2671114525932886, 0.17958915710486376, 0.04489728927621594, 0.08979457855243188, 0.08979457855243188, 0.22448644638107967, 0.1346918678286478, 0.04489728927621594, 0.04489728927621594, 0.08979457855243188, 0.04489728927621594, 0.09063270133789943, 0.18126540267579885, 0.10573815156088266, 0.04531635066894971, 0.10573815156088266, 0.04531635066894971, 0.2416872035677318, 0.07552725111491619, 0.06042180089193295, 0.06042180089193295, 0.17392555734184564, 0.17392555734184564, 0.17392555734184564, 0.19653660812920923, 0.06551220270973641, 0.09826830406460461, 0.06551220270973641, 0.16378050677434103, 0.06551220270973641, 0.03275610135486821, 0.09826830406460461, 0.16378050677434103, 0.06551220270973641, 0.16899613883948109, 0.15796374995381762, 0.11232977774493698, 0.08926205552945884, 0.07171052775681244, 0.0957811944164418, 0.06318549998152705, 0.07622377775549295, 0.09277236108398813, 0.07171052775681244, 0.34388596890536743, 0.1299603720621315, 0.1299603720621315, 0.1299603720621315, 0.1299603720621315, 0.1299603720621315, 0.259920744124263, 0.1299603720621315, 0.1299603720621315, 0.13502682482524261, 0.13502682482524261, 0.27005364965048523, 0.13502682482524261, 0.13502682482524261, 0.13502682482524261, 0.1522282253811341, 0.1522282253811341, 0.1522282253811341, 0.1522282253811341, 0.1522282253811341, 0.1522282253811341, 0.1522282253811341, 0.1522282253811341, 0.35207892806004293, 0.13734964068883657, 0.16708512991013108, 0.10279983416504675, 0.11865876174973715, 0.09685273632078785, 0.07023239358934323, 0.09005605307020624, 0.11072929795739195, 0.06173653952611623, 0.04417844112878042, 0.09412099212532607, 0.047060496062663035, 0.2588327283446467, 0.07059074409399455, 0.07059074409399455, 0.09412099212532607, 0.047060496062663035, 0.07059074409399455, 0.09412099212532607, 0.16471173621932061, 0.14185107614480724, 0.35462769036201813, 0.14185107614480724, 0.07092553807240362, 0.14185107614480724, 0.07092553807240362, 0.07092553807240362, 0.07092553807240362, 0.1345668172837089, 0.1345668172837089, 0.33641704320927224, 0.06728340864185445, 0.06728340864185445, 0.06728340864185445, 0.06728340864185445, 0.06728340864185445, 0.15992175186689178, 0.11994131390016884, 0.15992175186689178, 0.07996087593344589, 0.07996087593344589, 0.07996087593344589, 0.039980437966722945, 0.1999021898336147, 0.039980437966722945, 0.07996087593344589, 0.3545487030219313, 0.20359091513895686, 0.20359091513895686, 0.20359091513895686, 0.3438880289638463, 0.17194401448192315, 0.17194401448192315, 0.17194401448192315, 0.1416965898914589, 0.10229432960078583, 0.10305206537560647, 0.10911395157417156, 0.12123772397130173, 0.08259319945544931, 0.08486640677991121, 0.08107772790580803, 0.09926338650150329, 0.07425810593242231, 0.35512650920952493, 0.34173494516316355, 0.20289090901866763, 0.11123499465816668, 0.11317138721507866, 0.12930799185601194, 0.09552869947432496, 0.09488323528868763, 0.04948558756552869, 0.07336776243410993, 0.05809177670735977, 0.07229198879138105, 0.2625103003719035, 0.11236483080435074, 0.09282312109924626, 0.12018151468639253, 0.07946961946742487, 0.06057930008582388, 0.07360710655589353, 0.0670932033208587, 0.05667095814480298, 0.074258496879397, 0.39134466710227817, 0.17890041924675573, 0.0335438286087667, 0.08945020962337787, 0.12299403823214457, 0.04472510481168893, 0.055906381014611164, 0.011181276202922233, 0.04472510481168893, 0.011181276202922233, 0.13801438476216457, 0.13067319408332603, 0.1277367178117906, 0.17178386188482184, 0.10130843136797187, 0.08809428814606249, 0.08075309746722395, 0.05138833475186978, 0.06460247797377916, 0.04698362034456666, 0.074557498266662, 0.149114996533324, 0.149114996533324, 0.074557498266662, 0.074557498266662, 0.037278749133331, 0.111836247399993, 0.074557498266662, 0.074557498266662, 0.18639374566665498, 0.21512080640855438, 0.1321456382223977, 0.08720075545489615, 0.09949337296395641, 0.1321456382223977, 0.08220687959184043, 0.04955461433339914, 0.09411535280374254, 0.06568867481404071, 0.04225587268739461, 0.3635842013751087, 0.20865142469699005, 0.16370798916284307, 0.1081896276206615, 0.13889758583630038, 0.09456424218723233, 0.06405964793328642, 0.08500613598766261, 0.06060246058450588, 0.04474007157245401, 0.031521414062410776, 0.13678305340571542, 0.1703671852175252, 0.11209538035200092, 0.15035015301181076, 0.08473876967085783, 0.0851835926087626, 0.0711716700647625, 0.05827180486552429, 0.06894755537523867, 0.062275211306667175, 0.35644086890513565, 0.17822043445256783, 0.060778542887622175, 0.15194635721905544, 0.12155708577524435, 0.060778542887622175, 0.060778542887622175, 0.060778542887622175, 0.09116781433143327, 0.09116781433143327, 0.09116781433143327, 0.18233562866286654, 0.44005150148447536, 0.14668383382815844, 0.14668383382815844, 0.14668383382815844, 0.05458511462184909, 0.30021813042017, 0.1364627865546227, 0.1364627865546227, 0.10917022924369818, 0.08187767193277364, 0.027292557310924545, 0.05458511462184909, 0.05458511462184909, 0.05458511462184909, 0.17542790061504498, 0.17542790061504498, 0.17542790061504498, 0.17542790061504498, 0.17542790061504498, 0.2534675437722987, 0.2534675437722987, 0.2534675437722987, 0.34027646504023806, 0.34027646504023806, 0.2080496044842482, 0.2080496044842482, 0.2080496044842482, 0.2080496044842482, 0.2064539055746371, 0.05161347639365928, 0.1290336909841482, 0.05161347639365928, 0.07742021459048892, 0.2064539055746371, 0.10322695278731855, 0.05161347639365928, 0.10322695278731855, 0.05161347639365928, 0.22368085752374758, 0.11184042876187379, 0.11184042876187379, 0.3355212862856214, 0.11184042876187379, 0.11184042876187379, 0.11184042876187379, 0.3530853733176358, 0.09499842520792406, 0.15833070867987345, 0.06333228347194937, 0.12666456694389874, 0.12666456694389874, 0.2216629921518228, 0.06333228347194937, 0.031666141735974686, 0.031666141735974686, 0.06333228347194937, 0.5547269608455823, 0.11094539216911645, 0.11094539216911645, 0.11094539216911645, 0.11094539216911645, 0.11094539216911645, 0.1844944715111685, 0.195046738331918, 0.0956511927945357, 0.11199018658150264, 0.09463000568285027, 0.08680090449326193, 0.053442125511537736, 0.08441813456599592, 0.05071895988037658, 0.042549462986893105, 0.601426544356476, 0.1036942317855993, 0.041477692714239726, 0.062216539071359586, 0.041477692714239726, 0.041477692714239726, 0.020738846357119863, 0.041477692714239726, 0.041477692714239726, 0.020738846357119863, 0.05046967966051449, 0.10093935932102897, 0.32805291779334417, 0.05046967966051449, 0.05046967966051449, 0.07570451949077173, 0.12617419915128622, 0.12617419915128622, 0.025234839830257243, 0.05046967966051449, 0.1984274146044843, 0.04960685365112107, 0.04960685365112107, 0.1488205609533632, 0.04960685365112107, 0.24803426825560537, 0.09921370730224215, 0.04960685365112107, 0.09921370730224215, 0.04960685365112107, 0.1159181603314471, 0.1159181603314471, 0.3477544809943413, 0.1159181603314471, 0.1159181603314471, 0.1159181603314471, 0.1159181603314471, 0.16818794414188515, 0.12013424581563224, 0.0480536983262529, 0.0961073966525058, 0.07208054748937935, 0.21624164246813804, 0.07208054748937935, 0.07208054748937935, 0.07208054748937935, 0.0480536983262529, 0.376881417320455, 0.07178693663246762, 0.08973367079058453, 0.053840202474350715, 0.12562713910681833, 0.053840202474350715, 0.07178693663246762, 0.08973367079058453, 0.017946734158116904, 0.03589346831623381, 0.19982519024053777, 0.19982519024053777, 0.29973778536080664, 0.09991259512026889, 0.09991259512026889, 0.09991259512026889, 0.5498949304677533, 0.07855641863825047, 0.07855641863825047, 0.07855641863825047, 0.07855641863825047, 0.07855641863825047, 0.07855641863825047, 0.44826052920801207, 0.1494201764026707, 0.1494201764026707, 0.5376084210109873, 0.14662047845754198, 0.048873492819180665, 0.048873492819180665, 0.048873492819180665, 0.048873492819180665, 0.048873492819180665, 0.048873492819180665, 0.24713228709065888, 0.1369648820020519, 0.10659440816681431, 0.099448414323229, 0.10063941329715988, 0.07681943381854216, 0.05895444920957887, 0.05597695177475165, 0.06848244100102595, 0.04883095793116633, 0.4309618619996347, 0.08619237239992694, 0.057461581599951295, 0.11492316319990259, 0.057461581599951295, 0.057461581599951295, 0.028730790799975647, 0.057461581599951295, 0.028730790799975647, 0.057461581599951295, 0.4551693664636961, 0.09103387329273922, 0.060689248861826146, 0.12137849772365229, 0.04551693664636961, 0.04551693664636961, 0.04551693664636961, 0.060689248861826146, 0.030344624430913073, 0.04551693664636961, 0.2563078323324391, 0.16834045284056492, 0.08733452064660886, 0.08923309718240471, 0.09049881487293528, 0.08353736757501717, 0.07214590836024211, 0.06455160221705873, 0.05316014300228366, 0.03480723648959049, 0.3206054272249258, 0.12503611661772107, 0.09618162816747775, 0.10579979098422551, 0.08335741107848071, 0.07053319398948368, 0.04809081408373887, 0.05450292262823739, 0.05129686835598813, 0.04167870553924036, 0.18056397932790288, 0.10944066770310391, 0.12886179338662837, 0.1275495551647686, 0.07925918860032946, 0.08818240850897582, 0.08345835091028068, 0.06561191109298796, 0.0698110734029392, 0.06718659695921968, 0.2084141178851289, 0.11909378164864509, 0.15469246638057704, 0.08802583860986811, 0.08155335047678958, 0.07766985759694245, 0.07637535997032674, 0.0783171064102503, 0.06472488133078537, 0.05113265625132044, 0.0806275799208709, 0.09406550990768271, 0.13437929986811817, 0.14781722985492998, 0.05375171994724726, 0.05375171994724726, 0.20156894980217724, 0.0806275799208709, 0.12094136988130634, 0.02687585997362363, 0.1628139718880348, 0.15522457829958639, 0.14205053659888348, 0.07782708330795682, 0.11971194414986551, 0.06415185523820544, 0.09465262569744151, 0.06837614034875691, 0.06107313897119335, 0.05405652980451463, 0.16532438606065797, 0.11701973005116531, 0.10341278469637864, 0.12042146638986198, 0.08844514480611332, 0.07619889398680532, 0.09048618660933132, 0.07687924125454465, 0.07823993579002332, 0.08436306119967732, 0.36827372517871443, 0.10209131613097878, 0.510456580654894, 0.10209131613097878, 0.10209131613097878, 0.10209131613097878, 0.15997004729163325, 0.11102943921611075, 0.10445532768357788, 0.08911573410766785, 0.11979492125948792, 0.08619390675987545, 0.12052537809643601, 0.10445532768357788, 0.06354974481448444, 0.04090558286909344, 0.19223788861272903, 0.11352209578244722, 0.11191565103080882, 0.09076412846756983, 0.10816727994365254, 0.09718990747412344, 0.08460609025295594, 0.0827319047093778, 0.07978675599804073, 0.039090155623201166, 0.2561944928117452, 0.15189927065999012, 0.15189927065999012, 0.15189927065999012, 0.15189927065999012, 0.15189927065999012, 0.23390487806881555, 0.13358898556371843, 0.1037921858097292, 0.07846490601883833, 0.09981927917586396, 0.09932266584663081, 0.05711053286181272, 0.08144458599423726, 0.07548522604343942, 0.037245999692486556, 0.11951880277549853, 0.14722035706365147, 0.11616780830515747, 0.11035941788989959, 0.12309319687719569, 0.0902534510678531, 0.07573247502970842, 0.10924241973311923, 0.06031790046613945, 0.04803092074155549, 0.21109587215575917, 0.21109587215575917, 0.21109587215575917, 0.21109587215575917, 0.42137948114929147, 0.08427589622985829, 0.08427589622985829, 0.08427589622985829, 0.08427589622985829, 0.08427589622985829, 0.08427589622985829, 0.08427589622985829, 0.3452585068358523, 0.17262925341792615, 0.17262925341792615, 0.17262925341792615, 0.06190327702252935, 0.14856786485407045, 0.284755074303635, 0.08666458783154109, 0.04952262161802348, 0.07428393242703522, 0.11142589864055283, 0.1238065540450587, 0.02476131080901174, 0.03714196621351761, 0.1039323621050519, 0.06928824140336794, 0.1039323621050519, 0.06928824140336794, 0.06928824140336794, 0.24250884491178776, 0.1039323621050519, 0.13857648280673587, 0.06928824140336794, 0.03464412070168397, 0.13689310070531044, 0.11407758392109202, 0.10647241165968589, 0.15970861748952883, 0.08365689487546749, 0.053236205829842946, 0.07605172261406135, 0.11407758392109202, 0.1292879284439043, 0.03802586130703067, 0.13150123488483997, 0.17158850591811028, 0.10565056477927315, 0.14086741970569752, 0.08916607949456386, 0.08504495817338653, 0.08654354774472374, 0.06893512028151155, 0.05357457717530517, 0.06706188331734005, 0.19884913780898236, 0.16708590124025077, 0.0977330355960972, 0.08532845800120793, 0.0849525617104537, 0.0659697990273656, 0.10449916882967315, 0.08250923582055128, 0.07423951742395844, 0.039093214238438874, 0.2062637952248583, 0.0687545984082861, 0.0687545984082861, 0.17188649602071526, 0.0687545984082861, 0.24064109442900136, 0.0687545984082861, 0.0687545984082861, 0.0687545984082861, 0.03437729920414305, 0.10433831221697702, 0.11647067410267202, 0.10919125697125502, 0.16500012164545202, 0.09705889508556002, 0.10191183983983802, 0.06794122655989202, 0.06308828180561402, 0.12617656361122803, 0.04852944754278001, 0.15691824854924, 0.12961381536352318, 0.1730377332010487, 0.11809989775508838, 0.07928154696093677, 0.08947958827126473, 0.05625371174406717, 0.07270216547040259, 0.06612278397986843, 0.05888546434028084, 0.530722854810554, 0.128372845409832, 0.1026982763278656, 0.0513491381639328, 0.1540474144917984, 0.0770237072458992, 0.0513491381639328, 0.1026982763278656, 0.0513491381639328, 0.1026982763278656, 0.1797219835737648, 0.4149695921338757, 0.10374239803346892, 0.10374239803346892, 0.05187119901673446, 0.05187119901673446, 0.05187119901673446, 0.05187119901673446, 0.10374239803346892, 0.05187119901673446, 0.05187119901673446, 0.21421078531911483, 0.1836092445592413, 0.09180462227962065, 0.03060154075987355, 0.09180462227962065, 0.0612030815197471, 0.03060154075987355, 0.1836092445592413, 0.0612030815197471, 0.03060154075987355, 0.34420295227614267, 0.2689484362421306, 0.2689484362421306, 0.39126921305690915, 0.11738076391707274, 0.07825384261138182, 0.03912692130569091, 0.07825384261138182, 0.07825384261138182, 0.03912692130569091, 0.03912692130569091, 0.03912692130569091, 0.07825384261138182, 0.25763715774478113, 0.3047441308177415, 0.1805965906486894, 0.1805965906486894, 0.1805965906486894, 0.1805965906486894, 0.1805965906486894, 0.06506080294737944, 0.3253040147368972, 0.13012160589475888, 0.06506080294737944, 0.06506080294737944, 0.13012160589475888, 0.06506080294737944, 0.06506080294737944, 0.06506080294737944, 0.20653014500298597, 0.20653014500298597, 0.20653014500298597, 0.20653014500298597, 0.15825894042571798, 0.06330357617028719, 0.06330357617028719, 0.031651788085143594, 0.06330357617028719, 0.09495536425543079, 0.25321430468114875, 0.15825894042571798, 0.06330357617028719, 0.031651788085143594, 0.18418261513466383, 0.13698582000640622, 0.11396299311457325, 0.08748674218896532, 0.08173103546600707, 0.09324244891192357, 0.06216163260794905, 0.1415903853847728, 0.048347936472849254, 0.051801360506624206, 0.26605994959086976, 0.26605994959086976, 0.29763596825505395, 0.12025339475707748, 0.15146285351367972, 0.0837541972281698, 0.08622274198857902, 0.06506378689935717, 0.04249423480418721, 0.04901824595669727, 0.04619705194480103, 0.05765815261812953, 0.3656671167708004, 0.1828335583854002, 0.1828335583854002, 0.09166947366649368, 0.09166947366649368, 0.09166947366649368, 0.09166947366649368, 0.09166947366649368, 0.04583473683324684, 0.2291736841662342, 0.09166947366649368, 0.04583473683324684, 0.09166947366649368, 0.10078889551651915, 0.30236668654955745, 0.16126223282643065, 0.04031555820660766, 0.04031555820660766, 0.060473337309911494, 0.10078889551651915, 0.08063111641321533, 0.08063111641321533, 0.04031555820660766, 0.14472888029916917, 0.04824296009972306, 0.2653362805484768, 0.09648592019944612, 0.07236444014958458, 0.12060740024930765, 0.07236444014958458, 0.09648592019944612, 0.04824296009972306, 0.02412148004986153, 0.166457604201984, 0.129528268170418, 0.1008666939369638, 0.08984301153948142, 0.15157563296538276, 0.07055156734388725, 0.0777169609022508, 0.08488235446061435, 0.06503972614514605, 0.0633861737855237, 0.13535243678612072, 0.0451174789287069, 0.0902349578574138, 0.2255873946435345, 0.13535243678612072, 0.0902349578574138, 0.13535243678612072, 0.0451174789287069, 0.0451174789287069, 0.0451174789287069, 0.24103485278281017, 0.12195951525420821, 0.10692058854211835, 0.09229368721940082, 0.10980476626772463, 0.07725476050731095, 0.07210444314015689, 0.07292849391890154, 0.051091148282168306, 0.054387351397146906, 0.2036350210411553, 0.13777858870444123, 0.13994491871551737, 0.0883862644519057, 0.10506700553719182, 0.0738718533776957, 0.055241415282441066, 0.07907104540427838, 0.055458048283548673, 0.06152377231456181, 0.11284268306766462, 0.12412695137443108, 0.12412695137443108, 0.0902741464541317, 0.2144010978285628, 0.07898987814736523, 0.07898987814736523, 0.07898987814736523, 0.05642134153383231, 0.04513707322706585, 0.11800271748558855, 0.35400815245676565, 0.0786684783237257, 0.0786684783237257, 0.0786684783237257, 0.0786684783237257, 0.03933423916186285, 0.03933423916186285, 0.03933423916186285, 0.0786684783237257, 0.07657402845635887, 0.07657402845635887, 0.07657402845635887, 0.07657402845635887, 0.15314805691271774, 0.07657402845635887, 0.07657402845635887, 0.07657402845635887, 0.07657402845635887, 0.15314805691271774, 0.20183811725946715, 0.12110287035568029, 0.08073524690378686, 0.08073524690378686, 0.24220574071136058, 0.08073524690378686, 0.04036762345189343, 0.04036762345189343, 0.08073524690378686, 0.04036762345189343, 0.5447536844569968, 0.1815845614856656, 0.1110180511632747, 0.1110180511632747, 0.1110180511632747, 0.05550902558163735, 0.1110180511632747, 0.1110180511632747, 0.05550902558163735, 0.1110180511632747, 0.05550902558163735, 0.16652707674491207, 0.2663626005614516, 0.08323831267545362, 0.0665906501403629, 0.1331813002807258, 0.09988597521054435, 0.03329532507018145, 0.03329532507018145, 0.18312428788599797, 0.0665906501403629, 0.03329532507018145, 0.08547420986913194, 0.17094841973826388, 0.08547420986913194, 0.17094841973826388, 0.25642262960739587, 0.08547420986913194, 0.08547420986913194, 0.1182253287503347, 0.2955633218758368, 0.10344716265654287, 0.0738908304689592, 0.05911266437516735, 0.08866899656275103, 0.051723581328271435, 0.08866899656275103, 0.08866899656275103, 0.0369454152344796, 0.43464633187989576, 0.028976422125326386, 0.11590568850130555, 0.08692926637597916, 0.05795284425065277, 0.05795284425065277, 0.05795284425065277, 0.05795284425065277, 0.05795284425065277, 0.05795284425065277, 0.17380731028684906, 0.17380731028684906, 0.34761462057369813, 0.17380731028684906, 0.17380731028684906, 0.4119962457560784, 0.1029990614390196, 0.1029990614390196, 0.1029990614390196, 0.1029990614390196, 0.1029990614390196, 0.17482229413970982, 0.11784799010161355, 0.11337092185020421, 0.09481275184033001, 0.09748455063552591, 0.08708619856773649, 0.08491987522028036, 0.09055231592366629, 0.07221077824853772, 0.06686718065814592, 0.15117536884931315, 0.10345263588628845, 0.12363070418609083, 0.11834597201233306, 0.12106840979881434, 0.07638840142068047, 0.06677979746839362, 0.10585478687436016, 0.07526739762624701, 0.05781176711292589, 0.26719249144663765, 0.26719249144663765, 0.1576352392717241, 0.08983513635915459, 0.10509015951448274, 0.16526275084938818, 0.06525759905334814, 0.11187016980573969, 0.06949510548538375, 0.06356259648053392, 0.11526017495136816, 0.055935084902869844, 0.11797967528937445, 0.11797967528937445, 0.2359593505787489, 0.11797967528937445, 0.2359593505787489, 0.11797967528937445, 0.16744822781754665, 0.16744822781754665, 0.3348964556350933, 0.16744822781754665, 0.2568231512264936, 0.1437718119306208, 0.09891992188388866, 0.09277582735693907, 0.10506401641083828, 0.07680118158687009, 0.07864440994495496, 0.050995984573681735, 0.0405510238778674, 0.055911260195241425, 0.45365343083130605, 0.15121781027710202, 0.15121781027710202, 0.3482743001528558, 0.10849879491790311, 0.07233252994526873, 0.07233252994526873, 0.036166264972634365, 0.25316385480844056, 0.07233252994526873, 0.07233252994526873, 0.10849879491790311, 0.07233252994526873, 0.07233252994526873, 0.10965764639086945, 0.0685360289942934, 0.09595044059201076, 0.04112161739657604, 0.2741441159771736, 0.0685360289942934, 0.09595044059201076, 0.09595044059201076, 0.09595044059201076, 0.05482882319543472, 0.6147651333035502, 0.07684564166294378, 0.03842282083147189, 0.03842282083147189, 0.07684564166294378, 0.03842282083147189, 0.03842282083147189, 0.03842282083147189, 0.10794006784162279, 0.06476404070497367, 0.08635205427329823, 0.043176027136649114, 0.23746814925157014, 0.06476404070497367, 0.10794006784162279, 0.12952808140994734, 0.08635205427329823, 0.043176027136649114, 0.1463770476282004, 0.1463770476282004, 0.1463770476282004, 0.2927540952564008, 0.1463770476282004, 0.17092075071229512, 0.1182896658893018, 0.14260765887682347, 0.1082150687944714, 0.10456736984634316, 0.1122101676424214, 0.05541028402156722, 0.06930628001443674, 0.07191177926309977, 0.0465515865761129, 0.1753285170880247, 0.1416011992970459, 0.12232844627362945, 0.12527289465220695, 0.06531321857935576, 0.10974762138334371, 0.06451018720338007, 0.059691998947525964, 0.0786970748456172, 0.05728290481959891, 0.15819025899795572, 0.11742728445711134, 0.13032932685765383, 0.13743479948403955, 0.09293210250825532, 0.06955883728988124, 0.08676156049060456, 0.07329855972482109, 0.07872115725548387, 0.0555348781588568, 0.10769341002095768, 0.2249107270505715, 0.13553252281549097, 0.11428688410387347, 0.09890211124373666, 0.08424994661503493, 0.06300430790341742, 0.06300430790341742, 0.04175866919179992, 0.06520213259772269, 0.12940796703270133, 0.20236636819928602, 0.13153813932953592, 0.1054435286933122, 0.08201163342813171, 0.11343167480644191, 0.04792887667877827, 0.06337262583082905, 0.05751465201453393, 0.06710042735028958, 0.160164455300294, 0.14978342579008977, 0.07948902596384962, 0.13050437098542475, 0.09669187486647378, 0.08898025294460778, 0.09283606390554078, 0.07889582427755223, 0.07029439982624014, 0.05279495008046728, 0.12857669554945877, 0.06428834777472939, 0.06428834777472939, 0.06428834777472939, 0.25715339109891755, 0.06428834777472939, 0.06428834777472939, 0.06428834777472939, 0.06428834777472939, 0.06428834777472939, 0.27343437665641673, 0.35360136718221746, 0.17056883778179638, 0.06822753511271855, 0.30702390800723345, 0.06822753511271855, 0.034113767556359274, 0.06822753511271855, 0.1364550702254371, 0.06822753511271855, 0.06822753511271855, 0.034113767556359274, 0.3421677351164808, 0.14986845067587504, 0.10558913570345742, 0.09082936404598488, 0.11126597095633148, 0.15611296945403652, 0.08742326289426045, 0.11240133800690628, 0.06641897245862644, 0.06641897245862644, 0.05279456785172871, 0.19273262705235003, 0.12932899647064236, 0.09870792488288582, 0.09690668537772366, 0.07745329872197244, 0.07709305082094002, 0.08357751303952375, 0.12212403844999377, 0.074571315513713, 0.04791297083731319, 0.1662205812175087, 0.13077069422799265, 0.14179954795806432, 0.0926948896836976, 0.109500762034283, 0.08009048542075854, 0.0664357141359079, 0.06984940695712057, 0.07510124206667851, 0.0674860811578195, 0.15995715788281847, 0.1091944981894126, 0.11284648809541302, 0.11029009516121273, 0.1004297224150116, 0.08180457389440944, 0.09860372746201138, 0.11211609011421295, 0.06390982335500738, 0.05076265969340586, 0.21042887971446467, 0.21042887971446467, 0.21042887971446467, 0.21042887971446467, 0.17315262068930007, 0.17315262068930007, 0.34630524137860014, 0.40279949612451654, 0.08479989392095086, 0.04239994696047543, 0.08479989392095086, 0.074199907180832, 0.08479989392095086, 0.074199907180832, 0.05299993370059428, 0.05299993370059428, 0.021199973480237715, 0.23845092836405082, 0.09538037134562033, 0.06676625994193423, 0.21937485409492677, 0.10491840848018236, 0.06676625994193423, 0.047690185672810165, 0.06676625994193423, 0.06676625994193423, 0.0286141114036861, 0.2058543672986318, 0.13723624486575453, 0.1029271836493159, 0.1029271836493159, 0.03430906121643863, 0.03430906121643863, 0.06861812243287727, 0.06861812243287727, 0.06861812243287727, 0.17154530608219315, 0.14890782822494888, 0.07445391411247444, 0.07445391411247444, 0.29781565644989777, 0.07445391411247444, 0.07445391411247444, 0.07445391411247444, 0.07445391411247444, 0.07445391411247444, 0.07445391411247444, 0.18279883658319557, 0.060932945527731854, 0.060932945527731854, 0.14217687289804098, 0.060932945527731854, 0.24373178211092741, 0.060932945527731854, 0.08124392737030914, 0.060932945527731854, 0.04062196368515457, 0.20409375435830462, 0.10637701341023374, 0.08819044123969175, 0.0998818090636116, 0.12918239756059594, 0.09295359109388132, 0.07289062655653737, 0.06899350394856409, 0.07779811428509632, 0.05961154211455432, 0.2428541823330828, 0.13903865400749016, 0.10628732661905914, 0.06859240264369515, 0.11493862064619187, 0.0809513941110276, 0.0809513941110276, 0.04511031885576347, 0.07044625136379501, 0.05067186501606308, 0.13529791785808004, 0.06764895892904002, 0.13529791785808004, 0.06764895892904002, 0.2705958357161601, 0.06764895892904002, 0.06764895892904002, 0.13529791785808004, 0.06764895892904002, 0.1072700620500602, 0.1072700620500602, 0.1072700620500602, 0.1072700620500602, 0.2145401241001204, 0.1072700620500602, 0.1072700620500602, 0.1072700620500602, 0.1072700620500602, 0.3442736869168374, 0.19989446167045583, 0.19989446167045583, 0.3331574361174264, 0.06663148722348528, 0.06663148722348528, 0.06663148722348528, 0.06663148722348528, 0.9350180694235433, 0.25807949067314395, 0.19929964259629865, 0.14351481440546865, 0.11061606957497916, 0.09416669715973441, 0.08701479610962799, 0.07783652309532477, 0.07581015113112796, 0.08284285383039926, 0.07449896927194177, 0.054592844682478936, 0.1650351177094247, 0.11341919612129187, 0.11477750984729537, 0.10730678435427614, 0.08081966669720798, 0.05908664708115205, 0.10526931376527089, 0.13922715691535828, 0.0584074902181503, 0.05704917649214681, 0.1616633352650114, 0.12330254384619513, 0.13015268517098375, 0.10110808595388, 0.09973805768892229, 0.0827497072034465, 0.09782001811798147, 0.07918763371455642, 0.06329530584104683, 0.06110326061711447, 0.14637152077036972, 0.10497022202240322, 0.1339837305938128, 0.10594820545739456, 0.1352877085071346, 0.09062646497586364, 0.09845033245579432, 0.0720447797110283, 0.05965698953447139, 0.05248511101120161, 0.46075810385482635, 0.11518952596370659, 0.11518952596370659, 0.11518952596370659, 0.11518952596370659, 0.11518952596370659, 0.139975642345745, 0.0870829674718831, 0.11279874310116403, 0.14698903569918526, 0.11338319254728405, 0.11981213645460427, 0.08649851802576308, 0.0981875069481635, 0.04938597819714176, 0.04587928152042164, 0.15363365097134238, 0.12157097598601875, 0.11622719682179815, 0.08282857704541936, 0.17634471241927993, 0.06412534997064726, 0.08149263225436422, 0.07481290829908846, 0.07347696350803332, 0.057445626015371495, 0.14863831693472188, 0.14972857353815505, 0.10938907921112784, 0.13846258863601232, 0.10757198487207256, 0.07413744903345541, 0.08067898865405443, 0.08140582638967653, 0.0668690716772343, 0.0428834264017046, 0.16199567025822406, 0.11729487824436532, 0.1283806746638023, 0.11336120854714575, 0.08010381928883485, 0.08868637135549573, 0.07545493691939355, 0.08618312700271964, 0.0736669052388392, 0.07438211791106093, 0.19702579527666378, 0.13424296413928127, 0.129393883685581, 0.0643141197017089, 0.09391903194535269, 0.08626258912372067, 0.08549694484155747, 0.07120491824117771, 0.06635583778747743, 0.07197056252334091, 0.1727845946843125, 0.08639229734215625, 0.25917689202646876, 0.03702527028949554, 0.16044283792114733, 0.03702527028949554, 0.13575932439481697, 0.03702527028949554, 0.03702527028949554, 0.03702527028949554, 0.4363712043013942, 0.06981939268822307, 0.10472908903233462, 0.08727424086027884, 0.05236454451616731, 0.05236454451616731, 0.05236454451616731, 0.05236454451616731, 0.05236454451616731, 0.034909696344111535, 0.20742865856354883, 0.13828577237569922, 0.06914288618784961, 0.06914288618784961, 0.06914288618784961, 0.06914288618784961, 0.06914288618784961, 0.13828577237569922, 0.1040896559087067, 0.07806724193153003, 0.07806724193153003, 0.15613448386306006, 0.13011206988588336, 0.07806724193153003, 0.07806724193153003, 0.18215689784023673, 0.05204482795435335, 0.05204482795435335, 0.13777389551966931, 0.06888694775983466, 0.06888694775983466, 0.13777389551966931, 0.27554779103933863, 0.06888694775983466, 0.13777389551966931, 0.06888694775983466, 0.06888694775983466, 0.06888694775983466, 0.16766468591804548, 0.14080128320189456, 0.11023258355937797, 0.08915870729067335, 0.1375591483913246, 0.07966388391686138, 0.06136898034293101, 0.09332716633283471, 0.06901115525356015, 0.05141099485332334, 0.06759597004665673, 0.06759597004665673, 0.06759597004665673, 0.06759597004665673, 0.06759597004665673, 0.13519194009331345, 0.06759597004665673, 0.33797985023328364, 0.06759597004665673, 0.13318360394350143, 0.13318360394350143, 0.13318360394350143, 0.13318360394350143, 0.13318360394350143, 0.26636720788700285, 0.13318360394350143, 0.3477148938482908, 0.0579060888664092, 0.289530444332046, 0.0579060888664092, 0.08685913329961381, 0.08685913329961381, 0.0579060888664092, 0.08685913329961381, 0.144765222166023, 0.0579060888664092, 0.0289530444332046, 0.26544060773043043, 0.26544060773043043, 0.12319775718849318, 0.06159887859424659, 0.06159887859424659, 0.12319775718849318, 0.24639551437698637, 0.06159887859424659, 0.12319775718849318, 0.06159887859424659, 0.06159887859424659, 0.06159887859424659, 0.05206249042832354, 0.10412498085664708, 0.10412498085664708, 0.1561874712849706, 0.10412498085664708, 0.10412498085664708, 0.10412498085664708, 0.05206249042832354, 0.05206249042832354, 0.20824996171329416, 0.13144442680449786, 0.13144442680449786, 0.2628888536089957, 0.13144442680449786, 0.13144442680449786, 0.13144442680449786, 0.13144442680449786, 0.05693720694663934, 0.11387441389327868, 0.11387441389327868, 0.11387441389327868, 0.11387441389327868, 0.11387441389327868, 0.05693720694663934, 0.05693720694663934, 0.05693720694663934, 0.17081162083991802, 0.15733824492912807, 0.15733824492912807, 0.07866912246456403, 0.07866912246456403, 0.11800368369684605, 0.03933456123228202, 0.2753419286259741, 0.07866912246456403, 0.03933456123228202, 0.03933456123228202, 0.18810406770373828, 0.09405203385186914, 0.09405203385186914, 0.09405203385186914, 0.09405203385186914, 0.09405203385186914, 0.28215610155560744, 0.09405203385186914, 0.09405203385186914, 0.17177092413831585, 0.17177092413831585, 0.17177092413831585, 0.09763038955354415, 0.048815194776772076, 0.09763038955354415, 0.09763038955354415, 0.048815194776772076, 0.14644558433031624, 0.048815194776772076, 0.2928911686606325, 0.09763038955354415, 0.17206697477311317, 0.10803373468083197, 0.11733465223613537, 0.10696055188598926, 0.07333415764758461, 0.0740496128441464, 0.12735102498800058, 0.08513916839085432, 0.08513916839085432, 0.05043959135760698, 0.17492689898623995, 0.14826121316516677, 0.0927965866573346, 0.12266215477693654, 0.0826636260453268, 0.05919782252278242, 0.0826636260453268, 0.08799676320954143, 0.061864391104889734, 0.08746344949311997, 0.07816550623154472, 0.17098704488150407, 0.18564307729991872, 0.06350947381313009, 0.214955142136748, 0.04885344139471545, 0.06350947381313009, 0.04396809725524391, 0.06350947381313009, 0.06839481795260163, 0.1690134657342793, 0.3380269314685586, 0.05633782191142644, 0.05633782191142644, 0.05633782191142644, 0.11267564382285288, 0.05633782191142644, 0.05633782191142644, 0.05633782191142644, 0.05633782191142644, 0.5148854445033653, 0.07477724684007887, 0.16023695751445474, 0.2456966681888306, 0.08545971067437585, 0.08545971067437585, 0.09614217450867284, 0.08545971067437585, 0.04272985533718793, 0.06409478300578189, 0.06409478300578189, 0.12761398329116547, 0.3828419498734964, 0.12761398329116547, 0.12761398329116547, 0.12761398329116547, 0.12761398329116547, 0.14805249157365247, 0.12632001574632734, 0.10956789896276421, 0.10187098044058657, 0.13673349374692062, 0.09915442096217092, 0.06519742748197539, 0.08783542313543907, 0.06429190765583685, 0.061122588264351935, 0.14376872632159574, 0.1519322502136737, 0.09773552215237817, 0.0784605351849718, 0.13175020503603646, 0.07301818592358647, 0.1054455169393407, 0.09229317289099284, 0.06009260642779632, 0.06553495568918165, 0.30576806234172277, 0.09609853387882715, 0.0698898428209652, 0.11357099458406845, 0.0524173821157239, 0.0524173821157239, 0.0349449214104826, 0.061153612468344554, 0.14851591599455105, 0.061153612468344554, 0.18313990474066605, 0.18819298190353306, 0.1220932698271107, 0.12578065045947312, 0.07210877681064257, 0.08371719731993162, 0.07757156293266095, 0.04984792336341768, 0.04916507509816539, 0.04834565717986263, 0.19348831585302095, 0.11595576375588845, 0.06724062040282289, 0.11321124863740588, 0.16398477832933336, 0.06861287796206417, 0.08233545355447701, 0.07821868087675315, 0.06655449162320225, 0.050087400912306845, 0.09133241641049614, 0.1369986246157442, 0.1369986246157442, 0.2739972492314884, 0.04566620820524807, 0.04566620820524807, 0.09133241641049614, 0.04566620820524807, 0.04566620820524807, 0.04566620820524807, 0.14738479930886983, 0.14738479930886983, 0.14738479930886983, 0.29476959861773966, 0.14738479930886983, 0.14738479930886983, 0.20666702997389752, 0.10333351498694876, 0.20666702997389752, 0.10333351498694876, 0.20666702997389752, 0.10333351498694876, 0.10333351498694876, 0.10333351498694876, 0.11337944157001954, 0.45351776628007817, 0.11337944157001954, 0.11337944157001954, 0.11337944157001954, 0.11337944157001954, 0.07892583688597585, 0.1578516737719517, 0.07892583688597585, 0.07892583688597585, 0.1578516737719517, 0.3157033475439034, 0.07892583688597585, 0.07892583688597585, 0.2649079485017033, 0.2495056051208466, 0.046782300960158736, 0.09356460192031747, 0.10915870224037039, 0.21831740448074077, 0.046782300960158736, 0.046782300960158736, 0.046782300960158736, 0.07797050160026456, 0.046782300960158736, 0.23832054351946058, 0.13876239438687207, 0.07092873319031565, 0.11503350910138466, 0.09878438113414871, 0.10497452512166716, 0.059580135879865145, 0.07092873319031565, 0.0541637598907865, 0.048747383901707846, 0.21403644647050835, 0.12249470474927555, 0.10339606798729173, 0.10142034694294857, 0.14752050464428884, 0.05005159979002657, 0.06519879446332408, 0.07376025232214442, 0.05993020501174234, 0.06256449973753321, 0.16406573650083703, 0.04101643412520926, 0.08203286825041851, 0.08203286825041851, 0.08203286825041851, 0.04101643412520926, 0.04101643412520926, 0.20508217062604628, 0.08203286825041851, 0.08203286825041851, 0.13111612515340218, 0.13111612515340218, 0.06555806257670109, 0.13111612515340218, 0.13111612515340218, 0.19667418773010328, 0.06555806257670109, 0.06555806257670109, 0.06555806257670109, 0.06555806257670109, 0.11829133992877283, 0.13636908623017996, 0.11436139508064083, 0.14069202556312516, 0.10453653296031087, 0.0951046653247941, 0.07388296314488137, 0.08017087490189255, 0.08331483078039814, 0.05344724993459503, 0.20900750493024858, 0.20900750493024858, 0.20900750493024858, 0.26112333548994504, 0.26112333548994504, 0.07975494542368193, 0.15950989084736386, 0.07975494542368193, 0.039877472711840965, 0.039877472711840965, 0.1196324181355229, 0.039877472711840965, 0.07975494542368193, 0.2791423089828868, 0.039877472711840965, 0.12663008226392594, 0.18221198860114535, 0.14499627740144191, 0.10294735695502376, 0.08796440829020809, 0.09569754308495167, 0.06718160852933475, 0.07104817592670654, 0.05074869709050467, 0.07056485500203506, 0.1938122266436445, 0.18467971334629998, 0.09031040927374011, 0.10603862661916676, 0.08980304742388763, 0.09842819887137966, 0.07508955377816592, 0.06443495493126401, 0.046169928336574996, 0.05124354683509972, 0.07347232825207657, 0.22041698475622973, 0.2938893130083063, 0.14694465650415314, 0.07347232825207657, 0.07347232825207657, 0.07347232825207657, 0.07347232825207657, 0.07347232825207657, 0.35366133790086735, 0.35366133790086735, 0.15387344499057704, 0.13100036532981557, 0.1725877828948364, 0.12060351093856037, 0.07485735161703747, 0.09773043127779892, 0.08317483513004163, 0.0613414409084057, 0.049904901078024984, 0.056143013712778106, 0.18681021534630893, 0.15615418000742748, 0.09436310877749451, 0.10729612368608514, 0.08430409718192404, 0.09484210932966454, 0.07137108227333341, 0.07232908337767346, 0.046463053560492226, 0.08622009939060413, 0.08780662551724791, 0.11707550068966388, 0.3219576268965757, 0.05853775034483194, 0.05853775034483194, 0.05853775034483194, 0.08780662551724791, 0.14634437586207985, 0.05853775034483194, 0.02926887517241597, 0.15620732106267457, 0.148856388306784, 0.15436958787370192, 0.12195865708636625, 0.08804412641714385, 0.05847332874003861, 0.06916559456678853, 0.08988185960611648, 0.059809861968382345, 0.05329426248020662, 0.13416918874831907, 0.13416918874831907, 0.06708459437415953, 0.13416918874831907, 0.06708459437415953, 0.2012537831224786, 0.06708459437415953, 0.06708459437415953, 0.06708459437415953, 0.06708459437415953, 0.1611614693670781, 0.1007259183544238, 0.1007259183544238, 0.2417422040506171, 0.1007259183544238, 0.1007259183544238, 0.04029036734176952, 0.08058073468353905, 0.04029036734176952, 0.06043555101265428, 0.10705545694044849, 0.05888050131724667, 0.06423327416426909, 0.19805259533982972, 0.10170268409342607, 0.03746940992915697, 0.08029159270533637, 0.1712887311047176, 0.16058318541067274, 0.016058318541067273, 0.343440819404165, 0.5140578692471788, 0.5157887861928917, 0.13931498133242923, 0.10094079485762837, 0.17685494636212573, 0.0775825943947061, 0.13180698832648993, 0.0625666083828275, 0.12346477387544627, 0.05672705826709694, 0.06423505127303623, 0.0675719370534537, 0.26130440638452423, 0.12181484313809263, 0.0761342769613079, 0.06090742156904631, 0.2081003570275749, 0.11166360620991823, 0.06090742156904631, 0.08628551388948227, 0.08120989542539508, 0.131966080066267, 0.0659830400331335, 0.16232467776223622, 0.13913543808191675, 0.046378479360638915, 0.09275695872127783, 0.06956771904095838, 0.20870315712287513, 0.046378479360638915, 0.06956771904095838, 0.046378479360638915, 0.09275695872127783, 0.13300454914580676, 0.11333989806242134, 0.11763036738970542, 0.11727282827909842, 0.11906052383213346, 0.09331770786842893, 0.1262113060442736, 0.07079274390018746, 0.06399950079865432, 0.04504992793648293, 0.17804394449626904, 0.11271515866042871, 0.09293249816084326, 0.10857460181167827, 0.13617831413668122, 0.10121361185834415, 0.07406996140542459, 0.10167367373042753, 0.06210835273125664, 0.03174426917375339, 0.17395733075763145, 0.1004542332544069, 0.09677907837924567, 0.13108052388075048, 0.08085340725354702, 0.08942876862892321, 0.13720578200601918, 0.061252581252687135, 0.0918788718790307, 0.03675154875161228, 0.23614547092515104, 0.12735935510569943, 0.07871515697505034, 0.08048403690707395, 0.12382159524165223, 0.0521819579946963, 0.06102635765481431, 0.07340851717897953, 0.1096705557854634, 0.05660415782475531, 0.14117711838216843, 0.07885568774499498, 0.14181305134785388, 0.09348214595576017, 0.15453171066156274, 0.0864868833332203, 0.10874453713221081, 0.07058855919108421, 0.06804482732834244, 0.0572339669116899, 0.13132255636849752, 0.15327272429176225, 0.1082370349319605, 0.0965050486281466, 0.0900713787196035, 0.08401615998215116, 0.09461279277269274, 0.11239999781395899, 0.06244444322997721, 0.067742759625248, 0.18196115170245036, 0.15468148940575127, 0.08880400705095663, 0.09141588961127889, 0.09576902721181597, 0.07806626763629848, 0.08009773184988245, 0.11289136844059519, 0.061234135580888395, 0.055139742940136474, 0.17777350793632726, 0.12203593944143919, 0.12966318565652912, 0.09534057768862436, 0.11939573882852343, 0.09328708832302322, 0.06277810346266342, 0.07597910652724219, 0.05603092411854539, 0.06805850468849493, 0.11772426851400093, 0.13912868097109202, 0.1070220622854554, 0.11772426851400093, 0.06421323737127324, 0.07491544359981878, 0.07491544359981878, 0.06421323737127324, 0.09631985605690986, 0.13912868097109202, 0.18389345449848712, 0.10249799103194364, 0.09948334423688647, 0.06632222949125764, 0.10249799103194364, 0.07536616987642915, 0.10249799103194364, 0.06029293590114332, 0.15374698654791547, 0.05426364231102898, 0.05957878706992868, 0.05957878706992868, 0.05957878706992868, 0.2978939353496434, 0.05957878706992868, 0.11915757413985736, 0.17873636120978606, 0.11915757413985736, 0.05957878706992868, 0.15666040019629107, 0.12256311335861127, 0.12152356193063323, 0.08659463395057097, 0.1378445193498885, 0.08825791623533584, 0.08794605080694243, 0.07921381881192688, 0.0654917399626167, 0.05374480882646482, 0.229256693037586, 0.07054052093464185, 0.07054052093464185, 0.1410810418692837, 0.05290539070098138, 0.05290539070098138, 0.07054052093464185, 0.05290539070098138, 0.19398643257026507, 0.05290539070098138, 0.21540866452459595, 0.1268682755200034, 0.12389712152656071, 0.09804808178360919, 0.08408365801442849, 0.07665577303082173, 0.07576442683278892, 0.06685096485246081, 0.0858663504104941, 0.04664711769705043, 0.14364887194415632, 0.12961045945870467, 0.12895751004077669, 0.10643075512226127, 0.11165435046568514, 0.10577780570433329, 0.06790673946451026, 0.08259810136788988, 0.06203019470315841, 0.061377245285230426, 0.33926748845396876, 0.18554680097603712, 0.14304268797712114, 0.10707766928573068, 0.14467746155400252, 0.09154732030535752, 0.06007792895039087, 0.08051259866140818, 0.06457355628681469, 0.06293878270993329, 0.05966923555617053, 0.17374530221728018, 0.11879758296481878, 0.13537398989014232, 0.10037935304779262, 0.1218672879509898, 0.0758217131584244, 0.07275200817225336, 0.08226809362938355, 0.06170107022203766, 0.05771045374001533, 0.1667913000230416, 0.14792350590278802, 0.13282927060658517, 0.13924432060747138, 0.09848988530772365, 0.06377314412645708, 0.06943348236253315, 0.07056555000974836, 0.05396189118392522, 0.05660338236076072, 0.10329672321849166, 0.10329672321849166, 0.10329672321849166, 0.10329672321849166, 0.10329672321849166, 0.20659344643698332, 0.10329672321849166, 0.14792579580508844, 0.11819908085861135, 0.1235074228133394, 0.1369552224319838, 0.08776458698483716, 0.09625793411240205, 0.11572185461307159, 0.06263843506579105, 0.060161208820251284, 0.050960082765389325, 0.26563447264269413, 0.11745913619078452, 0.07341196011924032, 0.11745913619078452, 0.02936478404769613, 0.1541651162504047, 0.0660707641073163, 0.03670598005962016, 0.1101179401788605, 0.05138837208346823, 0.227577076369645, 0.12675379398627676, 0.0845025293241845, 0.13731661015179983, 0.07393971315866144, 0.12675379398627676, 0.07393971315866144, 0.052814080827615316, 0.0845025293241845, 0.04225126466209225, 0.2006935071449382, 0.10406786943767712, 0.10406786943767712, 0.2775143185004723, 0.10406786943767712, 0.10406786943767712, 0.06937857962511808, 0.06937857962511808, 0.06937857962511808, 0.06937857962511808, 0.06937857962511808, 0.17530676062386188, 0.10518405637431713, 0.26296014093579284, 0.08765338031193094, 0.052592028187158565, 0.07012270424954475, 0.10518405637431713, 0.052592028187158565, 0.03506135212477238, 0.052592028187158565, 0.14077451105202546, 0.2815490221040509, 0.14077451105202546, 0.14077451105202546, 0.1434200483020105, 0.4302601449060314, 0.1434200483020105, 0.1434200483020105, 0.1434200483020105, 0.20068937307741627, 0.0668964576924721, 0.0668964576924721, 0.2675858307698884, 0.0668964576924721, 0.0668964576924721, 0.0668964576924721, 0.0668964576924721, 0.0668964576924721, 0.11562831265375348, 0.11562831265375348, 0.11562831265375348, 0.11562831265375348, 0.11562831265375348, 0.11562831265375348, 0.23125662530750696, 0.14994276771121787, 0.13031303629161048, 0.0839584898067545, 0.14213817570101253, 0.10642625468461836, 0.10382472401454992, 0.07473488106742089, 0.07544438943198502, 0.06480176396352319, 0.0683493057863438, 0.08870348166954833, 0.2808943586202364, 0.106444178003458, 0.0798331335025935, 0.07391956805795694, 0.05913565444636555, 0.10053061255882144, 0.07687635078027522, 0.04730852355709244, 0.08574669894723005, 0.13384362071575412, 0.2441307641855355, 0.07923542346372643, 0.07923542346372643, 0.08244767036090453, 0.12099463312704171, 0.05674969518347974, 0.097438155881069, 0.03747621380041115, 0.06745718484074006, 0.11215854590690108, 0.12756964381777297, 0.1738029375503887, 0.12842581592393254, 0.09332275957139097, 0.09332275957139097, 0.07448697323588087, 0.06335673585580671, 0.07705548955435952, 0.05650735900653031, 0.047396573132568356, 0.3317760119279785, 0.047396573132568356, 0.14218971939770508, 0.09479314626513671, 0.09479314626513671, 0.047396573132568356, 0.09479314626513671, 0.047396573132568356, 0.047396573132568356, 0.2201571425351688, 0.1100785712675844, 0.04892380945225973, 0.0733857141783896, 0.04892380945225973, 0.1956952378090389, 0.04892380945225973, 0.13454047599371427, 0.061154761815324664, 0.0733857141783896, 0.08882222791819558, 0.17764445583639116, 0.08882222791819558, 0.08882222791819558, 0.08882222791819558, 0.26646668375458676, 0.08882222791819558, 0.08882222791819558, 0.2658018887949204, 0.2658018887949204, 0.05009415395306527, 0.1502824618591958, 0.1502824618591958, 0.05009415395306527, 0.05009415395306527, 0.05009415395306527, 0.05009415395306527, 0.10018830790613054, 0.10018830790613054, 0.1502824618591958, 0.12841239058043183, 0.10272991246434546, 0.12841239058043183, 0.05136495623217273, 0.0770474343482591, 0.0770474343482591, 0.05136495623217273, 0.12841239058043183, 0.05136495623217273, 0.2311423030447773, 0.17988526344625416, 0.1298683853172957, 0.10749241352276163, 0.15443807983678406, 0.12767466259234136, 0.05659804630382143, 0.07239284992349253, 0.0618629808437118, 0.05264934539890366, 0.05747553539380316, 0.2582475129836293, 0.2582475129836293, 0.13820627322331325, 0.12483810691451061, 0.1456101807174193, 0.10776798685865496, 0.09830743839396387, 0.07959200556164021, 0.06334454189401857, 0.0884355617351558, 0.07033712119400763, 0.08349962340575175, 0.18272170446720848, 0.18272170446720848, 0.18272170446720848, 0.18272170446720848, 0.18272170446720848, 0.1354618256772442, 0.10823158591006281, 0.12132967592465639, 0.11960624302799934, 0.12270842224198204, 0.09754630195078909, 0.10857627248939422, 0.06928200244561346, 0.058252031907008325, 0.058941405065671144, 0.10433989544105601, 0.12657626660062532, 0.08723499454907961, 0.12657626660062532, 0.10262940535185837, 0.10091891526266072, 0.08210352428148669, 0.09236646481667253, 0.08039303419228905, 0.09749793508426545, 0.12142326457628154, 0.11242894868174216, 0.08544600099812405, 0.1191746856026467, 0.10680750124765506, 0.1292932909840035, 0.06520879023541046, 0.08994315894539373, 0.12029897508946412, 0.050593026906783974, 0.11054210229753136, 0.16172908562742275, 0.13668013633832696, 0.10727484804243191, 0.09420583102203411, 0.1197993226869798, 0.0920276615186345, 0.0664341698536888, 0.0658896274778389, 0.04574155957139229, 0.11460491936781822, 0.3438147581034547, 0.11460491936781822, 0.11460491936781822, 0.11460491936781822, 0.11460491936781822, 0.11460491936781822, 0.11460491936781822, 0.0813968626506686, 0.0813968626506686, 0.04883811759040116, 0.11395560771093605, 0.09767623518080232, 0.29302870554240695, 0.06511749012053489, 0.14651435277120348, 0.04883811759040116, 0.032558745060267444, 0.08241702104668304, 0.08241702104668304, 0.08241702104668304, 0.08241702104668304, 0.08241702104668304, 0.2472510631400491, 0.08241702104668304, 0.16483404209336608, 0.08241702104668304, 0.21372099021778262, 0.21372099021778262, 0.07985728849403488, 0.07985728849403488, 0.03992864424701744, 0.11978593274105231, 0.05989296637052616, 0.3194291539761395, 0.07985728849403488, 0.15971457698806976, 0.03992864424701744, 0.01996432212350872, 0.1193400912599679, 0.1193400912599679, 0.1193400912599679, 0.1193400912599679, 0.1193400912599679, 0.1193400912599679, 0.2386801825199358, 0.14955263124422727, 0.13975825237180936, 0.11263535703280593, 0.07873173785905165, 0.13749801109355908, 0.09417671992709527, 0.07082089338517564, 0.0915397717691366, 0.0636634626707164, 0.062156635151882876, 0.2586463666576708, 0.2586463666576708, 0.22933192631622173, 0.12030527282162451, 0.07143125573783955, 0.21429376721351867, 0.05263355685946072, 0.07895033528919108, 0.05639309663513649, 0.07143125573783955, 0.04511447730810919, 0.06391217618648802, 0.1683646913295922, 0.10823444442616642, 0.13228654318753674, 0.24052098761370316, 0.0841823456647961, 0.0841823456647961, 0.04810419752274063, 0.04810419752274063, 0.036078148142055476, 0.07215629628411095, 0.15408544432487478, 0.3852136108121869, 0.07704272216243739, 0.07704272216243739, 0.07704272216243739, 0.07704272216243739, 0.07704272216243739, 0.07704272216243739, 0.07704272216243739, 0.07704272216243739, 0.18943217650961278, 0.14337415712296184, 0.08691593981029291, 0.11737366230791693, 0.06388693011696744, 0.09657326645588102, 0.07725861316470482, 0.07651574188427496, 0.10028762285803029, 0.04828663322794051, 0.25873241135536673, 0.25873241135536673, 0.11548904139642792, 0.11548904139642792, 0.11548904139642792, 0.11548904139642792, 0.11548904139642792, 0.23097808279285584, 0.11548904139642792, 0.11548904139642792, 0.10391963970945721, 0.10391963970945721, 0.10391963970945721, 0.20783927941891442, 0.10391963970945721, 0.10391963970945721, 0.10391963970945721, 0.10391963970945721, 0.10391963970945721, 0.10391963970945721, 0.11208408418155341, 0.16189923270668827, 0.07472272278770228, 0.12453787131283713, 0.06226893565641856, 0.0996302970502697, 0.04981514852513485, 0.174353019837972, 0.06226893565641856, 0.07472272278770228, 0.14813158625026498, 0.049377195416755, 0.09875439083351, 0.246885977083775, 0.09875439083351, 0.09875439083351, 0.09875439083351, 0.09875439083351, 0.049377195416755, 0.11543824197075872, 0.12106937572542988, 0.18864298078148378, 0.0929137069520741, 0.09713705726807746, 0.09572927382940967, 0.08869035663607072, 0.04364128659870147, 0.08728257319740294, 0.07179695537205726, 0.2231460486624889, 0.2231460486624889, 0.17468217382086404, 0.08734108691043202, 0.08734108691043202, 0.08734108691043202, 0.08734108691043202, 0.08734108691043202, 0.08734108691043202, 0.17468217382086404, 0.08734108691043202, 0.15852940120083173, 0.14367981172126013, 0.11117125096868452, 0.10354578610079641, 0.11357929250591234, 0.11117125096868452, 0.06501712150515124, 0.0742479473978579, 0.05578629561244458, 0.06301042022412805, 0.14953630773388918, 0.15473757061158966, 0.1174618533214028, 0.094923047518034, 0.1330656419545043, 0.08061957460435765, 0.08495396033577472, 0.0728176802878069, 0.06024796166669738, 0.05157919020386322, 0.17374536617130473, 0.17374536617130473, 0.34749073234260947, 0.17374536617130473, 0.12017428159668543, 0.12017428159668543, 0.14420913791602252, 0.09613942527734834, 0.07210456895801126, 0.04806971263867417, 0.07210456895801126, 0.07210456895801126, 0.04806971263867417, 0.19227885055469668, 0.07921109894726695, 0.3168443957890678, 0.07921109894726695, 0.1584221978945339, 0.07921109894726695, 0.07921109894726695, 0.07921109894726695, 0.07921109894726695, 0.1566415330325677, 0.11748114977442577, 0.07832076651628385, 0.07832076651628385, 0.2741226828069934, 0.07832076651628385, 0.039160383258141925, 0.07832076651628385, 0.039160383258141925, 0.039160383258141925, 0.1788993881181184, 0.1788993881181184, 0.1788993881181184, 0.1788993881181184, 0.1788993881181184, 0.18134766115340586, 0.23351616641671438, 0.10433701052661706, 0.08197907969948484, 0.08197907969948484, 0.0869475087721809, 0.06458957794504866, 0.047200076190612485, 0.04968429072696051, 0.07204222155409273, 0.1439154660915292, 0.12881719174710538, 0.12913843162677396, 0.10086932221593788, 0.08962592642753715, 0.10890031920765267, 0.09797816329892055, 0.058786897979352326, 0.08416484847317109, 0.05782317834034655, 0.1410787059267737, 0.13561318705309727, 0.1072608078959006, 0.12399895944653477, 0.10965197240313405, 0.07515088451305138, 0.09769614986696679, 0.0748092895834466, 0.06387825183609368, 0.07036855549858448, 0.14298819010058814, 0.16629865096398092, 0.13296078514817894, 0.11459891374181927, 0.10040427556243484, 0.09233026118517029, 0.08516782907630659, 0.06823844409171965, 0.066285053516575, 0.030603119010599464, 0.13947292636931022, 0.11157834109544819, 0.11157834109544819, 0.08368375582158613, 0.22315668219089638, 0.13947292636931022, 0.027894585273862047, 0.055789170547724094, 0.055789170547724094, 0.055789170547724094, 0.24675130080396185, 0.09824357346824407, 0.11423671333516752, 0.08567753500137563, 0.09367410493483737, 0.07768096506791392, 0.05940309093428711, 0.05254888813417706, 0.12566038466868426, 0.04797941960077036, 0.5058278979798333, 0.12645697449495832, 0.12645697449495832, 0.2143264550863242, 0.08573058203452967, 0.07501425928021346, 0.1285958730517945, 0.06429793652589726, 0.05358161377158105, 0.07501425928021346, 0.1071632275431621, 0.16074484131474315, 0.03214896826294863, 0.14031275587242434, 0.1686703572344914, 0.10802115200926125, 0.10474281659168125, 0.09556347742245723, 0.11605307378233227, 0.07113987856148617, 0.07015637793621217, 0.06294404001753615, 0.06228837293402015, 0.18374964944752636, 0.09187482472376318, 0.27562447417128955, 0.04593741236188159, 0.13781223708564477, 0.04593741236188159, 0.13781223708564477, 0.04593741236188159, 0.04593741236188159, 0.04593741236188159, 0.16292238918872615, 0.09039215612796435, 0.08119055939637516, 0.10717153840321521, 0.16292238918872615, 0.12070329830261108, 0.05845720276539012, 0.08281437058430266, 0.08389691137625434, 0.048714335637825096, 0.13309154680218993, 0.06654577340109497, 0.03327288670054748, 0.06654577340109497, 0.3327288670054748, 0.09981866010164245, 0.06654577340109497, 0.09981866010164245, 0.06654577340109497, 0.03327288670054748, 0.05603311025731108, 0.11206622051462216, 0.22413244102924432, 0.05603311025731108, 0.05603311025731108, 0.22413244102924432, 0.05603311025731108, 0.05603311025731108, 0.05603311025731108, 0.05603311025731108, 0.26519149765469685, 0.26519149765469685, 0.18890925920958782, 0.1759347221759623, 0.09691979164118276, 0.1110620370078346, 0.08355601849654845, 0.07187893516628548, 0.08355601849654845, 0.08005289349746957, 0.058644907391987425, 0.049692476838785804, 0.14908679077437093, 0.14908679077437093, 0.14908679077437093, 0.29817358154874185, 0.14908679077437093, 0.14908679077437093, 0.14908679077437093, 0.2061505045223969, 0.10307525226119844, 0.10307525226119844, 0.10307525226119844, 0.10307525226119844, 0.10307525226119844, 0.2061505045223969, 0.10953814195377191, 0.02738453548844298, 0.1369226774422149, 0.10953814195377191, 0.1369226774422149, 0.21907628390754383, 0.05476907097688596, 0.05476907097688596, 0.08215360646532893, 0.02738453548844298, 0.18558435954915, 0.0463960898872875, 0.0463960898872875, 0.18558435954915, 0.06959413483093126, 0.278376539323725, 0.0463960898872875, 0.06959413483093126, 0.06959413483093126, 0.0463960898872875, 0.26114300774152066, 0.16321437983845039, 0.13057150387076033, 0.06528575193538017, 0.06528575193538017, 0.19585725580614047, 0.03264287596769008, 0.03264287596769008, 0.03264287596769008, 0.13567332872385887, 0.1423898301458321, 0.12425527630650442, 0.1141805241735446, 0.06179181308215355, 0.09537432019201961, 0.07858306663708658, 0.08731451848565176, 0.09470267004982229, 0.0664933640775348, 0.16063948690768612, 0.14486820124974528, 0.11365081108144999, 0.1068220069821354, 0.09121331189798776, 0.09674139140695671, 0.07316575820694206, 0.0925140364883334, 0.05999592172969249, 0.06015851230348569, 0.21490374421325026, 0.13064867824378737, 0.08670806156097889, 0.09566682806912431, 0.08361515407602392, 0.1029191628614325, 0.07657612324819538, 0.08468167389842218, 0.06409784132613568, 0.06036502194774176, 0.1478520870521558, 0.0947177432677873, 0.06468528808531816, 0.15247246477253565, 0.1894354865355746, 0.07623623238626782, 0.07854642124645776, 0.09240755440759738, 0.057754721504748356, 0.04620377720379869, 0.1838396540507462, 0.12255976936716415, 0.08170651291144276, 0.10213314113930345, 0.06808876075953563, 0.17703077797479264, 0.04766213253167494, 0.10213314113930345, 0.06808876075953563, 0.04085325645572138, 0.19602323329928537, 0.11625846017484212, 0.09722960906999129, 0.13580864966612724, 0.13137727338143596, 0.08028611151087751, 0.06933800539575787, 0.05552253815524971, 0.05656521492811825, 0.06151792959924381, 0.21520444553949425, 0.13580251248539096, 0.13139129398238522, 0.09799206817391319, 0.0794019330541033, 0.07719632380260043, 0.06963423494030488, 0.08160754230560617, 0.06774371272473098, 0.044112185030057385, 0.19918247557708119, 0.39836495115416237, 0.1878013113338661, 0.15740274516459832, 0.11154728094316052, 0.11128966597562436, 0.09093808354026713, 0.06388851194896954, 0.07934541000113958, 0.0752235705205609, 0.07342026574780773, 0.04920445879940799, 0.1596748242296612, 0.0798374121148306, 0.0798374121148306, 0.3193496484593224, 0.0798374121148306, 0.0798374121148306, 0.0798374121148306, 0.0798374121148306, 0.0798374121148306, 0.0798374121148306, 0.10798242627573441, 0.12459510724123202, 0.14951412868947841, 0.06645072386199041, 0.09136974531023681, 0.19104583110322243, 0.07475706434473921, 0.033225361930995206, 0.09136974531023681, 0.06645072386199041, 0.2636705679736772, 0.08789018932455907, 0.08789018932455907, 0.1318352839868386, 0.043945094662279534, 0.043945094662279534, 0.043945094662279534, 0.043945094662279534, 0.17578037864911814, 0.043945094662279534, 0.2184502720499926, 0.2184502720499926, 0.2184502720499926, 0.1256232429602856, 0.1425993568738377, 0.07129967843691885, 0.10185668348131265, 0.0780901240023397, 0.11543757461215433, 0.06450923287149801, 0.09167101513318138, 0.15278502522196896, 0.054323564523366744, 0.09272012742565067, 0.14835220388104106, 0.037088050970260265, 0.14835220388104106, 0.14835220388104106, 0.037088050970260265, 0.07417610194052053, 0.07417610194052053, 0.07417610194052053, 0.1668962293661712, 0.14840756086260568, 0.14840756086260568, 0.29681512172521135, 0.14840756086260568, 0.1352600983614076, 0.0676300491807038, 0.0676300491807038, 0.1352600983614076, 0.0676300491807038, 0.0676300491807038, 0.0676300491807038, 0.0676300491807038, 0.1352600983614076, 0.20289014754211138, 0.1007478412195977, 0.1007478412195977, 0.08395653434966475, 0.15112176182939657, 0.1007478412195977, 0.21828698930912838, 0.0335826137398659, 0.15112176182939657, 0.05037392060979885, 0.01679130686993295, 0.16162154670559664, 0.16162154670559664, 0.28283770673479414, 0.08081077335279832, 0.08081077335279832, 0.04040538667639916, 0.04040538667639916, 0.08081077335279832, 0.04040538667639916, 0.04040538667639916, 0.33989566668525556, 0.11329855556175185, 0.074230088126665, 0.09376432184420842, 0.10157801533122579, 0.050789007665612895, 0.05469585440912158, 0.06250954789613895, 0.05860270115263026, 0.050789007665612895, 0.11569446634975294, 0.1818055899781832, 0.11569446634975294, 0.2809722754208286, 0.03305556181421513, 0.03305556181421513, 0.06611112362843026, 0.06611112362843026, 0.06611112362843026, 0.04958334272132269, 0.15553009780196017, 0.16371589242311596, 0.24557383863467394, 0.10641533007502538, 0.07367215159040219, 0.04092897310577899, 0.08185794621155798, 0.049114767726934794, 0.04092897310577899, 0.05730056234809059, 0.13391654873964104, 0.12006311266312646, 0.10620967658661186, 0.09697405253560214, 0.06003155633156323, 0.09235624051009726, 0.06926718038257296, 0.2170371651987286, 0.04156030822954377, 0.06003155633156323, 0.2534923014771583, 0.2534923014771583, 0.2534923014771583, 0.10320862691724758, 0.10320862691724758, 0.10320862691724758, 0.10320862691724758, 0.30962588075174274, 0.10320862691724758, 0.10320862691724758, 0.10320862691724758, 0.13502832084629102, 0.12020813928999077, 0.14216396381784296, 0.08727440249821247, 0.10977912263926098, 0.0784920726870716, 0.1235015129691686, 0.08288323759264205, 0.05049839641406005, 0.07025863848912703, 0.1568605534516143, 0.2012641562748405, 0.11004371134451711, 0.10714782420387192, 0.0680533478051619, 0.07143188280258128, 0.09073779707354919, 0.07191453065935548, 0.07046658708903289, 0.05260861638838756, 0.36400798293946873, 0.18200399146973437, 0.18200399146973437, 0.14257011439294776, 0.14361586095328724, 0.09864875885869001, 0.12060943662581888, 0.10004308760580931, 0.09167711512309355, 0.07633949890478132, 0.10318032728682772, 0.055424567697991915, 0.06832210860884538, 0.4290637957126799, 0.07151063261877999, 0.14302126523755998, 0.07151063261877999, 0.07151063261877999, 0.07151063261877999, 0.07151063261877999, 0.07151063261877999, 0.07151063261877999, 0.07151063261877999, 0.12787882962743843, 0.19181824444115764, 0.3196970740685961, 0.06393941481371922, 0.06393941481371922, 0.06393941481371922, 0.06393941481371922, 0.06393941481371922, 0.06393941481371922, 0.06393941481371922, 0.2138245662213472, 0.2138245662213472, 0.2138245662213472, 0.13603043746236695, 0.09068695830824464, 0.13603043746236695, 0.04534347915412232, 0.04534347915412232, 0.04534347915412232, 0.2720608749247339, 0.09068695830824464, 0.09068695830824464, 0.04534347915412232, 0.22553081243261658, 0.09021232497304663, 0.09021232497304663, 0.04510616248652331, 0.22553081243261658, 0.04510616248652331, 0.04510616248652331, 0.09021232497304663, 0.09021232497304663, 0.04510616248652331, 0.1435653153746851, 0.1435653153746851, 0.1435653153746851, 0.2871306307493702, 0.1435653153746851, 0.1435653153746851, 0.1149735190211576, 0.1149735190211576, 0.1149735190211576, 0.2299470380423152, 0.1149735190211576, 0.1149735190211576, 0.1149735190211576, 0.1149735190211576, 0.26913461876516004, 0.11553645140295155, 0.11553645140295155, 0.08220429778182713, 0.09909559184658612, 0.07657386642690746, 0.07522256290172674, 0.06351126568349383, 0.0556286617866063, 0.04752084063552198, 0.16123145670555356, 0.0688298031550024, 0.12445937008849749, 0.08203003937650971, 0.16971732284795113, 0.06034393701260484, 0.12917374016760724, 0.07354417323411215, 0.06788692913918044, 0.06222968504424874, 0.13472849512883123, 0.13472849512883123, 0.13472849512883123, 0.13472849512883123, 0.26945699025766245, 0.13472849512883123, 0.13472849512883123, 0.16946108692349385, 0.13202200957993124, 0.06502576591250345, 0.07980434907443605, 0.1413817789158219, 0.08867149897159561, 0.09753864886875517, 0.11724342641799865, 0.06748886310615888, 0.04138003285341128, 0.2364787756937886, 0.0788262585645962, 0.1576525171291924, 0.0788262585645962, 0.2364787756937886, 0.0788262585645962, 0.0788262585645962, 0.0788262585645962, 0.35530641546169395, 0.3446350914451215, 0.17231754572256075, 0.17231754572256075, 0.17231754572256075, 0.17194023102069458, 0.08597011551034729, 0.2579103465310419, 0.08597011551034729, 0.08597011551034729, 0.08597011551034729, 0.08597011551034729, 0.08597011551034729, 0.08597011551034729, 0.08597011551034729, 0.5162578809441672, 0.11626932224546924, 0.11626932224546924, 0.11626932224546924, 0.23253864449093847, 0.11626932224546924, 0.11626932224546924, 0.11626932224546924, 0.11626932224546924, 0.15406187656842354, 0.15406187656842354, 0.12324950125473882, 0.12324950125473882, 0.24649900250947765, 0.030812375313684706, 0.06162475062736941, 0.06162475062736941, 0.06162475062736941, 0.030812375313684706, 0.12500303365606552, 0.12500303365606552, 0.08333535577071036, 0.1666707115414207, 0.04166767788535518, 0.08333535577071036, 0.08333535577071036, 0.04166767788535518, 0.20833838942677588, 0.04166767788535518], \"Term\": [\"aaabbb\", \"aaabbb\", \"aaabbb\", \"abuam\", \"abuam\", \"abuam\", \"abuam\", \"accelerated\", \"accelerated\", \"accelerated\", \"accelerated\", \"accelerated\", \"accelerated\", \"accelerated\", \"accelerated\", \"accelerated\", \"accelerated\", \"action\", \"action\", \"action\", \"action\", \"action\", \"action\", \"action\", \"action\", \"action\", \"action\", \"actions\", \"actions\", \"actions\", \"actions\", \"actions\", \"actions\", \"actions\", \"actions\", \"actions\", \"actions\", \"adagrad\", \"adagrad\", \"adagrad\", \"adagrad\", \"adagrad\", \"adagrad\", \"adagrad\", \"adagrad\", \"adagrad\", \"adagrad\", \"adex\", \"adex\", \"adex\", \"adex\", \"adex\", \"adex\", \"adex\", \"adex\", \"adex\", \"adex\", \"adwords\", \"adwords\", \"adwords\", \"adwords\", \"adwords\", \"adwords\", \"adwords\", \"adwords\", \"adwords\", \"ahp\", \"ahp\", \"ahp\", \"ahp\", \"ahp\", \"ahp\", \"ahp\", \"ahp\", \"ahp\", \"ahp\", \"aifnn\", \"aifnn\", \"aifnn\", \"aifnn\", \"aifnn\", \"aifnn\", \"aifnn\", \"aifnn\", \"aifnn\", \"aifnn\", \"ald\", \"ald\", \"ald\", \"ald\", \"ald\", \"ald\", \"ald\", \"ald\", \"ald\", \"ald\", \"alertness\", \"alertness\", \"alertness\", \"alertness\", \"alertness\", \"alertness\", \"alertness\", \"alertness\", \"alertness\", \"alertness\", \"alexanders\", \"alexanders\", \"alexanders\", \"alexanders\", \"alexanders\", \"alexanders\", \"alexanders\", \"alexanders\", \"algorithm\", \"algorithm\", \"algorithm\", \"algorithm\", \"algorithm\", \"algorithm\", \"algorithm\", \"algorithm\", \"algorithm\", \"algorithm\", \"algorithms\", \"algorithms\", \"algorithms\", \"algorithms\", \"algorithms\", \"algorithms\", \"algorithms\", \"algorithms\", \"algorithms\", \"algorithms\", \"also\", \"also\", \"also\", \"also\", \"also\", \"also\", \"also\", \"also\", \"also\", \"also\", \"alstr\", \"alstr\", \"alstr\", \"analysis\", \"analysis\", \"analysis\", \"analysis\", \"analysis\", \"analysis\", \"analysis\", \"analysis\", \"analysis\", \"analysis\", \"andsmooth\", \"anificial\", \"anificial\", \"anificial\", \"approach\", \"approach\", \"approach\", \"approach\", \"approach\", \"approach\", \"approach\", \"approach\", \"approach\", \"approach\", \"arcachon\", \"argminkl\", \"argyy\", \"argyy\", \"argyy\", \"arrhythmia\", \"arrhythmia\", \"arrhythmia\", \"arrhythmia\", \"arrhythmia\", \"arrhythmia\", \"arrhythmia\", \"arrhythmia\", \"arrhythmia\", \"arrhythmia\", \"arrhythmias\", \"arrhythmias\", \"arrhythmias\", \"arrhythmias\", \"arrhythmias\", \"arrhythmias\", \"arrhythmias\", \"arrhythmias\", \"arrhythmias\", \"arrhythmias\", \"asap\", \"asap\", \"asap\", \"asap\", \"asap\", \"asap\", \"asap\", \"asap\", \"asap\", \"asap\", \"asogawa\", \"asogawa\", \"asogawa\", \"asogawa\", \"asogawa\", \"asogawa\", \"assumptions\", \"assumptions\", \"assumptions\", \"assumptions\", \"assumptions\", \"assumptions\", \"assumptions\", \"assumptions\", \"assumptions\", \"assumptions\", \"asymmetricity\", \"atda\", \"atda\", \"atda\", \"atda\", \"atda\", \"atda\", \"atda\", \"atda\", \"atda\", \"atrial\", \"atrial\", \"automorphism\", \"automorphism\", \"automorphism\", \"automorphism\", \"axoclamp\", \"axoclamp\", \"axoclamp\", \"axoclamp\", \"axoclamp\", \"axoclamp\", \"bacon\", \"bacon\", \"bacon\", \"bacon\", \"bain\", \"bain\", \"bain\", \"bain\", \"bain\", \"bain\", \"bain\", \"bain\", \"balleine\", \"balleine\", \"balleine\", \"balleine\", \"based\", \"based\", \"based\", \"based\", \"based\", \"based\", \"based\", \"based\", \"based\", \"based\", \"bcpg\", \"bcpg\", \"bcpg\", \"bcpg\", \"bcpg\", \"bcpg\", \"bcpg\", \"bcpg\", \"bcpg\", \"bcpg\", \"behavior\", \"behavior\", \"behavior\", \"behavior\", \"behavior\", \"behavior\", \"behavior\", \"behavior\", \"behavior\", \"behavior\", \"bio\", \"bio\", \"bio\", \"bio\", \"bio\", \"bio\", \"bio\", \"bio\", \"bio\", \"bio\", \"biologische\", \"biorthogonal\", \"biorthogonal\", \"biorthogonal\", \"biorthogonal\", \"biorthogonal\", \"biorthogonal\", \"biorthogonal\", \"biorthogonal\", \"biv\", \"blht\", \"blht\", \"blht\", \"blht\", \"blht\", \"blht\", \"blht\", \"blht\", \"blht\", \"blht\", \"blumer\", \"blumer\", \"blumer\", \"blumer\", \"bob\", \"bob\", \"bob\", \"bob\", \"bob\", \"bob\", \"bob\", \"bob\", \"bob\", \"bob\", \"booster\", \"booster\", \"booster\", \"booster\", \"booster\", \"booster\", \"booster\", \"booster\", \"booster\", \"booster\", \"bordeaux\", \"bound\", \"bound\", \"bound\", \"bound\", \"bound\", \"bound\", \"bound\", \"bound\", \"bound\", \"bound\", \"bounds\", \"bounds\", \"bounds\", \"bounds\", \"bounds\", \"bounds\", \"bounds\", \"bounds\", \"bounds\", \"bounds\", \"bourke\", \"bourke\", \"bourke\", \"bqu\", \"bqu\", \"bqu\", \"brede\", \"brede\", \"brede\", \"brede\", \"brede\", \"brede\", \"brede\", \"brede\", \"brede\", \"buchholtz\", \"bungie\", \"bungie\", \"bungie\", \"buyer\", \"buyer\", \"buyer\", \"buyer\", \"buyer\", \"buyer\", \"buyer\", \"buyer\", \"buyer\", \"buyer\", \"bvalue\", \"cardia\", \"cardia\", \"cardiologists\", \"cardiologists\", \"cardiologists\", \"case\", \"case\", \"case\", \"case\", \"case\", \"case\", \"case\", \"case\", \"case\", \"case\", \"category\", \"category\", \"category\", \"category\", \"category\", \"category\", \"category\", \"category\", \"category\", \"category\", \"cbmpi\", \"cbmpi\", \"cbmpi\", \"cbmpi\", \"cbmpi\", \"cbmpi\", \"cbmpi\", \"cbmpi\", \"cbmpi\", \"cbmpi\", \"ccn\", \"ccn\", \"ccn\", \"ccn\", \"ccn\", \"ccn\", \"ccn\", \"ccn\", \"ccn\", \"ccn\", \"cgd\", \"cgd\", \"cgd\", \"cgd\", \"cgd\", \"cgd\", \"chaperones\", \"chaperones\", \"class\", \"class\", \"class\", \"class\", \"class\", \"class\", \"class\", \"class\", \"class\", \"class\", \"classification\", \"classification\", \"classification\", \"classification\", \"classification\", \"classification\", \"classification\", \"classification\", \"classification\", \"classification\", \"clue\", \"clue\", \"clue\", \"clue\", \"clue\", \"clue\", \"clue\", \"clue\", \"clue\", \"clue\", \"clustering\", \"clustering\", \"clustering\", \"clustering\", \"clustering\", \"clustering\", \"clustering\", \"clustering\", \"clustering\", \"clustering\", \"componential\", \"componential\", \"componential\", \"componential\", \"componential\", \"componential\", \"componential\", \"componential\", \"componential\", \"componential\", \"compressibility\", \"compressibility\", \"compressibility\", \"compressibility\", \"compressibility\", \"compression\", \"compression\", \"compression\", \"compression\", \"compression\", \"compression\", \"compression\", \"compression\", \"compression\", \"compression\", \"congested\", \"congested\", \"congested\", \"congested\", \"congested\", \"congested\", \"congested\", \"consider\", \"consider\", \"consider\", \"consider\", \"consider\", \"consider\", \"consider\", \"consider\", \"consider\", \"consider\", \"convexokl\", \"convexokl\", \"convexokl\", \"convexokl\", \"convexokl\", \"convexokl\", \"convexokl\", \"coring\", \"coring\", \"coring\", \"correlation\", \"correlation\", \"correlation\", \"correlation\", \"correlation\", \"correlation\", \"correlation\", \"correlation\", \"correlation\", \"correlation\", \"cot\", \"cot\", \"cot\", \"cot\", \"cot\", \"cot\", \"cot\", \"cot\", \"cot\", \"cot\", \"crab\", \"crcn\", \"crcn\", \"crcn\", \"crcn\", \"crcn\", \"crcn\", \"crcn\", \"crcn\", \"crcn\", \"crcn\", \"csibra\", \"csibra\", \"csibra\", \"csibra\", \"csibra\", \"ctbns\", \"ctbns\", \"ctbns\", \"ctbns\", \"ctbns\", \"ctbns\", \"ctbns\", \"ctbns\", \"ctbns\", \"ctbns\", \"data\", \"data\", \"data\", \"data\", \"data\", \"data\", \"data\", \"data\", \"data\", \"data\", \"daubechies\", \"daubechies\", \"daubechies\", \"daubechies\", \"daubechies\", \"daubechies\", \"daubechies\", \"daubechies\", \"daubechies\", \"daubechies\", \"decision\", \"decision\", \"decision\", \"decision\", \"decision\", \"decision\", \"decision\", \"decision\", \"decision\", \"decision\", \"deepid\", \"deepid\", \"deepid\", \"deepid\", \"deepid\", \"deepid\", \"deepid\", \"deepid\", \"deepid\", \"deepid\", \"defender\", \"defender\", \"defender\", \"defender\", \"defender\", \"defender\", \"defender\", \"defenders\", \"defenders\", \"defenders\", \"defenders\", \"defenders\", \"defenders\", \"defenders\", \"defibrillators\", \"defibrillators\", \"defibrillators\", \"defibrillators\", \"defibrillators\", \"defibrillators\", \"defibrillators\", \"defibrillators\", \"defibrillators\", \"define\", \"define\", \"define\", \"define\", \"define\", \"define\", \"define\", \"define\", \"define\", \"define\", \"delays\", \"delays\", \"delays\", \"delays\", \"delays\", \"delays\", \"delays\", \"delays\", \"delays\", \"delays\", \"demultiplexing\", \"demultiplexing\", \"demultiplexing\", \"demultiplexing\", \"derin\", \"different\", \"different\", \"different\", \"different\", \"different\", \"different\", \"different\", \"different\", \"different\", \"different\", \"dirl\", \"dirl\", \"dirl\", \"dirl\", \"dirl\", \"dirl\", \"dirl\", \"dirl\", \"dirl\", \"disneyland\", \"disneyland\", \"disneyland\", \"disneyland\", \"disneyland\", \"disneyland\", \"disneyland\", \"disneyland\", \"disneyland\", \"disparity\", \"disparity\", \"disparity\", \"disparity\", \"disparity\", \"disparity\", \"disparity\", \"disparity\", \"disparity\", \"disparity\", \"distillation\", \"distillation\", \"distillation\", \"distillation\", \"distillation\", \"distillation\", \"distillation\", \"distillation\", \"distillation\", \"distillation\", \"distribution\", \"distribution\", \"distribution\", \"distribution\", \"distribution\", \"distribution\", \"distribution\", \"distribution\", \"distribution\", \"distribution\", \"distributions\", \"distributions\", \"distributions\", \"distributions\", \"distributions\", \"distributions\", \"distributions\", \"distributions\", \"distributions\", \"distributions\", \"documents\", \"documents\", \"documents\", \"documents\", \"documents\", \"documents\", \"documents\", \"documents\", \"documents\", \"documents\", \"dre\", \"dre\", \"dre\", \"dre\", \"dre\", \"dre\", \"dre\", \"dre\", \"dribbling\", \"drr\", \"drr\", \"drr\", \"drr\", \"drr\", \"drr\", \"drr\", \"drr\", \"drr\", \"drr\", \"duchenne\", \"duchenne\", \"duchenne\", \"duchenne\", \"duchenne\", \"duchenne\", \"duchenne\", \"duchenne\", \"duchenne\", \"duchenne\", \"eaton\", \"eaton\", \"edge\", \"edge\", \"edge\", \"edge\", \"edge\", \"edge\", \"edge\", \"edge\", \"edge\", \"edge\", \"edges\", \"edges\", \"edges\", \"edges\", \"edges\", \"edges\", \"edges\", \"edges\", \"edges\", \"edges\", \"eeg\", \"eeg\", \"eeg\", \"eeg\", \"eeg\", \"eeg\", \"eeg\", \"eeg\", \"eeg\", \"eeg\", \"elets\", \"elo\", \"elo\", \"elo\", \"elo\", \"elo\", \"elo\", \"elo\", \"elo\", \"elo\", \"elo\", \"entro\", \"entro\", \"entro\", \"entro\", \"entro\", \"entro\", \"entro\", \"enzymes\", \"enzymes\", \"enzymes\", \"enzymes\", \"enzymes\", \"enzymes\", \"enzymes\", \"enzymes\", \"epu\", \"epu\", \"epu\", \"epu\", \"epu\", \"epu\", \"epu\", \"epu\", \"epu\", \"epu\", \"equivalences\", \"equivalences\", \"equivalences\", \"equivalences\", \"equivalences\", \"equivalences\", \"equivalences\", \"equivalences\", \"equivalences\", \"equivalences\", \"error\", \"error\", \"error\", \"error\", \"error\", \"error\", \"error\", \"error\", \"error\", \"error\", \"erx\", \"etric\", \"etric\", \"etric\", \"etric\", \"etric\", \"etric\", \"etric\", \"etric\", \"etric\", \"etric\", \"ettt\", \"ettt\", \"ettt\", \"eus\", \"eus\", \"eus\", \"eus\", \"eus\", \"eus\", \"eus\", \"eus\", \"eus\", \"eus\", \"example\", \"example\", \"example\", \"example\", \"example\", \"example\", \"example\", \"example\", \"example\", \"example\", \"examples\", \"examples\", \"examples\", \"examples\", \"examples\", \"examples\", \"examples\", \"examples\", \"examples\", \"examples\", \"experiments\", \"experiments\", \"experiments\", \"experiments\", \"experiments\", \"experiments\", \"experiments\", \"experiments\", \"experiments\", \"experiments\", \"exploded\", \"exploded\", \"feature\", \"feature\", \"feature\", \"feature\", \"feature\", \"feature\", \"feature\", \"feature\", \"feature\", \"feature\", \"features\", \"features\", \"features\", \"features\", \"features\", \"features\", \"features\", \"features\", \"features\", \"features\", \"fhi\", \"fictive\", \"fictive\", \"fictive\", \"fictive\", \"fictive\", \"fictive\", \"fictive\", \"field\", \"field\", \"field\", \"field\", \"field\", \"field\", \"field\", \"field\", \"field\", \"field\", \"figure\", \"figure\", \"figure\", \"figure\", \"figure\", \"figure\", \"figure\", \"figure\", \"figure\", \"figure\", \"fingerprint\", \"fingerprint\", \"fingerprint\", \"fins\", \"fins\", \"fins\", \"first\", \"first\", \"first\", \"first\", \"first\", \"first\", \"first\", \"first\", \"first\", \"first\", \"fixed\", \"fixed\", \"fixed\", \"fixed\", \"fixed\", \"fixed\", \"fixed\", \"fixed\", \"fixed\", \"fixed\", \"fjv\", \"flops\", \"flops\", \"flops\", \"flops\", \"flops\", \"flops\", \"flops\", \"flops\", \"flops\", \"flops\", \"fmtl\", \"fmtl\", \"fmtl\", \"fmtlp\", \"fmtlp\", \"fmtlp\", \"fmtlp\", \"fmtlp\", \"fmtlp\", \"fmtlp\", \"fmtlp\", \"fnew\", \"fnew\", \"following\", \"following\", \"following\", \"following\", \"following\", \"following\", \"following\", \"following\", \"following\", \"following\", \"fom\", \"fom\", \"fom\", \"fom\", \"fom\", \"fom\", \"fom\", \"fom\", \"fom\", \"fom\", \"fouling\", \"fouling\", \"fouling\", \"fouling\", \"fouling\", \"fouling\", \"fouling\", \"fouling\", \"fouling\", \"fouling\", \"fpnns\", \"fpnns\", \"fpnns\", \"fpnns\", \"fpnns\", \"fpnns\", \"fpnns\", \"fpnns\", \"fpnns\", \"fpnns\", \"fscore\", \"fscore\", \"fscore\", \"fspyr\", \"ftks\", \"ftks\", \"ftks\", \"function\", \"function\", \"function\", \"function\", \"function\", \"function\", \"function\", \"function\", \"function\", \"function\", \"functions\", \"functions\", \"functions\", \"functions\", \"functions\", \"functions\", \"functions\", \"functions\", \"functions\", \"functions\", \"gaifman\", \"gaifman\", \"gaifman\", \"gaifman\", \"gaifman\", \"gaifman\", \"gaifman\", \"gaifman\", \"gaifman\", \"gaifman\", \"gater\", \"gater\", \"gater\", \"gater\", \"gater\", \"gater\", \"gater\", \"gater\", \"gater\", \"gater\", \"gaussain\", \"gaussain\", \"gaussian\", \"gaussian\", \"gaussian\", \"gaussian\", \"gaussian\", \"gaussian\", \"gaussian\", \"gaussian\", \"gaussian\", \"gaussian\", \"gaze\", \"gaze\", \"gaze\", \"gaze\", \"gaze\", \"gaze\", \"gaze\", \"gaze\", \"gaze\", \"gaze\", \"gazefollow\", \"gazefollow\", \"gazefollow\", \"gazefollow\", \"gazefollow\", \"gazefollow\", \"gazefollow\", \"gazefollow\", \"gergely\", \"gergely\", \"gergely\", \"gergely\", \"gergely\", \"gergely\", \"gergely\", \"gergely\", \"gergely\", \"gergely\", \"given\", \"given\", \"given\", \"given\", \"given\", \"given\", \"given\", \"given\", \"given\", \"given\", \"gjt\", \"gjt\", \"gjt\", \"gjt\", \"gjt\", \"gjt\", \"gjt\", \"gjt\", \"gjt\", \"gjtgkt\", \"glad\", \"glad\", \"glad\", \"glad\", \"glad\", \"glad\", \"glad\", \"glad\", \"glad\", \"glad\", \"gmmmfcc\", \"gmmmfcc\", \"gmmmfcc\", \"gms\", \"gms\", \"gms\", \"gms\", \"gms\", \"gms\", \"gms\", \"gms\", \"gms\", \"goalkeeper\", \"goalkeeper\", \"goalkeeper\", \"goalkeeper\", \"golowasch\", \"gpfa\", \"gpfa\", \"gpfa\", \"gpfa\", \"gpfa\", \"gpfa\", \"gpfa\", \"gpfa\", \"gpfa\", \"gpfa\", \"graduated\", \"graduated\", \"graduated\", \"graduated\", \"graph\", \"graph\", \"graph\", \"graph\", \"graph\", \"graph\", \"graph\", \"graph\", \"graph\", \"graph\", \"gratings\", \"gratings\", \"gratings\", \"gratings\", \"gratings\", \"gratings\", \"gratings\", \"gratings\", \"gratings\", \"gratings\", \"greebles\", \"greebles\", \"greebles\", \"gsm\", \"gsm\", \"gsm\", \"gsm\", \"gsm\", \"gsm\", \"gsm\", \"gsm\", \"gsm\", \"gsm\", \"gt\", \"gt\", \"gt\", \"gt\", \"gt\", \"gt\", \"gt\", \"gt\", \"gt\", \"gt\", \"halting\", \"halting\", \"halting\", \"halting\", \"halting\", \"halting\", \"halting\", \"halting\", \"halting\", \"halting\", \"hamm\", \"hamm\", \"hamm\", \"hawk\", \"hawk\", \"hawk\", \"hawk\", \"hawk\", \"hawk\", \"hawk\", \"hawk\", \"hawk\", \"hawk\", \"hca\", \"hca\", \"hca\", \"hca\", \"hca\", \"hca\", \"hca\", \"hca\", \"hca\", \"hca\", \"hcj\", \"heap\", \"heap\", \"heap\", \"heap\", \"heap\", \"heap\", \"heap\", \"heap\", \"heap\", \"heap\", \"hearer\", \"hearer\", \"hearer\", \"hearer\", \"hearer\", \"hearer\", \"hearer\", \"hearer\", \"hearer\", \"hearer\", \"heavysin\", \"heavysin\", \"heavysin\", \"hhb\", \"hhb\", \"hhb\", \"hhb\", \"hhb\", \"hhb\", \"hhb\", \"hhb\", \"hhb\", \"hhb\", \"hidden\", \"hidden\", \"hidden\", \"hidden\", \"hidden\", \"hidden\", \"hidden\", \"hidden\", \"hidden\", \"hidden\", \"hindmarsh\", \"hkk\", \"hkk\", \"hkk\", \"hkk\", \"hkk\", \"hkk\", \"hkk\", \"hkk\", \"hofsttter\", \"hofsttter\", \"hofsttter\", \"hofsttter\", \"hofsttter\", \"hofsttter\", \"hongjing\", \"hongjing\", \"hongjing\", \"hongjing\", \"hongjing\", \"hongjing\", \"hongjing\", \"hongjing\", \"hops\", \"however\", \"however\", \"however\", \"however\", \"however\", \"however\", \"however\", \"however\", \"however\", \"however\", \"hpnn\", \"hpnn\", \"hpnn\", \"hpnn\", \"hpnn\", \"hpnn\", \"hpnn\", \"hpnn\", \"hpnn\", \"hpnn\", \"hscrfs\", \"hscrfs\", \"hscrfs\", \"hscrfs\", \"hscrfs\", \"hscrfs\", \"hscrfs\", \"hscrfs\", \"humphreys\", \"humphreys\", \"humphreys\", \"humphreys\", \"humphreys\", \"humphreys\", \"humphreys\", \"humphreys\", \"hvc\", \"hvc\", \"hvc\", \"hvc\", \"hvc\", \"hvc\", \"hvc\", \"hvc\", \"hvc\", \"hvc\", \"hxgoal\", \"hxl\", \"hxl\", \"hxl\", \"hyperpolarized\", \"hyperpolarized\", \"hyperpolarized\", \"hyperpolarized\", \"ii\", \"ii\", \"ii\", \"ii\", \"ii\", \"ii\", \"ii\", \"ii\", \"ii\", \"ii\", \"iiab\", \"iifx\", \"image\", \"image\", \"image\", \"image\", \"image\", \"image\", \"image\", \"image\", \"image\", \"image\", \"images\", \"images\", \"images\", \"images\", \"images\", \"images\", \"images\", \"images\", \"images\", \"images\", \"iml\", \"iml\", \"iml\", \"iml\", \"iml\", \"iml\", \"iml\", \"iml\", \"iml\", \"iml\", \"implementation\", \"implementation\", \"implementation\", \"implementation\", \"implementation\", \"implementation\", \"implementation\", \"implementation\", \"implementation\", \"implementation\", \"infants\", \"infants\", \"infants\", \"infants\", \"infants\", \"infants\", \"infants\", \"infants\", \"infants\", \"infants\", \"inference\", \"inference\", \"inference\", \"inference\", \"inference\", \"inference\", \"inference\", \"inference\", \"inference\", \"inference\", \"infonnal\", \"information\", \"information\", \"information\", \"information\", \"information\", \"information\", \"information\", \"information\", \"information\", \"information\", \"input\", \"input\", \"input\", \"input\", \"input\", \"input\", \"input\", \"input\", \"input\", \"input\", \"insites\", \"insites\", \"intentional\", \"intentional\", \"intentional\", \"intentional\", \"intentional\", \"intentional\", \"intentional\", \"intentional\", \"intentional\", \"intentional\", \"inters\", \"inters\", \"inters\", \"inters\", \"interspike\", \"interspike\", \"interspike\", \"interspike\", \"interspike\", \"interspike\", \"interspike\", \"interspike\", \"interspike\", \"interspike\", \"interviews\", \"interviews\", \"interviews\", \"interviews\", \"interviews\", \"invex\", \"invex\", \"invex\", \"ioui\", \"ioui\", \"iout\", \"iout\", \"iout\", \"iout\", \"isb\", \"isb\", \"isb\", \"isb\", \"isb\", \"isb\", \"isb\", \"isb\", \"isb\", \"isb\", \"iwal\", \"iwal\", \"iwal\", \"iwal\", \"iwal\", \"iwal\", \"iwal\", \"iwf\", \"jhk\", \"jhk\", \"jhk\", \"jhk\", \"jhk\", \"jhk\", \"jhk\", \"jhk\", \"jhk\", \"jhk\", \"keh\", \"keh\", \"keh\", \"keh\", \"keh\", \"keh\", \"kernel\", \"kernel\", \"kernel\", \"kernel\", \"kernel\", \"kernel\", \"kernel\", \"kernel\", \"kernel\", \"kernel\", \"kgr\", \"kgr\", \"kgr\", \"kgr\", \"kgr\", \"kgr\", \"kgr\", \"kgr\", \"kgr\", \"kgr\", \"kgt\", \"kgt\", \"kgt\", \"kgt\", \"kgt\", \"kgt\", \"kgt\", \"kgt\", \"kgt\", \"kgt\", \"kliep\", \"kliep\", \"kliep\", \"kliep\", \"kliep\", \"kliep\", \"kliep\", \"kliep\", \"kliep\", \"kliep\", \"klmax\", \"klmax\", \"klmax\", \"klmax\", \"klmax\", \"klmax\", \"klmax\", \"kmm\", \"kmm\", \"kmm\", \"kmm\", \"kmm\", \"kmm\", \"kmm\", \"kmm\", \"kmm\", \"kmm\", \"koopman\", \"koopman\", \"koopman\", \"koopman\", \"koopman\", \"koopman\", \"koopman\", \"koopman\", \"koopman\", \"koopman\", \"krs\", \"krs\", \"krs\", \"krs\", \"krs\", \"krs\", \"kveh\", \"kveh\", \"kveh\", \"kveh\", \"kveh\", \"kveh\", \"kveh\", \"kvehg\", \"kvehg\", \"kvehg\", \"kvh\", \"kvh\", \"kvh\", \"kvh\", \"kvh\", \"kvh\", \"kvh\", \"kvh\", \"label\", \"label\", \"label\", \"label\", \"label\", \"label\", \"label\", \"label\", \"label\", \"label\", \"labeler\", \"labeler\", \"labeler\", \"labeler\", \"labeler\", \"labeler\", \"labeler\", \"labeler\", \"labeler\", \"labeler\", \"labelers\", \"labelers\", \"labelers\", \"labelers\", \"labelers\", \"labelers\", \"labelers\", \"labelers\", \"labelers\", \"labelers\", \"labels\", \"labels\", \"labels\", \"labels\", \"labels\", \"labels\", \"labels\", \"labels\", \"labels\", \"labels\", \"laplacian\", \"laplacian\", \"laplacian\", \"laplacian\", \"laplacian\", \"laplacian\", \"laplacian\", \"laplacian\", \"laplacian\", \"laplacian\", \"large\", \"large\", \"large\", \"large\", \"large\", \"large\", \"large\", \"large\", \"large\", \"large\", \"latent\", \"latent\", \"latent\", \"latent\", \"latent\", \"latent\", \"latent\", \"latent\", \"latent\", \"latent\", \"learnability\", \"learnability\", \"learnability\", \"learnability\", \"learnability\", \"learnability\", \"learnability\", \"learnability\", \"learnability\", \"learnability\", \"learning\", \"learning\", \"learning\", \"learning\", \"learning\", \"learning\", \"learning\", \"learning\", \"learning\", \"learning\", \"left\", \"left\", \"left\", \"left\", \"left\", \"left\", \"left\", \"left\", \"left\", \"left\", \"lehman\", \"lemasson\", \"lemasson\", \"lemasson\", \"lemasson\", \"lemasson\", \"lemma\", \"lemma\", \"lemma\", \"lemma\", \"lemma\", \"lemma\", \"lemma\", \"lemma\", \"lemma\", \"lemma\", \"let\", \"let\", \"let\", \"let\", \"let\", \"let\", \"let\", \"let\", \"let\", \"let\", \"lgt\", \"lifelong\", \"lifelong\", \"lifelong\", \"lifelong\", \"lifelong\", \"likelihood\", \"likelihood\", \"likelihood\", \"likelihood\", \"likelihood\", \"likelihood\", \"likelihood\", \"likelihood\", \"likelihood\", \"likelihood\", \"linear\", \"linear\", \"linear\", \"linear\", \"linear\", \"linear\", \"linear\", \"linear\", \"linear\", \"linear\", \"lior\", \"lior\", \"lior\", \"lior\", \"listnet\", \"listnet\", \"listnet\", \"listnet\", \"listnet\", \"listnet\", \"listnet\", \"listnet\", \"llinas\", \"llinas\", \"llinas\", \"llinas\", \"lmax\", \"lmax\", \"lmax\", \"lmax\", \"lmax\", \"lmax\", \"lmax\", \"lmax\", \"lmax\", \"lmax\", \"lmc\", \"lmc\", \"lmc\", \"lmc\", \"lmc\", \"lmc\", \"lmc\", \"lmc\", \"lmc\", \"lmc\", \"load\", \"load\", \"load\", \"load\", \"load\", \"load\", \"load\", \"load\", \"load\", \"load\", \"local\", \"local\", \"local\", \"local\", \"local\", \"local\", \"local\", \"local\", \"local\", \"local\", \"log\", \"log\", \"log\", \"log\", \"log\", \"log\", \"log\", \"log\", \"log\", \"log\", \"logreg\", \"logreg\", \"logreg\", \"logreg\", \"logreg\", \"logreg\", \"logreg\", \"logreg\", \"logreg\", \"logreg\", \"loop\", \"loop\", \"loop\", \"loop\", \"loop\", \"loop\", \"loop\", \"loop\", \"loop\", \"loop\", \"loss\", \"loss\", \"loss\", \"loss\", \"loss\", \"loss\", \"loss\", \"loss\", \"loss\", \"loss\", \"lpe\", \"lsbp\", \"lsbp\", \"lsbp\", \"lsbp\", \"lsbp\", \"lsbp\", \"lsbp\", \"lsbp\", \"lsbp\", \"lsbp\", \"lsm\", \"lsm\", \"lsm\", \"lsm\", \"lsm\", \"lsm\", \"lsm\", \"lsm\", \"lsm\", \"lsm\", \"lst\", \"lst\", \"lst\", \"lst\", \"lst\", \"lst\", \"lst\", \"lst\", \"lst\", \"lst\", \"lyapnov\", \"lyk\", \"lyk\", \"macbeth\", \"macbeth\", \"macbeth\", \"macbeth\", \"macbeth\", \"macbeth\", \"macbeth\", \"macbeth\", \"macbeth\", \"macbeth\", \"macintosh\", \"mahito\", \"mankowitz\", \"mankowitz\", \"mankowitz\", \"mankowitz\", \"mankowitz\", \"marder\", \"marder\", \"marder\", \"marder\", \"marder\", \"marder\", \"marder\", \"marder\", \"marder\", \"massarts\", \"massarts\", \"massarts\", \"massarts\", \"massed\", \"massed\", \"massed\", \"massed\", \"massed\", \"massed\", \"massed\", \"massed\", \"massed\", \"massed\", \"matching\", \"matching\", \"matching\", \"matching\", \"matching\", \"matching\", \"matching\", \"matching\", \"matching\", \"matching\", \"mathtauacil\", \"mathtauacil\", \"matrix\", \"matrix\", \"matrix\", \"matrix\", \"matrix\", \"matrix\", \"matrix\", \"matrix\", \"matrix\", \"matrix\", \"maxgg\", \"maxgg\", \"maxgg\", \"mbatch\", \"mbatch\", \"mbatch\", \"mbatch\", \"mbatch\", \"mbatch\", \"mbatch\", \"mbatch\", \"mbatch\", \"mbatch\", \"mcm\", \"mcm\", \"mcm\", \"mcm\", \"mcm\", \"mcm\", \"mcm\", \"mcm\", \"mcm\", \"mcm\", \"mcp\", \"mcp\", \"mcp\", \"mcp\", \"mcp\", \"mcp\", \"mcp\", \"mcp\", \"mcp\", \"mcp\", \"means\", \"means\", \"means\", \"means\", \"means\", \"means\", \"means\", \"means\", \"means\", \"means\", \"melodic\", \"melodic\", \"melodic\", \"melodic\", \"melodic\", \"melodic\", \"melodic\", \"melodic\", \"melodic\", \"melodic\", \"method\", \"method\", \"method\", \"method\", \"method\", \"method\", \"method\", \"method\", \"method\", \"method\", \"methods\", \"methods\", \"methods\", \"methods\", \"methods\", \"methods\", \"methods\", \"methods\", \"methods\", \"methods\", \"mf\", \"mf\", \"mf\", \"mf\", \"mf\", \"mf\", \"mf\", \"mf\", \"mf\", \"mf\", \"mfccs\", \"mfccs\", \"mfccs\", \"mfccs\", \"mfccs\", \"mfccs\", \"mfccs\", \"mfccs\", \"mfccs\", \"mfccs\", \"mii\", \"mii\", \"mii\", \"mii\", \"mii\", \"mii\", \"mii\", \"mii\", \"mii\", \"mii\", \"milsd\", \"milsd\", \"milsd\", \"milsd\", \"milsd\", \"milsd\", \"milsd\", \"milsd\", \"milsd\", \"milsd\", \"mise\", \"mise\", \"misspecified\", \"misspecified\", \"misspecified\", \"misspecified\", \"misspecified\", \"misspecified\", \"misspecified\", \"misspecified\", \"misspecified\", \"misspecified\", \"mixer\", \"mixer\", \"mixer\", \"mixer\", \"mixer\", \"mixer\", \"mixer\", \"mixer\", \"mixer\", \"mixer\", \"mkd\", \"mkd\", \"mkd\", \"mkd\", \"mkd\", \"mkd\", \"mkd\", \"mkl\", \"mkl\", \"mkl\", \"mkl\", \"mkl\", \"mkl\", \"mkl\", \"mkl\", \"mkl\", \"mkl\", \"mmh\", \"mmh\", \"mmh\", \"mmh\", \"mmh\", \"mmh\", \"mmh\", \"mmh\", \"mmh\", \"mmh\", \"mmnn\", \"mmnn\", \"mmnn\", \"mmnn\", \"mmnn\", \"mns\", \"mns\", \"mns\", \"mns\", \"mns\", \"mns\", \"model\", \"model\", \"model\", \"model\", \"model\", \"model\", \"model\", \"model\", \"model\", \"model\", \"models\", \"models\", \"models\", \"models\", \"models\", \"models\", \"models\", \"models\", \"models\", \"models\", \"morrone\", \"morrone\", \"motion\", \"motion\", \"motion\", \"motion\", \"motion\", \"motion\", \"motion\", \"motion\", \"motion\", \"motion\", \"mstd\", \"mstd\", \"mstd\", \"mstd\", \"mstd\", \"mstd\", \"mtrl\", \"mtrl\", \"mtrl\", \"mtrl\", \"multi\", \"multi\", \"multi\", \"multi\", \"multi\", \"multi\", \"multi\", \"multi\", \"multi\", \"multi\", \"mutag\", \"mutag\", \"mutag\", \"mxn\", \"nbd\", \"nbd\", \"nbd\", \"nbd\", \"nbd\", \"nbd\", \"nbd\", \"nbd\", \"nbd\", \"nbd\", \"nbnb\", \"nbnb\", \"nbnb\", \"nbnb\", \"nbnb\", \"nbnb\", \"nbnb\", \"nbnb\", \"nbnb\", \"nbnb\", \"nci\", \"nci\", \"nci\", \"nci\", \"nci\", \"nci\", \"nci\", \"nci\", \"ndb\", \"ndb\", \"ndb\", \"ndb\", \"ndb\", \"ndb\", \"ndb\", \"ndb\", \"ndb\", \"ndb\", \"nedelec\", \"nedelec\", \"nedelec\", \"nedelec\", \"nedelec\", \"network\", \"network\", \"network\", \"network\", \"network\", \"network\", \"network\", \"network\", \"network\", \"network\", \"networks\", \"networks\", \"networks\", \"networks\", \"networks\", \"networks\", \"networks\", \"networks\", \"networks\", \"networks\", \"neural\", \"neural\", \"neural\", \"neural\", \"neural\", \"neural\", \"neural\", \"neural\", \"neural\", \"neural\", \"neuron\", \"neuron\", \"neuron\", \"neuron\", \"neuron\", \"neuron\", \"neuron\", \"neuron\", \"neuron\", \"neuron\", \"neurons\", \"neurons\", \"neurons\", \"neurons\", \"neurons\", \"neurons\", \"neurons\", \"neurons\", \"neurons\", \"neurons\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"nhi\", \"nhi\", \"nhi\", \"nhi\", \"nhi\", \"nhi\", \"nhi\", \"nhi\", \"nhi\", \"nhi\", \"nishida\", \"nkkn\", \"nline\", \"nline\", \"nline\", \"nline\", \"nline\", \"nline\", \"nline\", \"nline\", \"nline\", \"nline\", \"noboru\", \"node\", \"node\", \"node\", \"node\", \"node\", \"node\", \"node\", \"node\", \"node\", \"node\", \"noise\", \"noise\", \"noise\", \"noise\", \"noise\", \"noise\", \"noise\", \"noise\", \"noise\", \"noise\", \"non\", \"non\", \"non\", \"non\", \"non\", \"non\", \"non\", \"non\", \"non\", \"non\", \"note\", \"note\", \"note\", \"note\", \"note\", \"note\", \"note\", \"note\", \"note\", \"note\", \"noulas\", \"noulas\", \"noulas\", \"noulas\", \"npe\", \"npe\", \"npe\", \"npn\", \"npn\", \"npn\", \"npn\", \"npn\", \"npn\", \"npn\", \"npn\", \"npn\", \"npn\", \"nq\", \"nq\", \"nq\", \"nq\", \"nq\", \"nq\", \"nq\", \"nq\", \"nq\", \"nq\", \"nrsfm\", \"nrsfm\", \"nrsfm\", \"nrsfm\", \"nrsfm\", \"nrsfm\", \"nrsfm\", \"nrsfm\", \"nrsfm\", \"nrsfm\", \"nsr\", \"nsr\", \"nsr\", \"nsr\", \"nsr\", \"nsr\", \"nsr\", \"nsr\", \"nsr\", \"nsr\", \"ntr\", \"ntr\", \"ntr\", \"ntr\", \"ntr\", \"ntr\", \"ntr\", \"ntr\", \"ntr\", \"ntr\", \"number\", \"number\", \"number\", \"number\", \"number\", \"number\", \"number\", \"number\", \"number\", \"number\", \"objective\", \"objective\", \"objective\", \"objective\", \"objective\", \"objective\", \"objective\", \"objective\", \"objective\", \"objective\", \"ocularity\", \"ocularity\", \"ocularity\", \"ocularity\", \"ocularity\", \"ocularity\", \"ocularity\", \"ocularity\", \"ocularity\", \"odh\", \"odh\", \"odh\", \"odh\", \"odh\", \"odh\", \"odh\", \"odh\", \"odh\", \"ohi\", \"okl\", \"okl\", \"okl\", \"okl\", \"okl\", \"okl\", \"okl\", \"okm\", \"olivary\", \"one\", \"one\", \"one\", \"one\", \"one\", \"one\", \"one\", \"one\", \"one\", \"one\", \"online\", \"online\", \"online\", \"online\", \"online\", \"online\", \"online\", \"online\", \"online\", \"online\", \"optimal\", \"optimal\", \"optimal\", \"optimal\", \"optimal\", \"optimal\", \"optimal\", \"optimal\", \"optimal\", \"optimal\", \"optimization\", \"optimization\", \"optimization\", \"optimization\", \"optimization\", \"optimization\", \"optimization\", \"optimization\", \"optimization\", \"optimization\", \"optspace\", \"optspace\", \"optspace\", \"optspace\", \"optspace\", \"optspace\", \"order\", \"order\", \"order\", \"order\", \"order\", \"order\", \"order\", \"order\", \"order\", \"order\", \"otherwise\", \"otherwise\", \"otherwise\", \"otherwise\", \"otherwise\", \"otherwise\", \"otherwise\", \"otherwise\", \"otherwise\", \"otherwise\", \"paper\", \"paper\", \"paper\", \"paper\", \"paper\", \"paper\", \"paper\", \"paper\", \"paper\", \"paper\", \"parameter\", \"parameter\", \"parameter\", \"parameter\", \"parameter\", \"parameter\", \"parameter\", \"parameter\", \"parameter\", \"parameter\", \"parameters\", \"parameters\", \"parameters\", \"parameters\", \"parameters\", \"parameters\", \"parameters\", \"parameters\", \"parameters\", \"parameters\", \"party\", \"party\", \"party\", \"party\", \"party\", \"party\", \"party\", \"party\", \"party\", \"party\", \"parzen\", \"parzen\", \"parzen\", \"parzen\", \"parzen\", \"parzen\", \"parzen\", \"parzen\", \"parzen\", \"parzen\", \"pbayes\", \"pbayes\", \"pbayes\", \"pbayes\", \"pbayes\", \"pbayes\", \"pbayes\", \"pbayes\", \"pcs\", \"pcs\", \"pcs\", \"pcs\", \"pcs\", \"pcs\", \"pcs\", \"pcs\", \"pcs\", \"pcs\", \"peg\", \"peg\", \"peg\", \"peg\", \"peg\", \"peg\", \"peg\", \"peg\", \"peg\", \"peg\", \"performance\", \"performance\", \"performance\", \"performance\", \"performance\", \"performance\", \"performance\", \"performance\", \"performance\", \"performance\", \"pert\", \"pert\", \"pert\", \"pert\", \"pert\", \"pert\", \"pert\", \"pert\", \"pert\", \"perts\", \"perts\", \"perts\", \"perts\", \"perts\", \"perts\", \"perts\", \"perturbator\", \"pgas\", \"pgas\", \"pgas\", \"pgas\", \"pgas\", \"pgas\", \"pgas\", \"pgas\", \"pgas\", \"pgas\", \"pgehler\", \"pgehler\", \"phi\", \"phi\", \"phi\", \"phi\", \"phi\", \"phi\", \"phi\", \"phi\", \"phi\", \"phi\", \"phmm\", \"phmm\", \"phmm\", \"phmm\", \"phmm\", \"phmm\", \"phmm\", \"phmm\", \"phmm\", \"phmm\", \"pickard\", \"pickard\", \"pickard\", \"pickard\", \"pickard\", \"pickard\", \"pickard\", \"pieraccini\", \"pieraccini\", \"pieraccini\", \"pieraccini\", \"pieraccini\", \"pieraccini\", \"pieraccini\", \"pieraccini\", \"pieraccini\", \"pieraccini\", \"pkq\", \"pkq\", \"pkq\", \"pkq\", \"pkq\", \"pkq\", \"pkq\", \"pkq\", \"pkq\", \"pkq\", \"pmax\", \"pmax\", \"pmax\", \"pmax\", \"pmax\", \"pmax\", \"pmax\", \"pmax\", \"pmax\", \"pntr\", \"pntr\", \"pntr\", \"poedges\", \"poedges\", \"poedges\", \"poedges\", \"poedges\", \"poedges\", \"poedges\", \"poedges\", \"poedges\", \"points\", \"points\", \"points\", \"points\", \"points\", \"points\", \"points\", \"points\", \"points\", \"points\", \"policy\", \"policy\", \"policy\", \"policy\", \"policy\", \"policy\", \"policy\", \"policy\", \"policy\", \"policy\", \"postsynaptic\", \"postsynaptic\", \"postsynaptic\", \"postsynaptic\", \"postsynaptic\", \"postsynaptic\", \"postsynaptic\", \"postsynaptic\", \"postsynaptic\", \"postsynaptic\", \"predominant\", \"predominant\", \"predominant\", \"predominant\", \"predominant\", \"predominant\", \"predominant\", \"predominant\", \"predominant\", \"predominant\", \"prestojst\", \"presynaptic\", \"presynaptic\", \"presynaptic\", \"presynaptic\", \"presynaptic\", \"presynaptic\", \"presynaptic\", \"presynaptic\", \"presynaptic\", \"presynaptic\", \"priigel\", \"priigel\", \"priigel\", \"priigel\", \"priigel\", \"priigel\", \"prior\", \"prior\", \"prior\", \"prior\", \"prior\", \"prior\", \"prior\", \"prior\", \"prior\", \"prior\", \"probability\", \"probability\", \"probability\", \"probability\", \"probability\", \"probability\", \"probability\", \"probability\", \"probability\", \"probability\", \"probing\", \"probing\", \"probing\", \"probing\", \"probing\", \"probing\", \"probing\", \"probing\", \"probing\", \"probing\", \"problem\", \"problem\", \"problem\", \"problem\", \"problem\", \"problem\", \"problem\", \"problem\", \"problem\", \"problem\", \"proof\", \"proof\", \"proof\", \"proof\", \"proof\", \"proof\", \"proof\", \"proof\", \"proof\", \"proof\", \"psp\", \"psp\", \"psp\", \"psp\", \"psp\", \"psp\", \"psp\", \"psp\", \"psp\", \"psp\", \"psx\", \"psx\", \"psx\", \"psx\", \"psx\", \"psx\", \"pte\", \"pte\", \"pte\", \"pte\", \"pte\", \"pte\", \"pte\", \"pte\", \"pyloric\", \"pyloric\", \"pyloric\", \"pyloric\", \"pyloric\", \"pyloric\", \"pym\", \"pym\", \"pym\", \"pym\", \"pym\", \"pym\", \"pym\", \"pym\", \"qmf\", \"qos\", \"qos\", \"qos\", \"qos\", \"qos\", \"qos\", \"qos\", \"qos\", \"qos\", \"qos\", \"random\", \"random\", \"random\", \"random\", \"random\", \"random\", \"random\", \"random\", \"random\", \"random\", \"rank\", \"rank\", \"rank\", \"rank\", \"rank\", \"rank\", \"rank\", \"rank\", \"rank\", \"rank\", \"ranknet\", \"ranknet\", \"ranknet\", \"ranknet\", \"ranknet\", \"ranknet\", \"ranknet\", \"ranknet\", \"ranknet\", \"ranknet\", \"raslie\", \"raslie\", \"raslie\", \"raslie\", \"raslie\", \"raslie\", \"raslie\", \"raslie\", \"raslie\", \"raslie\", \"rate\", \"rate\", \"rate\", \"rate\", \"rate\", \"rate\", \"rate\", \"rate\", \"rate\", \"rate\", \"razaviyayn\", \"razaviyayn\", \"razaviyayn\", \"rbfnn\", \"rbfnn\", \"recirculation\", \"recirculation\", \"recirculation\", \"recirculation\", \"recirculation\", \"recirculation\", \"recirculation\", \"recirculation\", \"recirculation\", \"recirculation\", \"recognition\", \"recognition\", \"recognition\", \"recognition\", \"recognition\", \"recognition\", \"recognition\", \"recognition\", \"recognition\", \"recognition\", \"regression\", \"regression\", \"regression\", \"regression\", \"regression\", \"regression\", \"regression\", \"regression\", \"regression\", \"regression\", \"rekl\", \"rekl\", \"rekl\", \"rekl\", \"rekl\", \"rekl\", \"rekl\", \"rekl\", \"rekl\", \"remodeling\", \"remodeling\", \"represent\", \"represent\", \"represent\", \"represent\", \"represent\", \"represent\", \"represent\", \"represent\", \"represent\", \"represent\", \"representation\", \"representation\", \"representation\", \"representation\", \"representation\", \"representation\", \"representation\", \"representation\", \"representation\", \"representation\", \"rescaledexp\", \"rescaledexp\", \"rescaledexp\", \"rescaledexp\", \"rescaledexp\", \"rescaledexp\", \"rescaledexp\", \"rescaledexp\", \"rescaledexp\", \"rescaledexp\", \"results\", \"results\", \"results\", \"results\", \"results\", \"results\", \"results\", \"results\", \"results\", \"results\", \"retinotopy\", \"retinotopy\", \"retinotopy\", \"retinotopy\", \"retinotopy\", \"retinotopy\", \"retinotopy\", \"retinotopy\", \"retinotopy\", \"retinotopy\", \"risks\", \"risks\", \"risks\", \"risks\", \"risks\", \"risks\", \"risks\", \"risks\", \"risks\", \"risks\", \"routing\", \"routing\", \"routing\", \"routing\", \"routing\", \"routing\", \"routing\", \"routing\", \"routing\", \"routing\", \"rtcr\", \"rtcrd\", \"rtcrdgtgt\", \"rule\", \"rule\", \"rule\", \"rule\", \"rule\", \"rule\", \"rule\", \"rule\", \"rule\", \"rule\", \"rva\", \"saliency\", \"saliency\", \"saliency\", \"saliency\", \"saliency\", \"saliency\", \"saliency\", \"saliency\", \"saliency\", \"saliency\", \"salinity\", \"salinity\", \"salinity\", \"salinity\", \"salinity\", \"salinity\", \"salinity\", \"salinity\", \"salinity\", \"salinity\", \"sample\", \"sample\", \"sample\", \"sample\", \"sample\", \"sample\", \"sample\", \"sample\", \"sample\", \"sample\", \"samples\", \"samples\", \"samples\", \"samples\", \"samples\", \"samples\", \"samples\", \"samples\", \"samples\", \"samples\", \"scheme\", \"scheme\", \"scheme\", \"scheme\", \"scheme\", \"scheme\", \"scheme\", \"scheme\", \"scheme\", \"scheme\", \"score\", \"score\", \"score\", \"score\", \"score\", \"score\", \"score\", \"score\", \"score\", \"score\", \"search\", \"search\", \"search\", \"search\", \"search\", \"search\", \"search\", \"search\", \"search\", \"search\", \"second\", \"second\", \"second\", \"second\", \"second\", \"second\", \"second\", \"second\", \"second\", \"second\", \"section\", \"section\", \"section\", \"section\", \"section\", \"section\", \"section\", \"section\", \"section\", \"section\", \"see\", \"see\", \"see\", \"see\", \"see\", \"see\", \"see\", \"see\", \"see\", \"see\", \"segmented\", \"segmented\", \"segmented\", \"segmented\", \"segmented\", \"segmented\", \"segmented\", \"segmented\", \"segmented\", \"segmented\", \"sentence\", \"sentence\", \"sentence\", \"sentence\", \"sentence\", \"sentence\", \"sentence\", \"sentence\", \"sentence\", \"sentence\", \"sesame\", \"sesame\", \"sesame\", \"sesame\", \"sesame\", \"sesame\", \"sesame\", \"sesame\", \"sesame\", \"set\", \"set\", \"set\", \"set\", \"set\", \"set\", \"set\", \"set\", \"set\", \"set\", \"sfa\", \"sfa\", \"sfa\", \"sfa\", \"sfa\", \"sfa\", \"sfa\", \"sfa\", \"sfa\", \"sfa\", \"show\", \"show\", \"show\", \"show\", \"show\", \"show\", \"show\", \"show\", \"show\", \"show\", \"shown\", \"shown\", \"shown\", \"shown\", \"shown\", \"shown\", \"shown\", \"shown\", \"shown\", \"shown\", \"silverstein\", \"similar\", \"similar\", \"similar\", \"similar\", \"similar\", \"similar\", \"similar\", \"similar\", \"similar\", \"similar\", \"since\", \"since\", \"since\", \"since\", \"since\", \"since\", \"since\", \"since\", \"since\", \"since\", \"single\", \"single\", \"single\", \"single\", \"single\", \"single\", \"single\", \"single\", \"single\", \"single\", \"sirp\", \"sirp\", \"sirp\", \"sirp\", \"sirp\", \"sirp\", \"sirp\", \"size\", \"size\", \"size\", \"size\", \"size\", \"size\", \"size\", \"size\", \"size\", \"size\", \"sjv\", \"skill\", \"skill\", \"skill\", \"skill\", \"skill\", \"skill\", \"skill\", \"skill\", \"skill\", \"skill\", \"skills\", \"skills\", \"skills\", \"skills\", \"skills\", \"skills\", \"skills\", \"skills\", \"skills\", \"skills\", \"sleep\", \"sleep\", \"sleep\", \"sleep\", \"sleep\", \"sleep\", \"sleep\", \"sleep\", \"sleep\", \"sleep\", \"sleeping\", \"sleeping\", \"sleeping\", \"sleeping\", \"sleeping\", \"sleeping\", \"sleeping\", \"sleeping\", \"sleeping\", \"sleeping\", \"smmfcc\", \"smmfcc\", \"smmfcc\", \"smmfcc\", \"smrsdn\", \"smrsdn\", \"smrsdn\", \"smrsdn\", \"smrsdn\", \"snq\", \"snq\", \"snq\", \"snq\", \"snq\", \"snq\", \"snq\", \"snq\", \"snq\", \"softening\", \"softening\", \"softening\", \"softening\", \"softening\", \"softening\", \"softening\", \"space\", \"space\", \"space\", \"space\", \"space\", \"space\", \"space\", \"space\", \"space\", \"space\", \"speaker\", \"speaker\", \"speaker\", \"speaker\", \"speaker\", \"speaker\", \"speaker\", \"speaker\", \"speaker\", \"speaker\", \"speech\", \"speech\", \"speech\", \"speech\", \"speech\", \"speech\", \"speech\", \"speech\", \"speech\", \"speech\", \"spike\", \"spike\", \"spike\", \"spike\", \"spike\", \"spike\", \"spike\", \"spike\", \"spike\", \"spike\", \"spk\", \"spk\", \"spk\", \"spk\", \"spk\", \"spk\", \"spk\", \"spk\", \"spk\", \"spk\", \"splines\", \"splines\", \"splines\", \"splines\", \"splines\", \"splines\", \"splines\", \"splines\", \"splines\", \"splines\", \"spotter\", \"spotter\", \"spotter\", \"spotter\", \"spotter\", \"spotter\", \"spotter\", \"spotter\", \"spotters\", \"spotters\", \"sprite\", \"sprite\", \"sprite\", \"sprite\", \"sprite\", \"sprite\", \"sprite\", \"sprite\", \"sprite\", \"sprite\", \"sps\", \"sps\", \"sps\", \"sps\", \"sps\", \"sps\", \"sps\", \"sps\", \"sps\", \"sps\", \"standard\", \"standard\", \"standard\", \"standard\", \"standard\", \"standard\", \"standard\", \"standard\", \"standard\", \"standard\", \"stasheff\", \"stasheff\", \"state\", \"state\", \"state\", \"state\", \"state\", \"state\", \"state\", \"state\", \"state\", \"state\", \"statuclaedu\", \"statuclaedu\", \"statuclaedu\", \"statuclaedu\", \"statuclaedu\", \"step\", \"step\", \"step\", \"step\", \"step\", \"step\", \"step\", \"step\", \"step\", \"step\", \"stimuli\", \"stimuli\", \"stimuli\", \"stimuli\", \"stimuli\", \"stimuli\", \"stimuli\", \"stimuli\", \"stimuli\", \"stimuli\", \"stimulus\", \"stimulus\", \"stimulus\", \"stimulus\", \"stimulus\", \"stimulus\", \"stimulus\", \"stimulus\", \"stimulus\", \"stimulus\", \"stochastic\", \"stochastic\", \"stochastic\", \"stochastic\", \"stochastic\", \"stochastic\", \"stochastic\", \"stochastic\", \"stochastic\", \"stochastic\", \"stomatogastric\", \"stomatogastric\", \"stomatogastric\", \"stomatogastric\", \"stomatogastric\", \"stomatogastric\", \"stomatogastric\", \"stomatogastric\", \"strata\", \"strata\", \"strata\", \"strata\", \"strata\", \"strata\", \"strata\", \"strata\", \"strata\", \"strata\", \"stratication\", \"stratication\", \"stratication\", \"stratication\", \"stratication\", \"stratication\", \"stratication\", \"stratication\", \"stratication\", \"straties\", \"straties\", \"stratum\", \"stratum\", \"stratum\", \"stratum\", \"stratum\", \"stratum\", \"stratum\", \"stratum\", \"stratum\", \"stratum\", \"striker\", \"striker\", \"striker\", \"striker\", \"striker\", \"striker\", \"striker\", \"structure\", \"structure\", \"structure\", \"structure\", \"structure\", \"structure\", \"structure\", \"structure\", \"structure\", \"structure\", \"studios\", \"studios\", \"style\", \"style\", \"style\", \"style\", \"style\", \"style\", \"style\", \"style\", \"style\", \"style\", \"styles\", \"styles\", \"styles\", \"styles\", \"styles\", \"styles\", \"styles\", \"styles\", \"styles\", \"styles\", \"subnets\", \"subnets\", \"subnets\", \"subnets\", \"subnets\", \"subnets\", \"subnets\", \"subnets\", \"subnets\", \"subnets\", \"sum\", \"sum\", \"sum\", \"sum\", \"sum\", \"sum\", \"sum\", \"sum\", \"sum\", \"sum\", \"suppb\", \"suppb\", \"svar\", \"svar\", \"svar\", \"svar\", \"svar\", \"svar\", \"svar\", \"svar\", \"svt\", \"svt\", \"svt\", \"svt\", \"svt\", \"svt\", \"svt\", \"svt\", \"svt\", \"svt\", \"syllable\", \"syllable\", \"syllable\", \"syllable\", \"syllable\", \"syllable\", \"syllable\", \"syllable\", \"syllable\", \"syllable\", \"symplectic\", \"symplectic\", \"symplectic\", \"symplectic\", \"symplectic\", \"symplectic\", \"symplectic\", \"symplectic\", \"symplectic\", \"synaptic\", \"synaptic\", \"synaptic\", \"synaptic\", \"synaptic\", \"synaptic\", \"synaptic\", \"synaptic\", \"synaptic\", \"synaptic\", \"synunetry\", \"synunetry\", \"syrinx\", \"syrinx\", \"syrinx\", \"syrinx\", \"syrinx\", \"syrinx\", \"syrinx\", \"syrinx\", \"syrinx\", \"system\", \"system\", \"system\", \"system\", \"system\", \"system\", \"system\", \"system\", \"system\", \"system\", \"systems\", \"systems\", \"systems\", \"systems\", \"systems\", \"systems\", \"systems\", \"systems\", \"systems\", \"systems\", \"tachycardia\", \"tachycardia\", \"tachycardia\", \"tachycardia\", \"talking\", \"talking\", \"talking\", \"talking\", \"talking\", \"talking\", \"talking\", \"talking\", \"talking\", \"talking\", \"tam\", \"tam\", \"tam\", \"tam\", \"tam\", \"tam\", \"tam\", \"tam\", \"teams\", \"teams\", \"teams\", \"teams\", \"teams\", \"teams\", \"teams\", \"teams\", \"teams\", \"teams\", \"teas\", \"teas\", \"teas\", \"teas\", \"teas\", \"tensor\", \"tensor\", \"tensor\", \"tensor\", \"tensor\", \"tensor\", \"tensor\", \"tensor\", \"tensor\", \"tensor\", \"test\", \"test\", \"test\", \"test\", \"test\", \"test\", \"test\", \"test\", \"test\", \"test\", \"thus\", \"thus\", \"thus\", \"thus\", \"thus\", \"thus\", \"thus\", \"thus\", \"thus\", \"thus\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"tmax\", \"tmax\", \"tmax\", \"tmax\", \"tmax\", \"tmax\", \"tmax\", \"tmax\", \"tmax\", \"tmax\", \"topic\", \"topic\", \"topic\", \"topic\", \"topic\", \"topic\", \"topic\", \"topic\", \"topic\", \"topic\", \"tottering\", \"tottering\", \"tottering\", \"traffic\", \"traffic\", \"traffic\", \"traffic\", \"traffic\", \"traffic\", \"traffic\", \"traffic\", \"traffic\", \"traffic\", \"training\", \"training\", \"training\", \"training\", \"training\", \"training\", \"training\", \"training\", \"training\", \"training\", \"transcript\", \"transcript\", \"transcript\", \"transcript\", \"transcript\", \"transcript\", \"transcript\", \"transcript\", \"transcript\", \"transcript\", \"tree\", \"tree\", \"tree\", \"tree\", \"tree\", \"tree\", \"tree\", \"tree\", \"tree\", \"tree\", \"trueskill\", \"trueskill\", \"trueskill\", \"trueskill\", \"trueskill\", \"trueskill\", \"trueskill\", \"trueskill\", \"trueskill\", \"trueskill\", \"tsoi\", \"tsoi\", \"tsoi\", \"tsoi\", \"tsoi\", \"tsoi\", \"tsoi\", \"tsoi\", \"tsoi\", \"tsoi\", \"tubes\", \"tubes\", \"two\", \"two\", \"two\", \"two\", \"two\", \"two\", \"two\", \"two\", \"two\", \"two\", \"uhi\", \"uhi\", \"uhi\", \"uhi\", \"uhi\", \"uhi\", \"uhi\", \"uit\", \"uit\", \"uit\", \"uit\", \"uit\", \"uit\", \"uit\", \"ulse\", \"ulse\", \"ulse\", \"ulse\", \"ulse\", \"ulse\", \"ulse\", \"ulse\", \"ulse\", \"ulse\", \"ulsif\", \"ulsif\", \"ulsif\", \"ulsif\", \"ulsif\", \"ulsif\", \"ulsif\", \"ulsif\", \"ulsif\", \"ulsif\", \"uluru\", \"uluru\", \"uluru\", \"uluru\", \"uluru\", \"uluru\", \"uluru\", \"uluru\", \"uluru\", \"units\", \"units\", \"units\", \"units\", \"units\", \"units\", \"units\", \"units\", \"units\", \"units\", \"used\", \"used\", \"used\", \"used\", \"used\", \"used\", \"used\", \"used\", \"used\", \"used\", \"using\", \"using\", \"using\", \"using\", \"using\", \"using\", \"using\", \"using\", \"using\", \"using\", \"ut\", \"ut\", \"ut\", \"ut\", \"ut\", \"ut\", \"ut\", \"ut\", \"ut\", \"ut\", \"uv\", \"uv\", \"uv\", \"uv\", \"uv\", \"uv\", \"uv\", \"uv\", \"uv\", \"uv\", \"value\", \"value\", \"value\", \"value\", \"value\", \"value\", \"value\", \"value\", \"value\", \"value\", \"values\", \"values\", \"values\", \"values\", \"values\", \"values\", \"values\", \"values\", \"values\", \"values\", \"variabilities\", \"variabilities\", \"vector\", \"vector\", \"vector\", \"vector\", \"vector\", \"vector\", \"vector\", \"vector\", \"vector\", \"vector\", \"ventricular\", \"ventricular\", \"ventricular\", \"ventricular\", \"ventricular\", \"ventricular\", \"ventricular\", \"ventricular\", \"ventricular\", \"ventricular\", \"verification\", \"verification\", \"verification\", \"verification\", \"verification\", \"verification\", \"verification\", \"verification\", \"verification\", \"verification\", \"vext\", \"vext\", \"vext\", \"vext\", \"vext\", \"vext\", \"vext\", \"vext\", \"vext\", \"vext\", \"vgw\", \"vgw\", \"vgw\", \"visible\", \"visible\", \"visible\", \"visible\", \"visible\", \"visible\", \"visible\", \"visible\", \"visible\", \"visible\", \"vkm\", \"vkm\", \"vkm\", \"vkm\", \"vkm\", \"vkm\", \"vkm\", \"vkm\", \"vkm\", \"vkm\", \"vlr\", \"vlr\", \"vlr\", \"vlr\", \"vmt\", \"vmt\", \"vmt\", \"vmt\", \"vmt\", \"vmt\", \"vmt\", \"vmt\", \"vmt\", \"vmt\", \"volterra\", \"volterra\", \"volterra\", \"volterra\", \"volterra\", \"volterra\", \"volterra\", \"volterra\", \"volterra\", \"volterra\", \"wake\", \"wake\", \"wake\", \"wake\", \"wake\", \"wake\", \"wake\", \"wake\", \"wake\", \"wake\", \"walk\", \"walk\", \"walk\", \"walk\", \"walk\", \"walk\", \"walk\", \"walk\", \"walk\", \"walk\", \"war\", \"war\", \"war\", \"war\", \"war\", \"war\", \"war\", \"war\", \"war\", \"war\", \"wasserstein\", \"wasserstein\", \"wasserstein\", \"wasserstein\", \"wasserstein\", \"wasserstein\", \"wasserstein\", \"wasserstein\", \"wasserstein\", \"wasserstein\", \"wavelet\", \"wavelet\", \"wavelet\", \"wavelet\", \"wavelet\", \"wavelet\", \"wavelet\", \"wavelet\", \"wavelet\", \"wavelet\", \"wcorrect\", \"wcorrect\", \"wcorrect\", \"wedge\", \"wedge\", \"wedge\", \"wedge\", \"wedge\", \"wedge\", \"wedge\", \"wedge\", \"weight\", \"weight\", \"weight\", \"weight\", \"weight\", \"weight\", \"weight\", \"weight\", \"weight\", \"weight\", \"weights\", \"weights\", \"weights\", \"weights\", \"weights\", \"weights\", \"weights\", \"weights\", \"weights\", \"weights\", \"weisfeiler\", \"weisfeiler\", \"weisfeiler\", \"well\", \"well\", \"well\", \"well\", \"well\", \"well\", \"well\", \"well\", \"well\", \"well\", \"wildlife\", \"wildlife\", \"wildlife\", \"wildlife\", \"wildlife\", \"wildlife\", \"wildlife\", \"wildlife\", \"wildlife\", \"wildlife\", \"wiles\", \"wiles\", \"wiles\", \"wiles\", \"wiles\", \"wiles\", \"wiles\", \"wiles\", \"wiles\", \"wiles\", \"wilpon\", \"wilpon\", \"wilpon\", \"wkr\", \"wkr\", \"wkr\", \"wkr\", \"wkr\", \"wkr\", \"wkr\", \"wkr\", \"wkr\", \"wkr\", \"wlrmf\", \"wlrmf\", \"wlrmf\", \"wlrmf\", \"wlrmf\", \"wlrmf\", \"wlrmf\", \"wlrmf\", \"wlrmf\", \"wlrmf\", \"wts\", \"wts\", \"wts\", \"wts\", \"wts\", \"wts\", \"xbox\", \"xbox\", \"xbox\", \"xbox\", \"xbox\", \"xbox\", \"xbox\", \"xbox\", \"xi\", \"xi\", \"xi\", \"xi\", \"xi\", \"xi\", \"xi\", \"xi\", \"xi\", \"xi\", \"xk\", \"xk\", \"xk\", \"xk\", \"xk\", \"xk\", \"xk\", \"xk\", \"xk\", \"xk\", \"xki\", \"xki\", \"xki\", \"xki\", \"xki\", \"xki\", \"xki\", \"xt\", \"xt\", \"xt\", \"xt\", \"xt\", \"xt\", \"xt\", \"xt\", \"xt\", \"xt\", \"xte\", \"xte\", \"xte\", \"xte\", \"xte\", \"xte\", \"xte\", \"xte\", \"yagent\", \"yarom\", \"yarom\", \"yarom\", \"yarom\", \"ybt\", \"ybt\", \"ybt\", \"ybt\", \"ybt\", \"ybt\", \"ybt\", \"ybt\", \"ybt\", \"ybt\", \"ygt\", \"ytrain\", \"ytrain\", \"ytrain\", \"ytrain\", \"ytrain\", \"ytrain\", \"ytrain\", \"ytrain\", \"zell\", \"zell\", \"zell\", \"zell\", \"zell\", \"zell\", \"zell\", \"zell\", \"zell\", \"zell\", \"zloop\", \"zloop\", \"zloop\", \"zloop\", \"zloop\", \"zloop\", \"zloop\", \"zloop\", \"zloop\", \"zloop\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [3, 10, 6, 4, 2, 7, 8, 5, 1, 9]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el1850019598491286725129869264\", ldavis_el1850019598491286725129869264_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://d3js.org/d3.v5\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el1850019598491286725129869264\", ldavis_el1850019598491286725129869264_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://d3js.org/d3.v5.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el1850019598491286725129869264\", ldavis_el1850019598491286725129869264_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "PreparedData(topic_coordinates=              x         y  topics  cluster       Freq\n",
       "topic                                                \n",
       "2      0.007118  0.000573       1        1  17.344911\n",
       "9      0.001392 -0.001816       2        1  13.200734\n",
       "5      0.000580 -0.001830       3        1  11.298552\n",
       "3     -0.000324 -0.000664       4        1  10.863193\n",
       "1      0.001140  0.003767       5        1  10.289710\n",
       "6      0.000148 -0.001493       6        1   8.498948\n",
       "7     -0.004713  0.003562       7        1   8.138272\n",
       "4     -0.000822  0.002754       8        1   8.010393\n",
       "0     -0.001722  0.000664       9        1   6.649390\n",
       "8     -0.002794 -0.005517      10        1   5.705898, topic_info=          Term          Freq         Total Category  logprob  loglift\n",
       "485      model  13848.000000  13848.000000  Default  30.0000  30.0000\n",
       "426   learning  13966.000000  13966.000000  Default  29.0000  29.0000\n",
       "729        set   9619.000000   9619.000000  Default  28.0000  28.0000\n",
       "169       data  12580.000000  12580.000000  Default  27.0000  27.0000\n",
       "319   function   9043.000000   9043.000000  Default  26.0000  26.0000\n",
       "...        ...           ...           ...      ...      ...      ...\n",
       "1309    neural    296.569315   5347.990485  Topic10  -6.0881  -0.0285\n",
       "478     method    264.482756   4854.069801  Topic10  -6.2026  -0.0461\n",
       "62       based    272.084753   5612.950653  Topic10  -6.1743  -0.1630\n",
       "507    network    267.943187   5757.054049  Topic10  -6.1896  -0.2037\n",
       "299      first    264.362708   5286.088937  Topic10  -6.2031  -0.1318\n",
       "\n",
       "[901 rows x 6 columns], token_table=       Topic      Freq    Term\n",
       "term                          \n",
       "12214      1  0.208916  aaabbb\n",
       "12214      7  0.208916  aaabbb\n",
       "12214      8  0.208916  aaabbb\n",
       "29041      1  0.149033   abuam\n",
       "29041      3  0.149033   abuam\n",
       "...      ...       ...     ...\n",
       "27582      6  0.083335   zloop\n",
       "27582      7  0.083335   zloop\n",
       "27582      8  0.041668   zloop\n",
       "27582      9  0.208338   zloop\n",
       "27582     10  0.041668   zloop\n",
       "\n",
       "[4330 rows x 3 columns], R=30, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[3, 10, 6, 4, 2, 7, 8, 5, 1, 9])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyLDAvis.gensim_models as gensimvis\n",
    "import pickle \n",
    "import pyLDAvis\n",
    "\n",
    "# Visualize the topics\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "LDAvis_data_filepath = os.path.join('./results/ldavis_prepared_'+str(num_topics))\n",
    "\n",
    "# # this is a bit time consuming - make the if statement True\n",
    "# # if you want to execute visualization prep yourself\n",
    "if 1 == 1:\n",
    "    LDAvis_prepared = gensimvis.prepare(lda_model, corpus, id2word)\n",
    "    with open(LDAvis_data_filepath, 'wb') as f:\n",
    "        pickle.dump(LDAvis_prepared, f)\n",
    "\n",
    "# load the pre-prepared pyLDAvis data from disk\n",
    "with open(LDAvis_data_filepath, 'rb') as f:\n",
    "    LDAvis_prepared = pickle.load(f)\n",
    "\n",
    "pyLDAvis.save_html(LDAvis_prepared, './results/ldavis_prepared_'+ str(num_topics) +'.html')\n",
    "\n",
    "LDAvis_prepared"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** **\n",
    "#### Closing Notes\n",
    "Machine learning has become increasingly popular over the past decade, and recent advances in computational availability have led to exponential growth to people looking for ways how new methods can be incorporated to advance the field of Natural Language Processing.\n",
    "\n",
    "Often, we treat topic models as black-box algorithms, but hopefully, this article addressed to shed light on the underlying math, and intuitions behind it, and high-level code to get you started with any textual data.\n",
    "\n",
    "In the next article, we’ll go one step deeper into understanding how you can evaluate the performance of topic models, tune its hyper-parameters to get more intuitive and reliable results.\n",
    "\n",
    "** **\n",
    "#### References:\n",
    "1. Topic model — Wikipedia. https://en.wikipedia.org/wiki/Topic_model\n",
    "2. Distributed Strategies for Topic Modeling. https://www.ideals.illinois.edu/bitstream/handle/2142/46405/ParallelTopicModels.pdf?sequence=2&isAllowed=y\n",
    "3. Topic Mapping — Software — Resources — Amaral Lab. https://amaral.northwestern.edu/resources/software/topic-mapping\n",
    "4. A Survey of Topic Modeling in Text Mining. https://thesai.org/Downloads/Volume6No1/Paper_21-A_Survey_of_Topic_Modeling_in_Text_Mining.pdf\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
